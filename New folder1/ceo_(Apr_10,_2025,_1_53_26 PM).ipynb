{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Environment Setup and Configuration (Reworked)\n",
        "# Status: Uses detailed dataclasses. Enforces EnhancedConfig usage. Enhanced secret loading.\n",
        "#         Added Vector DB & Service Endpoint configs. Added MIZ OKI schema version.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import uuid\n",
        "import datetime\n",
        "import re\n",
        "import hashlib\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import deque, defaultdict, Counter\n",
        "from typing import Dict, Any, Optional, List, Union, Tuple, Set, Type, Callable, TypeVar, Protocol\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, field, asdict\n",
        "\n",
        "# --- Data Handling ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Cloud Provider SDKs ---\n",
        "# Wrap imports in try-except to handle environments where they might be missing initially\n",
        "try:\n",
        "    from google.cloud import aiplatform\n",
        "    from google.cloud import storage\n",
        "    from google.cloud import bigquery\n",
        "    from google.cloud import secretmanager # Import Secret Manager client\n",
        "    from google.cloud import exceptions as gcp_exceptions\n",
        "    GCP_SDK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GCP_SDK_AVAILABLE = False\n",
        "    # Define dummy classes/exceptions if SDKs are missing\n",
        "    class aiplatform: pass; class storage: pass; class bigquery: pass; class secretmanager: pass; class gcp_exceptions: class NotFound(Exception): pass; class PermissionDenied(Exception): pass; class GoogleAPIError(Exception): pass\n",
        "    logging.warning(\"Google Cloud SDK components not found. Functionality will be limited.\")\n",
        "\n",
        "\n",
        "# --- Database/Graph (Check import, no connectivity test here) ---\n",
        "try:\n",
        "    from neo4j import GraphDatabase, basic_auth, exceptions as neo4j_exceptions\n",
        "    NEO4J_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NEO4J_AVAILABLE = False\n",
        "    class GraphDatabase: pass; class basic_auth: pass; class neo4j_exceptions: pass # Dummy types\n",
        "    logging.warning(\"Neo4j Python driver not found. Neo4j functionality unavailable.\")\n",
        "\n",
        "# --- Setup Logging (Early Configuration) ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger('MIZ-OKI.Environment')\n",
        "\n",
        "# --- Custom Exceptions ---\n",
        "class ConfigurationError(Exception):\n",
        "    \"\"\"Custom exception for configuration errors\"\"\"\n",
        "    pass\n",
        "\n",
        "# --- Configuration Dataclasses ---\n",
        "\n",
        "@dataclass\n",
        "class Neo4jConfig:\n",
        "    \"\"\"Neo4j database configuration\"\"\"\n",
        "    uri: str = field(default_factory=lambda: os.getenv(\"NEO4J_URI\", \"neo4j+s://localhost:7687\"))\n",
        "    user: str = field(default_factory=lambda: os.getenv(\"NEO4J_USER\", \"neo4j\"))\n",
        "    password: Optional[str] = field(default=None) # Loaded later\n",
        "    max_connection_lifetime: int = 3600\n",
        "    connection_timeout: int = 30\n",
        "    max_retry_time: int = 30\n",
        "\n",
        "@dataclass\n",
        "class DatabaseConfig:\n",
        "    \"\"\"Database configuration settings\"\"\"\n",
        "    neo4j: Neo4jConfig = field(default_factory=Neo4jConfig)\n",
        "    bigquery_project_id: Optional[str] = field(default_factory=lambda: os.getenv(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "    bigquery_dataset_id: str = field(default_factory=lambda: os.getenv(\"BQ_DATASET\", \"miz3_data\"))\n",
        "    storage_bucket: Optional[str] = field(default_factory=lambda: os.getenv(\"GCS_BUCKET_NAME\"))\n",
        "\n",
        "@dataclass\n",
        "class SecretManagerConfig:\n",
        "    \"\"\"Configuration for accessing secrets via Google Secret Manager.\"\"\"\n",
        "    enabled: bool = field(default_factory=lambda: os.getenv(\"USE_SECRET_MANAGER\", \"true\").lower() == \"true\")\n",
        "    project_id: Optional[str] = None # Defaults to main GCP project if None\n",
        "    neo4j_uri_secret: Optional[str] = field(default_factory=lambda: os.environ.get(\"NEO4J_URI_SECRET_ID\"))\n",
        "    neo4j_user_secret: Optional[str] = field(default_factory=lambda: os.environ.get(\"NEO4J_USER_SECRET_ID\"))\n",
        "    neo4j_password_secret: Optional[str] = field(default_factory=lambda: os.environ.get(\"NEO4J_PASSWORD_SECRET_ID\"))\n",
        "    miz_salt_secret: Optional[str] = field(default_factory=lambda: os.environ.get(\"MIZ_SALT_SECRET_ID\"))\n",
        "    openai_api_key_secret: Optional[str] = field(default_factory=lambda: os.environ.get(\"OPENAI_API_KEY_SECRET_ID\"))\n",
        "    anthropic_api_key_secret: Optional[str] = field(default_factory=lambda: os.environ.get(\"ANTHROPIC_API_KEY_SECRET_ID\"))\n",
        "    # Add other secrets as needed (e.g., external API keys, service endpoint keys)\n",
        "    kg_tool_api_key_secret: Optional[str] = field(default_factory=lambda: os.environ.get(\"KG_TOOL_API_KEY_SECRET_ID\"))\n",
        "    moe_registry_api_key_secret: Optional[str] = field(default_factory=lambda: os.environ.get(\"MOE_REGISTRY_API_KEY_SECRET_ID\"))\n",
        "\n",
        "@dataclass\n",
        "class GcpConfig:\n",
        "    project_id: Optional[str] = field(default_factory=lambda: os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "    region: str = field(default_factory=lambda: os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\"))\n",
        "    gcs_bucket_name: Optional[str] = None # Derived later\n",
        "    bq_dataset: str = \"miz3_data\"\n",
        "    bq_dataset_location: str = \"US\" # Or derive from region\n",
        "    secrets: SecretManagerConfig = field(default_factory=SecretManagerConfig)\n",
        "\n",
        "@dataclass\n",
        "class VertexAIConfig:\n",
        "    # Paths for storing tool/agent configurations used by Agent Builder/Workflows\n",
        "    agent_builder_tool_config_gcs_path: Optional[str] = None # Derived later\n",
        "    # Default Workflow IDs for core processes\n",
        "    default_workflow_id: str = field(default_factory=lambda: os.getenv(\"DEFAULT_WORKFLOW_ID\", \"miz3-core-cycle\"))\n",
        "    planning_workflow_id: str = field(default_factory=lambda: os.getenv(\"PLANNING_WORKFLOW_ID\", \"miz3-goal-planning\"))\n",
        "    experiment_execution_workflow_id: str = field(default_factory=lambda: os.getenv(\"EXPERIMENT_EXEC_WORKFLOW_ID\", \"miz3-experiment-execution\"))\n",
        "    experiment_analysis_workflow_id: str = field(default_factory=lambda: os.getenv(\"EXPERIMENT_ANALYSIS_WORKFLOW_ID\", \"miz3-experiment-analysis\"))\n",
        "    # API endpoint for interacting with Workflows API\n",
        "    workflows_api_endpoint: Optional[str] = None # Derived later\n",
        "    # Service account used by Workflow executions to call other services/APIs\n",
        "    workflow_service_account: Optional[str] = None # Derived later\n",
        "    # Agent Engine specific configurations (if using Agent Engine runtime)\n",
        "    agent_engine_network: Optional[str] = field(default_factory=lambda: os.getenv(\"AGENT_ENGINE_NETWORK\")) # VPC Network for Agent Engine\n",
        "    agent_engine_security_policy: Optional[str] = field(default_factory=lambda: os.getenv(\"AGENT_ENGINE_SECURITY_POLICY\")) # Security policy (e.g., IAP)\n",
        "    # Vertex AI Search configuration (for RAG grounding)\n",
        "    search_default_datastore_id: Optional[str] = field(default_factory=lambda: os.environ.get(\"VERTEX_SEARCH_DATASTORE_ID\"))\n",
        "    # Default model for simple tasks within Vertex AI context\n",
        "    default_model: str = field(default_factory=lambda: os.getenv(\"VERTEX_DEFAULT_MODEL\", \"gemini-1.5-flash-001\"))\n",
        "\n",
        "@dataclass\n",
        "class AdkConfig:\n",
        "    # Default runtime environment for ADK agents (if not using Agent Engine)\n",
        "    default_agent_runtime: str = field(default_factory=lambda: os.getenv(\"ADK_DEFAULT_RUNTIME\", \"cloud_run\")) # 'cloud_run' or 'cloud_functions'\n",
        "    # Default service account for deployed ADK agents\n",
        "    default_agent_service_account: Optional[str] = None # Derived later\n",
        "    # Base container image for ADK agents\n",
        "    default_agent_base_image: str = field(default_factory=lambda: os.getenv(\"ADK_BASE_IMAGE\", \"python:3.10-slim\"))\n",
        "    # How ADK tools are registered/discovered (GCS file or dedicated API)\n",
        "    tool_registry_type: str = field(default_factory=lambda: os.getenv(\"ADK_TOOL_REGISTRY_TYPE\", \"gcs\")) # 'gcs' or 'api'\n",
        "    # Location of the tool registry (GCS path or API endpoint)\n",
        "    tool_registry_location: Optional[str] = None # Derived later\n",
        "\n",
        "@dataclass\n",
        "class KgConfig:\n",
        "    # Primary storage for graph structure (Neo4j or potentially others)\n",
        "    storage_type: str = field(default_factory=lambda: os.getenv(\"KG_STORAGE_TYPE\", \"neo4j\").lower())\n",
        "    neo4j: Neo4jConfig = field(default_factory=Neo4jConfig)\n",
        "    # Performance/Scale parameters\n",
        "    memory_efficiency: float = field(default_factory=lambda: float(os.getenv(\"KG_MEMORY_EFFICIENCY\", \"0.85\")))\n",
        "    entity_resolution_accuracy: float = field(default_factory=lambda: float(os.getenv(\"KG_ENTITY_RESOLUTION_ACCURACY\", \"0.995\")))\n",
        "    cache_ttl_minutes: int = field(default_factory=lambda: int(os.getenv(\"KG_CACHE_TTL_MINUTES\", \"5\")))\n",
        "    # Vector Database configuration\n",
        "    vector_db_type: str = field(default_factory=lambda: os.getenv(\"VECTOR_DB_TYPE\", \"vertex_vector_search\").lower()) # 'vertex_vector_search', 'neo4j', 'none'\n",
        "    vector_index_name: str = field(default_factory=lambda: os.getenv(\"VECTOR_INDEX_NAME\", \"miz3_entity_embeddings\"))\n",
        "    vector_dimensions: int = field(default_factory=lambda: int(os.getenv(\"VECTOR_DIMENSIONS\", \"768\"))) # Match embedding model\n",
        "    # Specifics for Vertex AI Vector Search (Matching Engine)\n",
        "    vector_db_endpoint: Optional[str] = field(default_factory=lambda: os.getenv(\"VERTEX_VECTOR_INDEX_ENDPOINT_NAME\")) # Resource name: projects/.../indexEndpoints/...\n",
        "    vector_db_public_domain_name: Optional[str] = field(default_factory=lambda: os.getenv(\"VERTEX_VECTOR_ENDPOINT_DOMAIN\")) # Public endpoint domain if needed for direct access\n",
        "\n",
        "@dataclass\n",
        "class FoundationModelDefaults:\n",
        "    # Default models for different providers/tasks\n",
        "    vertex: str = field(default_factory=lambda: os.getenv(\"VERTEX_DEFAULT_MODEL\", \"gemini-1.5-flash-001\"))\n",
        "    llama4_scout: str = field(default_factory=lambda: os.getenv(\"LLAMA4_SCOUT_MODEL\", \"llama3-8b-instruct\")) # Example mapping\n",
        "    llama4_maverick: str = field(default_factory=lambda: os.getenv(\"LLAMA4_MAVERICK_MODEL\", \"llama3-70b-instruct\")) # Example mapping\n",
        "    llama4_embedding_model: str = field(default_factory=lambda: os.getenv(\"LLAMA4_EMBEDDING_MODEL\", \"text-embedding-004\")) # Example mapping\n",
        "    openai: str = field(default_factory=lambda: os.getenv(\"OPENAI_DEFAULT_MODEL\", \"gpt-4-turbo\"))\n",
        "    anthropic: str = field(default_factory=lambda: os.getenv(\"ANTHROPIC_DEFAULT_MODEL\", \"claude-3-5-sonnet-20240620\"))\n",
        "    # Specific models for internal tasks\n",
        "    xai_counterfactual_model: str = field(default_factory=lambda: os.getenv(\"XAI_COUNTERFACTUAL_MODEL\", \"llama4_maverick\")) # Alias used by XAI component\n",
        "    feedback_analyzer_model: str = field(default_factory=lambda: os.getenv(\"CV_FEEDBACK_ANALYZER_MODEL\", \"llama4_scout\")) # Alias used by CV component\n",
        "\n",
        "@dataclass\n",
        "class FoundationModelPricingEntry:\n",
        "    prompt: float = 0.0 # Price per million tokens\n",
        "    completion: float = 0.0 # Price per million tokens\n",
        "\n",
        "@dataclass\n",
        "class FoundationModelConfig:\n",
        "    # Stores API keys loaded from secrets/env vars\n",
        "    keys: Dict[str, Optional[str]] = field(default_factory=dict)\n",
        "    defaults: FoundationModelDefaults = field(default_factory=FoundationModelDefaults)\n",
        "    # Pricing information (can be loaded from external source or hardcoded)\n",
        "    pricing: Dict[str, Dict[str, FoundationModelPricingEntry]] = field(default_factory=lambda: {\n",
        "        \"vertex\": {\n",
        "            \"gemini-1.5-flash-001\": FoundationModelPricingEntry(prompt=0.35, completion=0.70),\n",
        "            \"gemini-1.5-pro-001\": FoundationModelPricingEntry(prompt=3.50, completion=10.50),\n",
        "            \"llama3-8b-instruct\": FoundationModelPricingEntry(prompt=0.50, completion=0.50), # Example pricing\n",
        "            \"llama3-70b-instruct\": FoundationModelPricingEntry(prompt=2.65, completion=2.65), # Example pricing\n",
        "            \"text-embedding-004\": FoundationModelPricingEntry(prompt=0.025, completion=0.0), # Example pricing\n",
        "        },\n",
        "        \"openai\": {\n",
        "            \"gpt-4-turbo\": FoundationModelPricingEntry(prompt=10.00, completion=30.00),\n",
        "            \"gpt-3.5-turbo\": FoundationModelPricingEntry(prompt=0.50, completion=1.50),\n",
        "        },\n",
        "        \"anthropic\": {\n",
        "            \"claude-3-5-sonnet-20240620\": FoundationModelPricingEntry(prompt=3.00, completion=15.00),\n",
        "            \"claude-3-opus-20240229\": FoundationModelPricingEntry(prompt=15.00, completion=75.00),\n",
        "        }\n",
        "    })\n",
        "\n",
        "@dataclass\n",
        "class BusinessImpactConfig:\n",
        "    # Load KPI definitions and targets (e.g., from JSON env var or config file)\n",
        "    kpis: Dict[str, Dict[str, Any]] = field(default_factory=lambda: json.loads(os.getenv(\"MIZ_KPIS\", '{}')))\n",
        "    # Specific targets mentioned in whitepaper\n",
        "    roas_target: float = field(default_factory=lambda: float(os.getenv(\"MIZ_ROAS_TARGET\", \"9.0\"))) # Target 8-10x\n",
        "    cac_reduction_target_factor: float = field(default_factory=lambda: float(os.getenv(\"MIZ_CAC_TARGET_FACTOR\", \"0.40\"))) # Target 55-65% reduction -> factor ~0.4\n",
        "    clv_increase_target_factor: float = field(default_factory=lambda: float(os.getenv(\"MIZ_CLV_TARGET_FACTOR\", \"1.50\"))) # Target 45-55% increase -> factor ~1.5\n",
        "\n",
        "@dataclass\n",
        "class SystemThresholdsConfig:\n",
        "    # Confidence thresholds for automated vs. human review\n",
        "    decision_confidence_threshold: float = field(default_factory=lambda: float(os.getenv(\"DECISION_CONF_THRESHOLD\", \"0.85\")))\n",
        "    human_review_confidence_threshold: float = field(default_factory=lambda: float(os.getenv(\"HUMAN_REVIEW_CONF_THRESHOLD\", \"0.75\")))\n",
        "    # Thresholds for triggering optimization or goal generation\n",
        "    optimization_threshold: float = field(default_factory=lambda: float(os.getenv(\"OPTIMIZATION_THRESHOLD\", \"0.7\"))) # Score below which PO triggers HDE\n",
        "    goal_generation_threshold: float = field(default_factory=lambda: float(os.getenv(\"GOAL_GEN_THRESHOLD\", \"0.6\"))) # Score below which AGG generates goal\n",
        "\n",
        "@dataclass\n",
        "class XaiConfig:\n",
        "    # Where XAI decision logs are stored ('kg' via API, 'log_file' via Cloud Logging)\n",
        "    storage_type: str = field(default_factory=lambda: os.getenv(\"XAI_STORAGE_TYPE\", \"kg\").lower())\n",
        "    # Log name for Cloud Logging (if storage_type is 'log_file')\n",
        "    log_name: str = field(default_factory=lambda: os.getenv(\"XAI_LOG_NAME\", \"miz_oki_xai_decisions\"))\n",
        "    # Alias for the model used for counterfactual explanations\n",
        "    counterfactual_model_alias: str = field(default_factory=lambda: os.getenv(\"XAI_COUNTERFACTUAL_MODEL_ALIAS\", \"llama4_maverick\"))\n",
        "\n",
        "@dataclass\n",
        "class LearningFlowsConfig:\n",
        "    # Knowledge Distillation settings\n",
        "    kd: Dict[str, Any] = field(default_factory=lambda: {\n",
        "        \"teacher_model_alias\": os.getenv(\"KD_TEACHER_MODEL_ALIAS\", \"llama4_maverick\"),\n",
        "        \"default_student_architecture\": os.getenv(\"KD_DEFAULT_STUDENT_ARCH\", \"distilbert-base-uncased\"),\n",
        "        \"output_gcs_prefix\": \"kd_outputs/\" # Relative to bucket root\n",
        "    })\n",
        "    # Continuous Validation settings\n",
        "    cv: Dict[str, Any] = field(default_factory=lambda: {\n",
        "        \"feedback_queue_maxsize\": int(os.getenv(\"CV_QUEUE_MAXSIZE\", \"1000\")),\n",
        "        \"monitoring_interval_seconds\": int(os.getenv(\"CV_MONITOR_INTERVAL_SEC\", \"300\")), # 5 minutes\n",
        "        \"drift_detection_threshold\": float(os.getenv(\"CV_DRIFT_THRESHOLD\", \"0.1\")),\n",
        "        \"bias_detection_threshold\": float(os.getenv(\"CV_BIAS_THRESHOLD\", \"0.05\")),\n",
        "        \"feedback_analyzer_model_alias\": os.getenv(\"CV_FEEDBACK_ANALYZER_MODEL_ALIAS\", \"feedback_analyzer_model\") # Use alias from FM defaults\n",
        "    })\n",
        "    # Dynamic Reward System settings\n",
        "    drs: Dict[str, Any] = field(default_factory=lambda: {\n",
        "        \"base_weights\": json.loads(os.getenv(\"DRS_BASE_WEIGHTS\", '{\"task_completion\": 1.0, \"efficiency\": 0.5, \"quality\": 0.8}')),\n",
        "        \"objective_influence_factor\": float(os.getenv(\"DRS_OBJECTIVE_FACTOR\", \"0.3\")),\n",
        "        \"update_interval_seconds\": int(os.getenv(\"DRS_UPDATE_INTERVAL_SEC\", \"600\")) # 10 minutes\n",
        "    })\n",
        "    # Distributed Reinforcement Learning settings\n",
        "    drl: Dict[str, Any] = field(default_factory=lambda: {\n",
        "        \"buffer_size\": int(os.getenv(\"DRL_BUFFER_SIZE\", \"50000\")),\n",
        "        \"buffer_save_interval_sec\": int(os.getenv(\"DRL_SAVE_INTERVAL_SEC\", \"600\")), # 10 minutes\n",
        "        \"min_buffer_for_train\": int(os.getenv(\"DRL_MIN_BUFFER_TRAIN\", \"1000\")),\n",
        "        \"buffer_gcs_prefix\": \"rl_buffer/\" # Relative to bucket root\n",
        "    })\n",
        "\n",
        "@dataclass\n",
        "class ServiceEndpointsConfig:\n",
        "    \"\"\"Configuration for dependent internal service endpoints.\"\"\"\n",
        "    # Endpoint for the deployed KG Tool service (Cell 3)\n",
        "    kg_tool_api_endpoint: Optional[str] = field(default_factory=lambda: os.getenv(\"KG_TOOL_API_ENDPOINT\")) # e.g., Cloud Run URL\n",
        "    # Endpoint for the deployed MoE Registry service (Cell 4)\n",
        "    moe_registry_api_endpoint: Optional[str] = field(default_factory=lambda: os.getenv(\"MOE_REGISTRY_API_ENDPOINT\")) # e.g., Cloud Run URL\n",
        "    # Endpoint for the deployed Expert Invoker service (used by HDE, PO, HP, etc.)\n",
        "    expert_invoker_api_endpoint: Optional[str] = field(default_factory=lambda: os.getenv(\"EXPERT_INVOKER_API_ENDPOINT\"))\n",
        "    # Add other internal service endpoints as needed (e.g., Causal Tool, Sim Tool, External API Tools)\n",
        "    ads_platform_api_endpoint: Optional[str] = field(default_factory=lambda: os.getenv(\"ADS_PLATFORM_API_ENDPOINT\"))\n",
        "    crm_api_endpoint: Optional[str] = field(default_factory=lambda: os.getenv(\"CRM_API_ENDPOINT\"))\n",
        "\n",
        "# --- Main Configuration Class (Reworked) ---\n",
        "@dataclass\n",
        "class EnhancedConfig:\n",
        "    \"\"\" Root configuration object for MIZ OKI 3.0 Platform. \"\"\"\n",
        "    miz_oki_schema_version: str = \"3.0\" # Added schema version\n",
        "    gcp: GcpConfig = field(default_factory=GcpConfig)\n",
        "    db: DatabaseConfig = field(default_factory=DatabaseConfig)\n",
        "    vertex_ai: VertexAIConfig = field(default_factory=VertexAIConfig)\n",
        "    adk: AdkConfig = field(default_factory=AdkConfig)\n",
        "    kg: KgConfig = field(default_factory=KgConfig)\n",
        "    foundation_models: FoundationModelConfig = field(default_factory=FoundationModelConfig)\n",
        "    business_impact: BusinessImpactConfig = field(default_factory=BusinessImpactConfig)\n",
        "    system_thresholds: SystemThresholdsConfig = field(default_factory=SystemThresholdsConfig)\n",
        "    xai: XaiConfig = field(default_factory=XaiConfig)\n",
        "    learning_flows: LearningFlowsConfig = field(default_factory=LearningFlowsConfig)\n",
        "    service_endpoints: ServiceEndpointsConfig = field(default_factory=ServiceEndpointsConfig) # Added service endpoints\n",
        "\n",
        "    # Top-level settings / Other service configs\n",
        "    mlops_pipeline_root: Optional[str] = None # Derived\n",
        "    mlops_serving_image: str = field(default_factory=lambda: os.getenv(\"MLOPS_SERVING_IMAGE\", \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2.11:latest\")) # Example\n",
        "    mlops_trigger_topic: str = field(default_factory=lambda: os.getenv(\"MLOPS_TRIGGER_TOPIC\", \"miz3-mlops-trigger\"))\n",
        "    mlops_rl_train_topic: str = field(default_factory=lambda: os.getenv(\"MLOPS_RL_TRAIN_TOPIC\", \"miz3-mlops-rl-train\"))\n",
        "    # Dead Letter Queue target ('log_only', 'pubsub_topic', 'gcs_path')\n",
        "    dlq_target: str = field(default_factory=lambda: os.getenv(\"DLQ_TARGET\", \"log_only\"))\n",
        "    # Salt for pseudonymization (loaded from Secret Manager or Env Var)\n",
        "    miz_salt: Optional[str] = None\n",
        "    # Configuration for external data sources used by AKA (Cell 4)\n",
        "    external_data_sources: Dict[str, Optional[str]] = field(default_factory=lambda: json.loads(os.getenv(\"EXTERNAL_DATA_SOURCES\", '{}'))) # e.g., {\"news_api_key_secret\": \"NEWS_API_KEY_SECRET_ID\"}\n",
        "    # Privacy policies used by PrivacyControlsTool (Cell 7)\n",
        "    privacy_policies: Dict[str, Dict[str, Any]] = field(default_factory=lambda: json.loads(os.getenv(\"PRIVACY_POLICIES\", '{\"default\": {\"requires_pseudonymization\": true, \"allowed_fields\": null}}')))\n",
        "    # Objectives used by HolisticOptimizerTool (Cell 5)\n",
        "    optimizer_objectives: Dict[str, Dict[str, Any]] = field(default_factory=lambda: json.loads(os.getenv(\"OPTIMIZER_OBJECTIVES\", '{}'))) # e.g., {\"ROAS\": {\"target\": 8.0, \"weight\": 0.6}}\n",
        "    # Forecasting models used by HolisticOptimizerTool (Cell 5)\n",
        "    optimizer_forecasting_models: Dict[str, str] = field(default_factory=lambda: json.loads(os.getenv(\"OPTIMIZER_FORECASTERS\", '{}'))) # e.g., {\"ROAS\": \"roas_forecaster_v1\"}\n",
        "    # Parameters for BEAB Tool (Cell 7)\n",
        "    beab_equity_weight: float = field(default_factory=lambda: float(os.getenv(\"BEAB_EQUITY_WEIGHT\", \"0.2\")))\n",
        "    rtb_min_bid_threshold: float = field(default_factory=lambda: float(os.getenv(\"RTB_MIN_BID\", \"0.01\")))\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\" Validate critical values and derive defaults after loading from environment. \"\"\"\n",
        "        logger.debug(\"Running EnhancedConfig __post_init__ ...\")\n",
        "        if not self.gcp.project_id:\n",
        "            raise ConfigurationError(\"CRITICAL: GOOGLE_CLOUD_PROJECT environment variable not set.\")\n",
        "\n",
        "        # Derive dependent GCP settings\n",
        "        if not self.db.bigquery_project_id: self.db.bigquery_project_id = self.gcp.project_id\n",
        "        if not self.gcp.gcs_bucket_name:\n",
        "            self.gcp.gcs_bucket_name = f\"{self.gcp.project_id}-miz3-data\"\n",
        "            logger.info(f\"GCS Bucket Name derived: {self.gcp.gcs_bucket_name}\")\n",
        "        if not self.db.storage_bucket: self.db.storage_bucket = self.gcp.gcs_bucket_name\n",
        "\n",
        "        if self.gcp.gcs_bucket_name:\n",
        "            self.mlops_pipeline_root = self.mlops_pipeline_root or f\"gs://{self.gcp.gcs_bucket_name}/miz3_pipelines\"\n",
        "            self.vertex_ai.agent_builder_tool_config_gcs_path = self.vertex_ai.agent_builder_tool_config_gcs_path or f\"gs://{self.gcp.gcs_bucket_name}/config/vertex_tools.json\"\n",
        "            if self.adk.tool_registry_type == 'gcs':\n",
        "                self.adk.tool_registry_location = self.adk.tool_registry_location or f\"gs://{self.gcp.gcs_bucket_name}/config/adk_tools_registry.json\"\n",
        "\n",
        "        # Derive API endpoints and service accounts\n",
        "        self.vertex_ai.workflows_api_endpoint = self.vertex_ai.workflows_api_endpoint or f\"{self.gcp.region}-workflows.googleapis.com\"\n",
        "        def_sa_suffix = f\"@{self.gcp.project_id}.iam.gserviceaccount.com\"\n",
        "        self.vertex_ai.workflow_service_account = self.vertex_ai.workflow_service_account or f\"miz-workflow-runner{def_sa_suffix}\"\n",
        "        self.adk.default_agent_service_account = self.adk.default_agent_service_account or f\"miz-adk-agent{def_sa_suffix}\"\n",
        "\n",
        "        # Resolve model aliases used internally after defaults are set\n",
        "        self.xai.counterfactual_model_alias = self.foundation_models.defaults.xai_counterfactual_model\n",
        "        self.learning_flows.cv['feedback_analyzer_model_alias'] = self.foundation_models.defaults.feedback_analyzer_model\n",
        "        self.learning_flows.kd['teacher_model_alias'] = self.foundation_models.defaults.llama4_maverick # Example, could be configurable\n",
        "\n",
        "        logger.debug(\"EnhancedConfig basic post-init derivation complete.\")\n",
        "\n",
        "    def get(self, key: str, default: Any = None) -> Any:\n",
        "        \"\"\"Provides dictionary-like access, navigating nested dataclasses using dot notation.\"\"\"\n",
        "        try:\n",
        "            value = self\n",
        "            keys = key.split('.')\n",
        "            for k in keys:\n",
        "                if isinstance(value, dict):\n",
        "                    value = value.get(k)\n",
        "                elif hasattr(value, k):\n",
        "                    value = getattr(value, k)\n",
        "                else:\n",
        "                    return default # Key part not found\n",
        "                if value is None: # Stop if any part resolves to None\n",
        "                    return default\n",
        "            return value\n",
        "        except (AttributeError, KeyError, TypeError):\n",
        "            return default\n",
        "\n",
        "    def get_model_info(self, model_alias_or_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\" Resolves an alias (from defaults) or ID to get provider, actual model ID, and pricing. \"\"\"\n",
        "        if not model_alias_or_id: return None\n",
        "        resolved_id = model_alias_or_id\n",
        "        provider = None\n",
        "\n",
        "        # Check if input matches an alias key in defaults dataclass\n",
        "        if hasattr(self.foundation_models.defaults, model_alias_or_id):\n",
        "            resolved_id = getattr(self.foundation_models.defaults, model_alias_or_id)\n",
        "            logger.debug(f\"Resolved alias '{model_alias_or_id}' to model ID '{resolved_id}'.\")\n",
        "        else:\n",
        "            # Check if input matches an alias *value* (less common but possible)\n",
        "            for field_name in self.foundation_models.defaults.__dataclass_fields__:\n",
        "                 if getattr(self.foundation_models.defaults, field_name) == model_alias_or_id:\n",
        "                      resolved_id = model_alias_or_id # It was already the resolved ID\n",
        "                      logger.debug(f\"Input '{model_alias_or_id}' matches a default model ID directly.\")\n",
        "                      break\n",
        "            # If still not resolved, assume input IS the model_id\n",
        "            model_id = resolved_id\n",
        "            logger.debug(f\"Assuming '{model_alias_or_id}' is the final model ID.\")\n",
        "\n",
        "        model_id = resolved_id # Final model ID\n",
        "\n",
        "        # Determine provider based on model ID patterns or config keys\n",
        "        if model_id:\n",
        "            if any(p in model_id for p in [\"gemini\", \"llama3\", \"text-embedding\"]): provider = \"vertex\"\n",
        "            elif \"gpt-\" in model_id: provider = \"openai\"\n",
        "            elif \"claude-\" in model_id: provider = \"anthropic\"\n",
        "            # Add more provider patterns if needed\n",
        "\n",
        "        if not provider: # Fallback based on available keys if pattern matching fails\n",
        "            if \"vertex\" in self.foundation_models.keys: provider = \"vertex\"\n",
        "            elif \"openai\" in self.foundation_models.keys: provider = \"openai\"\n",
        "            elif \"anthropic\" in self.foundation_models.keys: provider = \"anthropic\"\n",
        "\n",
        "        if not provider:\n",
        "            logger.warning(f\"Could not determine provider for '{model_alias_or_id}' (resolved to '{model_id}').\")\n",
        "            return None\n",
        "\n",
        "        # Look up pricing\n",
        "        provider_pricing = self.foundation_models.pricing.get(provider, {})\n",
        "        model_pricing_entry = provider_pricing.get(model_id)\n",
        "        pricing_dict = asdict(model_pricing_entry) if model_pricing_entry else None\n",
        "        if not pricing_dict and model_id:\n",
        "            logger.warning(f\"Pricing not found for model '{model_id}' under provider '{provider}'.\")\n",
        "\n",
        "        return {\"provider\": provider, \"model_id\": model_id, \"pricing\": pricing_dict}\n",
        "\n",
        "# --- Helper Function to Load Secrets (Enhanced Error Handling) ---\n",
        "def _load_secret(secret_id: Optional[str], project_id: Optional[str] = None) -> Optional[str]:\n",
        "    \"\"\"Loads a secret from Google Secret Manager. Returns None if failed or not configured.\"\"\"\n",
        "    if not secret_id:\n",
        "        # logger.debug(\"Secret ID is None or empty, skipping load.\")\n",
        "        return None\n",
        "    if not GCP_SDK_AVAILABLE or not hasattr(secretmanager, 'SecretManagerServiceClient'):\n",
        "        logger.warning(\"Secret Manager SDK not available. Cannot load secrets.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        client = secretmanager.SecretManagerServiceClient()\n",
        "        if '/' not in secret_id:\n",
        "            if not project_id:\n",
        "                raise ValueError(\"Project ID needed for short secret names.\")\n",
        "            # Assume short name is just the secret ID, construct full name\n",
        "            secret_name = f\"projects/{project_id}/secrets/{secret_id}/versions/latest\"\n",
        "        elif '/versions/' not in secret_id:\n",
        "            # Assume name is projects/.../secrets/SECRET_ID, add version\n",
        "            secret_name = f\"{secret_id}/versions/latest\"\n",
        "        else:\n",
        "            # Assume full version name is provided\n",
        "            secret_name = secret_id\n",
        "\n",
        "        logger.debug(f\"Accessing secret: {secret_name}\")\n",
        "        response = client.access_secret_version(name=secret_name)\n",
        "        payload = response.payload.data.decode(\"UTF-8\")\n",
        "        logger.debug(f\"Successfully accessed secret: {secret_id} (short name or full path)\")\n",
        "        return payload\n",
        "    except gcp_exceptions.NotFound:\n",
        "        logger.error(f\"Secret not found: {secret_id} (Full path tried: {secret_name})\")\n",
        "        return None\n",
        "    except gcp_exceptions.PermissionDenied:\n",
        "        logger.error(f\"Permission denied accessing secret: {secret_id}. Check SA permissions for {secret_name}.\")\n",
        "        return None\n",
        "    except ValueError as ve:\n",
        "        logger.error(f\"Value error loading secret '{secret_id}': {ve}\") # e.g., missing project ID\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected exceptions during secret access\n",
        "        logger.error(f\"Failed to load secret '{secret_id}' from {secret_name}: {type(e).__name__} - {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "# --- Config Loading Function (Reworked) ---\n",
        "def load_configuration() -> EnhancedConfig:\n",
        "    \"\"\" Loads configuration from environment variables and Secret Manager, returning EnhancedConfig. \"\"\"\n",
        "    logger.info(\"Loading MIZ 3.0 OKI configuration (Reworked Cell 1)...\")\n",
        "    try:\n",
        "        # Step 1: Instantiate EnhancedConfig - loads defaults from environment variables\n",
        "        cfg = EnhancedConfig()\n",
        "    except ConfigurationError as e:\n",
        "        logger.critical(f\"Initial config validation failed during __post_init__: {e}\", exc_info=True)\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Critical error during initial config instantiation: {e}\", exc_info=True)\n",
        "        raise ConfigurationError(f\"Failed to instantiate EnhancedConfig: {e}\") from e\n",
        "\n",
        "    # Step 2: Load secrets if enabled\n",
        "    secret_proj_id = cfg.gcp.secrets.project_id or cfg.gcp.project_id # Use specific secret project ID if set\n",
        "    if cfg.gcp.secrets.enabled:\n",
        "        if not GCP_SDK_AVAILABLE:\n",
        "            logger.warning(\"Secret Manager is enabled in config, but SDK is not available. Skipping secret loading.\")\n",
        "        else:\n",
        "            logger.info(f\"Attempting to load secrets from Google Secret Manager (Project: {secret_proj_id})\")\n",
        "            # --- Neo4j Secrets ---\n",
        "            cfg.db.neo4j.uri = _load_secret(cfg.gcp.secrets.neo4j_uri_secret, secret_proj_id) or cfg.db.neo4j.uri\n",
        "            cfg.db.neo4j.user = _load_secret(cfg.gcp.secrets.neo4j_user_secret, secret_proj_id) or cfg.db.neo4j.user\n",
        "            cfg.db.neo4j.password = _load_secret(cfg.gcp.secrets.neo4j_password_secret, secret_proj_id) or os.getenv(\"NEO4J_PASSWORD\") # Fallback to env var\n",
        "\n",
        "            # --- MIZ Salt ---\n",
        "            cfg.miz_salt = _load_secret(cfg.gcp.secrets.miz_salt_secret, secret_proj_id) or os.getenv(\"MIZ_SALT\")\n",
        "\n",
        "            # --- Foundation Model API Keys ---\n",
        "            cfg.foundation_models.keys['openai'] = _load_secret(cfg.gcp.secrets.openai_api_key_secret, secret_proj_id) or os.getenv(\"OPENAI_API_KEY\")\n",
        "            cfg.foundation_models.keys['anthropic'] = _load_secret(cfg.gcp.secrets.anthropic_api_key_secret, secret_proj_id) or os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "            # --- Service Endpoint Secrets (e.g., API Keys for internal tools) ---\n",
        "            # Example: Load API key for KG Tool if secured via API Key\n",
        "            # cfg.service_endpoints.kg_tool_api_key = _load_secret(cfg.gcp.secrets.kg_tool_api_key_secret, secret_proj_id) or os.getenv(\"KG_TOOL_API_KEY\")\n",
        "            # Example: Load API key for MoE Registry if secured via API Key\n",
        "            # cfg.service_endpoints.moe_registry_api_key = _load_secret(cfg.gcp.secrets.moe_registry_api_key_secret, secret_proj_id) or os.getenv(\"MOE_REGISTRY_API_KEY\")\n",
        "\n",
        "            # Load service endpoints themselves if they are stored in secrets (less common, usually env vars)\n",
        "            cfg.service_endpoints.kg_tool_api_endpoint = _load_secret(os.getenv(\"KG_TOOL_API_ENDPOINT_SECRET_ID\"), secret_proj_id) or cfg.service_endpoints.kg_tool_api_endpoint\n",
        "            cfg.service_endpoints.moe_registry_api_endpoint = _load_secret(os.getenv(\"MOE_REGISTRY_API_ENDPOINT_SECRET_ID\"), secret_proj_id) or cfg.service_endpoints.moe_registry_api_endpoint\n",
        "            cfg.service_endpoints.expert_invoker_api_endpoint = _load_secret(os.getenv(\"EXPERT_INVOKER_API_ENDPOINT_SECRET_ID\"), secret_proj_id) or cfg.service_endpoints.expert_invoker_api_endpoint\n",
        "            cfg.service_endpoints.ads_platform_api_endpoint = _load_secret(os.getenv(\"ADS_PLATFORM_API_ENDPOINT_SECRET_ID\"), secret_proj_id) or cfg.service_endpoints.ads_platform_api_endpoint\n",
        "            cfg.service_endpoints.crm_api_endpoint = _load_secret(os.getenv(\"CRM_API_ENDPOINT_SECRET_ID\"), secret_proj_id) or cfg.service_endpoints.crm_api_endpoint\n",
        "\n",
        "    else:\n",
        "        logger.warning(\"Secret Manager not enabled. Loading secrets/endpoints from environment variables ONLY.\")\n",
        "        # Load sensitive values directly from environment variables as fallback\n",
        "        cfg.db.neo4j.password = os.getenv(\"NEO4J_PASSWORD\")\n",
        "        cfg.miz_salt = os.getenv(\"MIZ_SALT\")\n",
        "        cfg.foundation_models.keys['openai'] = os.getenv(\"OPENAI_API_KEY\")\n",
        "        cfg.foundation_models.keys['anthropic'] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "        # cfg.service_endpoints.kg_tool_api_key = os.getenv(\"KG_TOOL_API_KEY\")\n",
        "        # cfg.service_endpoints.moe_registry_api_key = os.getenv(\"MOE_REGISTRY_API_KEY\")\n",
        "        # Endpoints typically still come from env vars even if secrets are disabled\n",
        "        cfg.service_endpoints.kg_tool_api_endpoint = cfg.service_endpoints.kg_tool_api_endpoint # Already loaded from env by default\n",
        "        cfg.service_endpoints.moe_registry_api_endpoint = cfg.service_endpoints.moe_registry_api_endpoint\n",
        "        cfg.service_endpoints.expert_invoker_api_endpoint = cfg.service_endpoints.expert_invoker_api_endpoint\n",
        "        cfg.service_endpoints.ads_platform_api_endpoint = cfg.service_endpoints.ads_platform_api_endpoint\n",
        "        cfg.service_endpoints.crm_api_endpoint = cfg.service_endpoints.crm_api_endpoint\n",
        "\n",
        "\n",
        "    # Step 3: Finalize loaded keys/auth\n",
        "    # Remove providers with no keys/auth loaded\n",
        "    cfg.foundation_models.keys = {k:v for k, v in cfg.foundation_models.keys.items() if v is not None}\n",
        "    # Add 'vertex' key if GCP project is set (implies SDK auth is possible)\n",
        "    if cfg.gcp.project_id:\n",
        "        cfg.foundation_models.keys['vertex'] = \"gcp_authenticated\"\n",
        "\n",
        "    # --- Final Validation ---\n",
        "    logger.debug(\"Performing final configuration validation...\")\n",
        "    critical_errors = []\n",
        "    warnings = []\n",
        "    try:\n",
        "        # Critical: MIZ Salt\n",
        "        if not cfg.miz_salt or cfg.miz_salt == \"default_insecure_salt_replace_me_!!\":\n",
        "            critical_errors.append(\"MIZ_SALT missing or insecure. Set MIZ_SALT env var or MIZ_SALT_SECRET_ID.\")\n",
        "\n",
        "        # Critical: Neo4j Config (if used)\n",
        "        if cfg.kg.storage_type == \"neo4j\":\n",
        "            if not all([cfg.db.neo4j.uri, cfg.db.neo4j.user, cfg.db.neo4j.password]):\n",
        "                critical_errors.append(\"Neo4j configured (kg.storage_type='neo4j') but connection details (URI, User, Password) missing. Check NEO4J_* env vars or secret IDs.\")\n",
        "            if cfg.db.neo4j.password == \"password\":\n",
        "                warnings.append(\"SECURITY ALERT: Using default 'password' for Neo4j!\")\n",
        "            if not NEO4J_AVAILABLE:\n",
        "                critical_errors.append(\"Neo4j configured but 'neo4j' library not installed. Run 'pip install neo4j'.\")\n",
        "\n",
        "        # Warning: Vector DB Endpoint (if used)\n",
        "        if cfg.kg.vector_db_type == \"vertex_vector_search\" and not cfg.kg.vector_db_endpoint:\n",
        "            warnings.append(\"Vertex Vector Search configured (kg.vector_db_type='vertex_vector_search') but endpoint name (VERTEX_VECTOR_INDEX_ENDPOINT_NAME env var) missing.\")\n",
        "\n",
        "        # Warning: Foundation Model Keys\n",
        "        if not cfg.foundation_models.keys:\n",
        "            warnings.append(\"No Foundation Model API keys or authentication methods loaded. Check *_API_KEY env vars or secret IDs.\")\n",
        "\n",
        "        # Warning: Required Service Endpoints (adjust based on actual dependencies)\n",
        "        if not cfg.service_endpoints.kg_tool_api_endpoint:\n",
        "            warnings.append(\"KG Tool API endpoint (KG_TOOL_API_ENDPOINT env var or secret ID) not configured. KG interactions via API will fail.\")\n",
        "        if not cfg.service_endpoints.moe_registry_api_endpoint:\n",
        "            warnings.append(\"MoE Registry API endpoint (MOE_REGISTRY_API_ENDPOINT env var or secret ID) not configured. Dynamic expert routing will fail.\")\n",
        "        if not cfg.service_endpoints.expert_invoker_api_endpoint:\n",
        "            warnings.append(\"Expert Invoker API endpoint (EXPERT_INVOKER_API_ENDPOINT env var or secret ID) not configured. Calls to expert models will fail.\")\n",
        "\n",
        "        # Log warnings\n",
        "        for warning in warnings:\n",
        "            logger.warning(f\"CONFIG WARNING: {warning}\")\n",
        "\n",
        "        # Raise critical errors\n",
        "        if critical_errors:\n",
        "            error_message = \"CRITICAL CONFIGURATION ERRORS:\\n\" + \"\\n\".join([f\"- {err}\" for err in critical_errors])\n",
        "            logger.critical(error_message)\n",
        "            raise ConfigurationError(error_message)\n",
        "\n",
        "        logger.debug(\"EnhancedConfig final validation complete.\")\n",
        "    except (ConfigurationError, ValueError, ImportError) as final_val_e:\n",
        "        logger.critical(f\"CRITICAL CONFIGURATION ERROR during final validation: {final_val_e}.\", exc_info=True)\n",
        "        raise # Re-raise critical errors\n",
        "\n",
        "    logger.info(\"Configuration loading complete.\")\n",
        "    return cfg\n",
        "\n",
        "# --- Global Config Instance ---\n",
        "CONFIG_OBJ: Optional[EnhancedConfig] = None\n",
        "PROJECT_ID: Optional[str] = None\n",
        "REGION: Optional[str] = None\n",
        "BUCKET_NAME: Optional[str] = None\n",
        "\n",
        "try:\n",
        "    CONFIG_OBJ = load_configuration()\n",
        "    PROJECT_ID = CONFIG_OBJ.gcp.project_id\n",
        "    REGION = CONFIG_OBJ.gcp.region\n",
        "    BUCKET_NAME = CONFIG_OBJ.gcp.gcs_bucket_name\n",
        "    logger.info(\"Global configuration object (CONFIG_OBJ) created successfully.\")\n",
        "except (ConfigurationError, ValueError, ImportError, NameError, AttributeError) as config_ve:\n",
        "    logger.critical(f\"CRITICAL CONFIGURATION ERROR: {config_ve}. Review environment variables and Secret Manager setup.\", exc_info=True)\n",
        "    # Attempt to set fallbacks for basic GCP info if possible, but CONFIG_OBJ is None\n",
        "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "    REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "    BUCKET_NAME = f\"{PROJECT_ID}-miz3-data\" if PROJECT_ID else None\n",
        "    CONFIG_OBJ = None\n",
        "except Exception as config_e:\n",
        "     logger.critical(f\"CRITICAL: Unexpected error during configuration loading: {config_e}\", exc_info=True)\n",
        "     PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "     REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "     BUCKET_NAME = f\"{PROJECT_ID}-miz3-data\" if PROJECT_ID else None\n",
        "     CONFIG_OBJ = None\n",
        "     logger.error(\"Falling back to default/guessed PROJECT_ID/REGION/BUCKET_NAME and empty CONFIG_OBJ.\")\n",
        "\n",
        "# --- Initialize Vertex AI SDK ---\n",
        "vertex_ai_initialized = False\n",
        "if CONFIG_OBJ and PROJECT_ID and REGION and BUCKET_NAME and GCP_SDK_AVAILABLE and hasattr(aiplatform, 'init'):\n",
        "    try:\n",
        "        # Check if already initialized (simple check)\n",
        "        if not getattr(aiplatform.initializer.global_config, 'project', None):\n",
        "            staging_bucket_uri = f\"gs://{BUCKET_NAME}/vertex_staging\"\n",
        "            logger.info(f\"Initializing Vertex AI SDK for project {PROJECT_ID} in {REGION} with staging bucket {staging_bucket_uri}...\")\n",
        "            aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=staging_bucket_uri)\n",
        "            logger.info(f\"Vertex AI SDK initialized.\")\n",
        "        else:\n",
        "            logger.info(f\"Vertex AI SDK already initialized for project {aiplatform.initializer.global_config.project}.\")\n",
        "        vertex_ai_initialized = True\n",
        "    except Exception as ai_init_e:\n",
        "        logger.error(f\"Failed to initialize Vertex AI SDK: {ai_init_e}\", exc_info=True)\n",
        "elif not GCP_SDK_AVAILABLE:\n",
        "     logger.warning(\"Vertex AI SDK not available. Skipping initialization.\")\n",
        "else:\n",
        "     logger.warning(\"Config object not loaded or critical GCP values missing. Skipping Vertex AI initialization.\")\n",
        "\n",
        "# --- Check GCS Bucket Presence ---\n",
        "gcs_bucket_checked = False\n",
        "if CONFIG_OBJ and BUCKET_NAME and GCP_SDK_AVAILABLE and hasattr(storage, 'Client'):\n",
        "    try:\n",
        "        storage_client = storage.Client(project=PROJECT_ID)\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        if bucket.exists():\n",
        "            logger.info(f\"GCS Bucket '{BUCKET_NAME}' confirmed to exist.\")\n",
        "            gcs_bucket_checked = True\n",
        "        else:\n",
        "            logger.error(f\"CRITICAL: GCS Bucket '{BUCKET_NAME}' configured but does not exist! Create the bucket or correct GCS_BUCKET_NAME.\")\n",
        "    except Exception as gcs_check_e:\n",
        "        logger.error(f\"Failed to check GCS bucket '{BUCKET_NAME}' existence: {gcs_check_e}\")\n",
        "elif not GCP_SDK_AVAILABLE:\n",
        "    logger.warning(\"Storage SDK not available. Skipping GCS bucket check.\")\n",
        "else:\n",
        "     logger.warning(\"GCS Bucket name not configured or PROJECT_ID missing. Skipping bucket check.\")\n",
        "\n",
        "# --- Final Status Check ---\n",
        "print(\"\\n--- MIZ 3.0 OKI Environment Status (Reworked Cell 1) ---\")\n",
        "if CONFIG_OBJ is None:\n",
        "    print(\"❌ CRITICAL: Configuration loading failed. Check logs above for critical errors.\")\n",
        "else:\n",
        "     print(f\"✅ EnhancedConfig Initialized (Project: {PROJECT_ID}, Region: {REGION})\")\n",
        "     print(f\"✅ MIZ OKI Schema Version: {CONFIG_OBJ.miz_oki_schema_version}\")\n",
        "\n",
        "     # Vertex AI Status\n",
        "     if not GCP_SDK_AVAILABLE: print(\"🟡 WARNING: Google Cloud SDKs not fully available.\")\n",
        "     elif not vertex_ai_initialized: print(\"❌ WARNING: Vertex AI SDK initialization FAILED. Check credentials and project/region.\")\n",
        "     else: print(\"✅ Vertex AI SDK Initialized.\")\n",
        "\n",
        "     # GCS Status\n",
        "     if not BUCKET_NAME: print(f\"❌ CRITICAL: GCS Bucket name configuration missing (GCS_BUCKET_NAME).\")\n",
        "     elif not gcs_bucket_checked: print(f\"❌ CRITICAL: GCS Bucket '{BUCKET_NAME}' check failed or bucket does not exist.\")\n",
        "     else: print(f\"✅ GCS Bucket '{BUCKET_NAME}' Confirmed.\")\n",
        "\n",
        "     # Secret Manager Status\n",
        "     if CONFIG_OBJ.gcp.secrets.enabled:\n",
        "         if not GCP_SDK_AVAILABLE or not hasattr(secretmanager, 'SecretManagerServiceClient'): print(\"🟡 WARNING: Secret Manager enabled but SDK unavailable.\")\n",
        "         else: print(\"✅ Secret Manager: Enabled (Secret loading attempted). Check logs for errors.\")\n",
        "     else: print(\"ℹ️ Secret Manager: Disabled (Using Environment Variables for secrets).\")\n",
        "\n",
        "     # KG Status\n",
        "     if CONFIG_OBJ.kg.storage_type == \"neo4j\":\n",
        "          if NEO4J_AVAILABLE: print(\"✅ Neo4j Configured (Library found; Connectivity checked by KG component).\")\n",
        "          else: print(f\"❌ CRITICAL: Neo4j configured but 'neo4j' library not installed.\")\n",
        "     else: print(f\"ℹ️ KG Storage Type: '{CONFIG_OBJ.kg.storage_type}'.\")\n",
        "\n",
        "     # Vector DB Status\n",
        "     if CONFIG_OBJ.kg.vector_db_type == \"vertex_vector_search\":\n",
        "         if CONFIG_OBJ.kg.vector_db_endpoint: print(\"✅ Vertex Vector Search: Configured (Endpoint found).\")\n",
        "         else: print(\"🟡 WARNING: Vertex Vector Search configured but endpoint name missing (VERTEX_VECTOR_INDEX_ENDPOINT_NAME).\")\n",
        "     else: print(f\"ℹ️ Vector DB Type: '{CONFIG_OBJ.kg.vector_db_type}'.\")\n",
        "\n",
        "     # MIZ Salt Status\n",
        "     if CONFIG_OBJ.miz_salt and CONFIG_OBJ.miz_salt != \"default_insecure_salt_replace_me_!!\": print(\"✅ MIZ Salt Configured.\")\n",
        "     else: print(\"❌ CRITICAL WARNING: Using default or missing MIZ Salt! Set MIZ_SALT or MIZ_SALT_SECRET_ID.\")\n",
        "\n",
        "     # Foundation Models Status\n",
        "     fm_keys_found = list(CONFIG_OBJ.foundation_models.keys.keys())\n",
        "     if not fm_keys_found: print(\"🟡 WARNING: No Foundation Model keys/auth loaded.\")\n",
        "     else: print(f\"✅ Foundation Models Configured. Providers with keys/auth: {fm_keys_found}\")\n",
        "\n",
        "     # Service Endpoints Status\n",
        "     print(f\"✅ Service Endpoints:\")\n",
        "     print(f\"  - KG Tool API: '{CONFIG_OBJ.service_endpoints.kg_tool_api_endpoint or 'Not Set (WARNING)'}'\")\n",
        "     print(f\"  - MoE Registry API: '{CONFIG_OBJ.service_endpoints.moe_registry_api_endpoint or 'Not Set (WARNING)'}'\")\n",
        "     print(f\"  - Expert Invoker API: '{CONFIG_OBJ.service_endpoints.expert_invoker_api_endpoint or 'Not Set (WARNING)'}'\")\n",
        "     # Add checks for other critical endpoints if necessary\n",
        "\n",
        "     print(\"ℹ️ Orchestration: Relies on Vertex AI Workflows/Agent Engine.\")\n",
        "print(\"-------------------------------------------------------------\")\n",
        "logger.info(\"MIZ 3.0 OKI BGI Platform - Environment configuration complete (Reworked Cell 1).\")\n",
        "\n",
        "# Check if critical errors occurred during setup\n",
        "if CONFIG_OBJ is None or not gcs_bucket_checked:\n",
        "    print(\"\\n\\n*** CRITICAL ERRORS DETECTED DURING SETUP. SYSTEM MAY NOT FUNCTION CORRECTLY. REVIEW LOGS. ***\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "EzMtlzbdQQeM",
        "outputId": "046f44ec-8535-4ac4-ceaa-a458affac8ac"
      },
      "id": "EzMtlzbdQQeM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-35cc5112588d>, line 35)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-35cc5112588d>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    class aiplatform: pass; class storage: pass; class bigquery: pass; class secretmanager: pass; class gcp_exceptions: class NotFound(Exception): pass; class PermissionDenied(Exception): pass; class GoogleAPIError(Exception): pass\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Data Extraction and KG Prep (Reworked)\n",
        "# Status: Logic structured for deployment as a Tool/Service callable by Vertex AI Workflow.\n",
        "#         Uses MIZ OKI I/O structure. Uses real FMClientTool proxy (Cell 18).\n",
        "#         Outputs KG-ready data. Requires specific transform logic implementation.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime\n",
        "import logging\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import uuid\n",
        "import asyncio\n",
        "import functools\n",
        "from typing import Dict, Any, Optional, List, Union, Tuple\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "import chardet # For robust CSV encoding detection\n",
        "\n",
        "# --- Cloud/External Libs ---\n",
        "# Assume google-cloud-storage, google-cloud-bigquery are available via Cell 1 checks\n",
        "try:\n",
        "    from google.cloud import storage, bigquery, exceptions as gcp_exceptions\n",
        "    GCP_SDK_AVAILABLE = True # Re-check based on Cell 1\n",
        "except ImportError:\n",
        "    GCP_SDK_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import aio_gcsfs # Recommended: `pip install aio-gcsfs gcsfs`\n",
        "    import gcsfs\n",
        "    AIO_GCS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    AIO_GCS_AVAILABLE = False\n",
        "    logging.warning(\"aio-gcsfs/gcsfs not installed. GCS reads will use less efficient sync methods in threads.\")\n",
        "\n",
        "# --- Assume Real dependencies from other cells are loaded/injected ---\n",
        "# These would typically be clients/proxies passed during service initialization\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    # Use foundation_model_client proxy (representing deployed Cell 18 service)\n",
        "    # This would be injected into the service running this logic\n",
        "    if 'foundation_model_client' not in globals() or not foundation_model_client:\n",
        "        # If running standalone, create a mock for testing\n",
        "        class MockFMClientTool:\n",
        "            async def extract_kg_data_from_content(self, *args, **kwargs): await asyncio.sleep(0.05); return {\"status\": \"success\", \"payload\": {\"entities\": [], \"relationships\": []}}\n",
        "            async def summarize(self, *args, **kwargs): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"summary\": \"Mock summary.\"}}\n",
        "            async def extract_entities(self, *args, **kwargs): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"entities\": [{\"entity\": \"mock\"}]}}\n",
        "        foundation_model_client = MockFMClientTool()\n",
        "        logging.warning(\"Using Mock FoundationModelClient for Cell 2.\")\n",
        "\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _fm_client_tool = foundation_model_client # Use real or mock client proxy\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real/mock CONFIG_OBJ and foundation_model_client in Cell 2 (Reworked).\")\n",
        "except NameError as e:\n",
        "    # This block should ideally not be hit if Cell 1 ran successfully\n",
        "    logging.critical(f\"CRITICAL DEPENDENCY ERROR ({e}). Cannot proceed with Cell 2 logic.\", exc_info=True)\n",
        "    # Define minimal mocks to prevent immediate crashes, but functionality is broken\n",
        "    _config_obj = None; _fm_client_tool = None; _real_dependencies = False\n",
        "\n",
        "logger = logging.getLogger('MIZ-OKI.DataExtractionTool')\n",
        "\n",
        "# --- Data Validation Function (Improved Error Handling) ---\n",
        "def validate_dataframe_schema(df: pd.DataFrame, schema: List[bigquery.SchemaField], file_path: str) -> Tuple[bool, pd.DataFrame, List[str]]:\n",
        "    \"\"\"Validates the DataFrame against the provided BigQuery schema. Handles type coercion.\"\"\"\n",
        "    if df.empty: return True, df, [] # Skip validation for empty dataframes\n",
        "    errors = []; df_validated = df.copy()\n",
        "    type_mapping = {\n",
        "        \"STRING\": \"string\", \"BYTES\": \"object\", \"INTEGER\": \"Int64\", \"INT64\": \"Int64\",\n",
        "        \"FLOAT\": \"float64\", \"FLOAT64\": \"float64\", \"NUMERIC\": \"float64\", \"BIGNUMERIC\": \"float64\",\n",
        "        \"BOOLEAN\": \"boolean\", \"BOOL\": \"boolean\", \"TIMESTAMP\": \"datetime64[ns, UTC]\",\n",
        "        \"DATE\": \"datetime64[ns]\", \"TIME\": \"object\", \"DATETIME\": \"datetime64[ns]\",\n",
        "        \"GEOGRAPHY\": \"object\", \"JSON\": \"object\",\n",
        "    }\n",
        "    schema_dict = {col.name: col for col in schema}\n",
        "    present_schema_cols = {col.name for col in schema if col.name in df_validated.columns}\n",
        "    missing_required_cols = [col.name for col in schema if col.name not in df_validated.columns and col.mode == 'REQUIRED']\n",
        "    if missing_required_cols:\n",
        "        errors.append(f\"Missing REQUIRED columns: {', '.join(missing_required_cols)}\")\n",
        "        # Decide whether to fail or add NA columns - failing is safer for required cols\n",
        "        return False, df, errors\n",
        "\n",
        "    for col_name in present_schema_cols:\n",
        "        target_field = schema_dict[col_name]\n",
        "        target_bq_type = target_field.field_type\n",
        "        target_pd_type = type_mapping.get(target_bq_type)\n",
        "\n",
        "        if target_pd_type:\n",
        "            try:\n",
        "                original_null_count = df_validated[col_name].isnull().sum()\n",
        "                current_series = df_validated[col_name]\n",
        "\n",
        "                if target_pd_type.startswith(\"datetime\"):\n",
        "                    # More robust datetime parsing\n",
        "                    converted_series = pd.to_datetime(current_series, errors='coerce', infer_datetime_format=True)\n",
        "                    if target_pd_type.endswith(\"UTC]\"):\n",
        "                        # Localize timezone-naive datetimes before converting to UTC\n",
        "                        converted_series = converted_series.apply(lambda x: x.tz_localize('UTC') if x.tzinfo is None else x)\n",
        "                        converted_series = converted_series.dt.tz_convert('UTC')\n",
        "                    df_validated[col_name] = converted_series\n",
        "                elif target_pd_type == \"Int64\":\n",
        "                    # Coerce non-numeric to NaN before converting to Int64\n",
        "                    converted_series = pd.to_numeric(current_series, errors='coerce')\n",
        "                    df_validated[col_name] = converted_series.astype(target_pd_type)\n",
        "                elif target_pd_type == \"boolean\":\n",
        "                    # Handle various boolean representations\n",
        "                    bool_map = {'true': True, 'false': False, '1': True, '0': False, 'yes': True, 'no': False, 't': True, 'f': False, 'y': True, 'n': False, 1: True, 0: False, 1.0: True, 0.0: False}\n",
        "                    # Convert to string, lower, then map, handling potential NA values correctly\n",
        "                    df_validated[col_name] = current_series.astype(str).str.lower().map(bool_map).astype(target_pd_type)\n",
        "                else: # float, string, object\n",
        "                    df_validated[col_name] = current_series.astype(target_pd_type)\n",
        "\n",
        "                # Check if coercion introduced more nulls (indicating failure)\n",
        "                if df_validated[col_name].isnull().sum() > original_null_count:\n",
        "                    errors.append(f\"Type coercion failed for some values in '{col_name}' (Expected BQ Type: {target_bq_type}). Check data format.\")\n",
        "            except Exception as e:\n",
        "                errors.append(f\"Type conversion error in column '{col_name}' (Expected BQ Type: {target_bq_type}): {e}\")\n",
        "        else:\n",
        "            warnings.append(f\"Unsupported BQ type '{target_bq_type}' for validation in column '{col_name}'. Skipping type check.\")\n",
        "\n",
        "    # Check for nulls in required columns AFTER type coercion\n",
        "    for col_name, field in schema_dict.items():\n",
        "        if col_name in df_validated.columns and field.mode == 'REQUIRED' and df_validated[col_name].isnull().any():\n",
        "            errors.append(f\"REQUIRED column '{col_name}' contains null/NA values after processing.\")\n",
        "\n",
        "    is_valid = not errors\n",
        "    if not is_valid:\n",
        "        logger.error(f\"Schema validation errors for {file_path}:\\n\" + \"\\n\".join(errors))\n",
        "    else:\n",
        "        logger.debug(f\"Schema validation successful for {file_path}.\")\n",
        "    return is_valid, df_validated, errors\n",
        "\n",
        "# --- Core Logic Functions (Callable by ADK Agent/Tool - Reworked) ---\n",
        "\n",
        "async def read_data_source_reformed(source_type: str, source_uri_or_query: str, config: EnhancedConfig, bq_schema: Optional[List[bigquery.SchemaField]] = None) -> Optional[pd.DataFrame]:\n",
        "    \"\"\" Reads data from GCS (CSV, JSONL, Parquet) or BQ asynchronously. Includes validation for CSV. \"\"\"\n",
        "    if not config or not config.gcp or not config.gcp.project_id:\n",
        "        logger.critical(\"GCP configuration (project_id) is missing. Cannot read data.\")\n",
        "        return None\n",
        "    if not GCP_SDK_AVAILABLE:\n",
        "        logger.error(\"GCP SDKs are unavailable. Cannot read data.\")\n",
        "        return None\n",
        "\n",
        "    project_id = config.gcp.project_id\n",
        "    bucket_name = config.gcp.gcs_bucket_name\n",
        "    start_time = time.monotonic()\n",
        "    logger.info(f\"Reading source. Type='{source_type}', Source='{source_uri_or_query[:100]}...'\")\n",
        "    df = None\n",
        "\n",
        "    if source_type == 'gcs':\n",
        "        if not bucket_name:\n",
        "            logger.error(\"GCS Bucket name missing in config.\")\n",
        "            return None\n",
        "        gcs_path = source_uri_or_query if source_uri_or_query.startswith(\"gs://\") else f\"gs://{bucket_name}/{source_uri_or_query.lstrip('/')}\"\n",
        "        actual_bucket_name = gcs_path.split('/')[2]\n",
        "        blob_name = '/'.join(gcs_path.split('/')[3:])\n",
        "        ext = os.path.splitext(gcs_path.lower())[1]\n",
        "        logger.info(f\"Target GCS path: {gcs_path} (Bucket: {actual_bucket_name}, Blob: {blob_name})\")\n",
        "\n",
        "        try:\n",
        "            storage_client = storage.Client(project=project_id)\n",
        "            bucket = storage_client.bucket(actual_bucket_name)\n",
        "            blob = bucket.blob(blob_name)\n",
        "\n",
        "            # Check blob existence asynchronously\n",
        "            loop = asyncio.get_running_loop()\n",
        "            exists = await loop.run_in_executor(None, blob.exists)\n",
        "            if not exists:\n",
        "                raise FileNotFoundError(f\"GCS object not found: {gcs_path}\")\n",
        "\n",
        "            # --- Read content bytes asynchronously ---\n",
        "            content_bytes = None\n",
        "            read_start_time = time.monotonic()\n",
        "            try:\n",
        "                # Use aio-gcsfs if available (preferred)\n",
        "                if AIO_GCS_AVAILABLE:\n",
        "                    logger.debug(\"Using aio-gcsfs for GCS read.\")\n",
        "                    fs = aio_gcsfs.GCSFileSystem(project=project_id)\n",
        "                    async with fs.open(gcs_path, 'rb') as f:\n",
        "                        content_bytes = await f.read()\n",
        "                # Fallback: Use standard client in thread\n",
        "                else:\n",
        "                    logger.warning(\"aio-gcsfs not available. Using sync GCS read in thread (less efficient).\")\n",
        "                    def _sync_read_gcs():\n",
        "                        return blob.download_as_bytes()\n",
        "                    content_bytes = await loop.run_in_executor(None, _sync_read_gcs)\n",
        "                read_duration = (time.monotonic() - read_start_time) * 1000\n",
        "                logger.info(f\"Read {len(content_bytes or b'0')} bytes from GCS in {read_duration:.2f} ms.\")\n",
        "            except Exception as read_err:\n",
        "                logger.error(f\"Failed to read GCS object {gcs_path}: {read_err}\", exc_info=True)\n",
        "                return None\n",
        "            # --- End Read content bytes ---\n",
        "\n",
        "            if content_bytes is None: return None\n",
        "\n",
        "            # --- Parse content based on extension ---\n",
        "            parse_start_time = time.monotonic()\n",
        "            if ext == '.csv':\n",
        "                logger.info(f\"Parsing CSV file: {gcs_path}\")\n",
        "                def _parse_csv_sync(): # Sync function for executor\n",
        "                    try:\n",
        "                        # Detect encoding robustly\n",
        "                        detected = chardet.detect(content_bytes[:10000]) # Check first 10k bytes\n",
        "                        encoding = detected['encoding'] if detected['encoding'] and detected['confidence'] > 0.7 else 'utf-8'\n",
        "                        logger.info(f\"Detected encoding for {gcs_path}: {encoding} (Confidence: {detected['confidence']:.2f})\")\n",
        "                        try:\n",
        "                            # Use BytesIO for pandas\n",
        "                            df_csv = pd.read_csv(io.BytesIO(content_bytes), encoding=encoding, on_bad_lines='warn', quoting=csv.QUOTE_MINIMAL, low_memory=False)\n",
        "                        except UnicodeDecodeError:\n",
        "                            logger.warning(f\"Decode failed with {encoding}, trying utf-8 as fallback.\")\n",
        "                            df_csv = pd.read_csv(io.BytesIO(content_bytes), encoding='utf-8', on_bad_lines='warn', quoting=csv.QUOTE_MINIMAL, low_memory=False)\n",
        "                        if df_csv.empty:\n",
        "                            logger.warning(f\"CSV {gcs_path} is empty or failed parse.\")\n",
        "                            return None\n",
        "                        # Perform validation if schema provided\n",
        "                        if bq_schema:\n",
        "                            is_valid, df_validated, _ = validate_dataframe_schema(df_csv, bq_schema, gcs_path)\n",
        "                            if not is_valid:\n",
        "                                logger.error(f\"Schema validation failed for {gcs_path}. Skipping.\")\n",
        "                                return None\n",
        "                            return df_validated\n",
        "                        else:\n",
        "                            return clean_generic_data(df_csv) # Generic cleaning if no schema\n",
        "                    except Exception as e_inner:\n",
        "                        logger.error(f\"Error parsing CSV {gcs_path} in thread: {e_inner}\", exc_info=True)\n",
        "                        return None\n",
        "                df = await loop.run_in_executor(None, _parse_csv_sync)\n",
        "\n",
        "            elif ext in ['.jsonl', '.json']:\n",
        "                logger.info(f\"Parsing JSON(L) file: {gcs_path}\")\n",
        "                try:\n",
        "                    df = pd.read_json(io.BytesIO(content_bytes), lines=(ext == '.jsonl'), encoding='utf-8')\n",
        "                    df = clean_generic_data(df)\n",
        "                except Exception as json_e:\n",
        "                    logger.error(f\"Failed to parse JSON(L) file {gcs_path}: {json_e}\", exc_info=True)\n",
        "                    return None\n",
        "            elif ext == '.parquet':\n",
        "                logger.info(f\"Parsing Parquet file: {gcs_path}\")\n",
        "                try:\n",
        "                    df = pd.read_parquet(io.BytesIO(content_bytes))\n",
        "                    df = clean_generic_data(df)\n",
        "                except Exception as pq_e:\n",
        "                    logger.error(f\"Failed to parse Parquet file {gcs_path}: {pq_e}\", exc_info=True)\n",
        "                    return None\n",
        "            else:\n",
        "                logger.warning(f\"Unsupported GCS file extension: {ext}\")\n",
        "                return None\n",
        "\n",
        "            parse_duration = (time.monotonic() - parse_start_time) * 1000\n",
        "            if df is not None: logger.info(f\"Parsed GCS data ({df.shape=}) in {parse_duration:.2f} ms.\")\n",
        "            # --- End Parse content ---\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"GCS file/object not found: {gcs_path}\")\n",
        "            return None\n",
        "        except gcp_exceptions.GoogleAPIError as api_e:\n",
        "             logger.error(f\"GCP API error accessing GCS {gcs_path}: {api_e}\", exc_info=True)\n",
        "             return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing GCS source {gcs_path}: {e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    elif source_type == 'bq':\n",
        "        logger.info(f\"Reading BigQuery: {source_uri_or_query[:150]}...\")\n",
        "        try:\n",
        "            bq_client = bigquery.Client(project=project_id)\n",
        "            # Run sync BQ client call in a thread\n",
        "            def query_bq_sync():\n",
        "                # Consider adding query parameters for security if query is dynamic\n",
        "                return bq_client.query(source_uri_or_query).to_dataframe(create_bqstorage_client=True) # Use Storage API for speed\n",
        "            df = await asyncio.to_thread(query_bq_sync)\n",
        "            df = clean_generic_data(df) # Generic cleaning\n",
        "        except gcp_exceptions.GoogleAPIError as api_e:\n",
        "             logger.error(f\"GCP API error querying BigQuery: {api_e}\", exc_info=True)\n",
        "             return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading BQ: {e}\", exc_info=True)\n",
        "            return None\n",
        "    else:\n",
        "        logger.error(f\"Unsupported data source type: {source_type}\")\n",
        "        return None\n",
        "\n",
        "    if df is None:\n",
        "        logger.warning(f\"No DataFrame produced for source: {source_uri_or_query}\")\n",
        "        return None\n",
        "\n",
        "    total_duration = (time.monotonic() - start_time) * 1000\n",
        "    logger.info(f\"Successfully processed source ({df.shape=}) in {total_duration:.2f} ms.\")\n",
        "    return df\n",
        "\n",
        "# --- Transformation Helpers (Remain sync, same as before, but ensure robustness) ---\n",
        "def _add_entity(entities_list: List[Dict], entity_dict: Dict) -> bool:\n",
        "    \"\"\"Safely adds a validated entity dictionary to the list.\"\"\"\n",
        "    if not isinstance(entity_dict, dict): logger.warning(f\"Skipping non-dict entity: {type(entity_dict)}\"); return False\n",
        "    if not entity_dict.get('type') or not entity_dict.get('_resolution_hints'):\n",
        "        logger.warning(f\"Skipping entity: missing 'type' or '_resolution_hints': {str(entity_dict)[:150]}...\")\n",
        "        return False\n",
        "    cleaned_entity = {}\n",
        "    for k, v in entity_dict.items():\n",
        "        if pd.isna(v): continue # Skip NaN/NA values\n",
        "        # Convert numpy types to standard Python types\n",
        "        if isinstance(v, (np.integer, np.int64)): v = int(v)\n",
        "        elif isinstance(v, (np.floating, np.float64)):\n",
        "            if np.isnan(v) or np.isinf(v): continue # Skip NaN/inf floats\n",
        "            v = float(v)\n",
        "        elif isinstance(v, (datetime.datetime, datetime.date, pd.Timestamp)):\n",
        "            try: v = pd.to_datetime(v).isoformat() # Ensure ISO format\n",
        "            except Exception: v = str(v) # Fallback to string if conversion fails\n",
        "        elif isinstance(v, (list, dict)):\n",
        "             try: json.dumps(v, default=str); # Test serializability\n",
        "             except TypeError: v = str(v); logger.debug(f\"Converted unserializable {type(v)} to string for key '{k}'.\")\n",
        "        elif not isinstance(v, (str, int, float, bool)):\n",
        "             v = str(v) # Convert other types to string as fallback\n",
        "\n",
        "        # Final check for NaN floats that might have slipped through\n",
        "        if isinstance(v, float) and np.isnan(v): continue\n",
        "\n",
        "        cleaned_entity[k] = v\n",
        "    entities_list.append(cleaned_entity)\n",
        "    return True\n",
        "\n",
        "def _add_relationship(relationships_list: List[Dict], rel_dict: Dict) -> bool:\n",
        "    \"\"\"Safely adds a validated relationship dictionary to the list.\"\"\"\n",
        "    if not isinstance(rel_dict, dict): logger.warning(f\"Skipping non-dict relationship: {type(rel_dict)}\"); return False\n",
        "    if not rel_dict.get('type') or not rel_dict.get('source_hints') or not rel_dict.get('target_hints'):\n",
        "        logger.warning(f\"Skipping relationship: missing 'type', 'source_hints', or 'target_hints': {str(rel_dict)[:150]}...\")\n",
        "        return False\n",
        "    cleaned_rel = {}\n",
        "    for k, v in rel_dict.items():\n",
        "        if pd.isna(v): continue\n",
        "        if isinstance(v, (np.integer, np.int64)): v = int(v)\n",
        "        elif isinstance(v, (np.floating, np.float64)):\n",
        "            if np.isnan(v) or np.isinf(v): continue\n",
        "            v = float(v)\n",
        "        elif isinstance(v, (datetime.datetime, datetime.date, pd.Timestamp)):\n",
        "            try: v = pd.to_datetime(v).isoformat()\n",
        "            except Exception: v = str(v)\n",
        "        elif isinstance(v, (list, dict)):\n",
        "             try: json.dumps(v, default=str);\n",
        "             except TypeError: v = str(v); logger.debug(f\"Converted unserializable {type(v)} to string for key '{k}'.\")\n",
        "        elif not isinstance(v, (str, int, float, bool)):\n",
        "             v = str(v)\n",
        "        if isinstance(v, float) and np.isnan(v): continue\n",
        "        cleaned_rel[k] = v\n",
        "    relationships_list.append(cleaned_rel)\n",
        "    return True\n",
        "\n",
        "def _safe_get(row: Union[pd.Series, Dict], key: Optional[str], default: Any = None) -> Any:\n",
        "    \"\"\"Safely get value from dict or Series, handling NA/None.\"\"\"\n",
        "    if key is None: return default\n",
        "    val = default\n",
        "    try:\n",
        "        if isinstance(row, dict): val = row.get(key, default)\n",
        "        elif isinstance(row, pd.Series): val = row.get(key, default)\n",
        "        elif hasattr(row, key): val = getattr(row, key, default) # For objects\n",
        "        else: return default\n",
        "        # Return default if value is NaN, None, or NaT\n",
        "        return default if pd.isna(val) else val\n",
        "    except (TypeError, KeyError, AttributeError):\n",
        "        return default\n",
        "\n",
        "def _parse_date(date_str: Any, default: Optional[str] = None) -> Optional[str]:\n",
        "    \"\"\"Parse date string robustly into ISO format.\"\"\"\n",
        "    if date_str is None or pd.isna(date_str) or str(date_str).strip() == '': return default\n",
        "    try:\n",
        "        # Attempt parsing with pandas, coercing errors\n",
        "        dt_obj = pd.to_datetime(date_str, errors='coerce', infer_datetime_format=True)\n",
        "        # Return ISO format if valid, otherwise return original string (or default)\n",
        "        return dt_obj.isoformat() if pd.notna(dt_obj) else str(date_str)\n",
        "    except Exception:\n",
        "        # Fallback to string representation if parsing fails unexpectedly\n",
        "        return str(date_str)\n",
        "\n",
        "def map_to_funnel_stage(event: str, funnel_stages_config: Dict[str, List[str]]) -> Optional[str]:\n",
        "    \"\"\"Maps an event name to a funnel stage based on config.\"\"\"\n",
        "    if not event or not isinstance(event, str) or not funnel_stages_config: return None\n",
        "    event_lower = event.lower().strip()\n",
        "    for stage, events in funnel_stages_config.items():\n",
        "        if isinstance(events, list) and event_lower in [e.lower().strip() for e in events]:\n",
        "            return stage\n",
        "    return None # Return None if no match found\n",
        "\n",
        "# --- Platform Specific Transformations (Stubs with Guidance - Reworked Signatures) ---\n",
        "\n",
        "def _transform_facebook_ads_reformed(df: pd.DataFrame, config: EnhancedConfig) -> Tuple[List[Dict], List[Dict]]:\n",
        "    entities = []; relationships = []; errors = []\n",
        "    logger.info(f\"Running Facebook Ads transformation on {len(df)} rows...\")\n",
        "    required_cols = ['campaign_id', 'ad_id', 'date', 'spend', 'impressions', 'clicks'] # Example required\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    if missing_cols: raise ValueError(f\"Missing required FB Ads columns: {missing_cols}\")\n",
        "\n",
        "    # --- Placeholder: IMPLEMENTATION NEEDED ---\n",
        "    # Iterate through df rows (df.itertuples() is faster)\n",
        "    # Extract IDs, create hints, create entity/relationship dicts using _safe_get, _parse_date\n",
        "    # Use _add_entity and _add_relationship\n",
        "    # Log errors encountered during row processing\n",
        "    # Example (Partial):\n",
        "    # for row in df.itertuples(index=False):\n",
        "    #     try:\n",
        "    #         campaign_id = str(_safe_get(row, 'campaign_id')); ad_id = str(_safe_get(row, 'ad_id'))\n",
        "    #         camp_hints = {\"platform\": \"facebook\", \"type\": \"Campaign\", \"original_id\": campaign_id}\n",
        "    #         ad_hints = {\"platform\": \"facebook\", \"type\": \"Ad\", \"original_id\": ad_id}\n",
        "    #         # ... create entity/rel dicts ...\n",
        "    #         _add_entity(entities, campaign_data)\n",
        "    #         _add_entity(entities, ad_data)\n",
        "    #         _add_relationship(relationships, rel_data)\n",
        "    #     except Exception as e: errors.append(...)\n",
        "    # --- End Placeholder ---\n",
        "\n",
        "    logger.info(f\"FB Ads transform yielded {len(entities)} entities, {len(relationships)} relationships, {len(errors)} row errors.\")\n",
        "    if errors: logger.warning(f\"FB Ads transform errors: {errors[:5]}\") # Log first few errors\n",
        "    return entities, relationships\n",
        "\n",
        "def _transform_shopify_reformed(df: pd.DataFrame, config: EnhancedConfig) -> Tuple[List[Dict], List[Dict]]:\n",
        "    entities = []; relationships = []; errors = []\n",
        "    logger.info(f\"Running Shopify transformation on {len(df)} rows...\")\n",
        "    required_cols = ['order_id', 'customer_id', 'created_at', 'total_price', 'email'] # Example\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    if missing_cols: raise ValueError(f\"Missing required Shopify columns: {missing_cols}\")\n",
        "    # Access funnel stages config safely\n",
        "    funnel_stages_config = config.get(\"business_impact.funnel_stages\", {}) if config else {}\n",
        "\n",
        "    # --- Placeholder: IMPLEMENTATION NEEDED ---\n",
        "    # Iterate, extract, create hints, create dicts, use helpers _add_entity, _add_relationship\n",
        "    # Use map_to_funnel_stage for events\n",
        "    # --- End Placeholder ---\n",
        "\n",
        "    logger.info(f\"Shopify transform yielded {len(entities)} entities, {len(relationships)} relationships, {len(errors)} row errors.\")\n",
        "    if errors: logger.warning(f\"Shopify transform errors: {errors[:5]}\")\n",
        "    return entities, relationships\n",
        "\n",
        "def _transform_ga4_reformed(df: pd.DataFrame, config: EnhancedConfig) -> Tuple[List[Dict], List[Dict]]:\n",
        "    entities = []; relationships = []; errors = []\n",
        "    logger.info(f\"Running GA4 transformation on {len(df)} rows...\")\n",
        "    required_cols = ['event_name', 'user_pseudo_id', 'event_timestamp'] # Example\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    if missing_cols: raise ValueError(f\"Missing required GA4 columns: {missing_cols}\")\n",
        "    funnel_stages_config = config.get(\"business_impact.funnel_stages\", {}) if config else {}\n",
        "\n",
        "    # --- Placeholder: IMPLEMENTATION NEEDED ---\n",
        "    # Iterate, extract, create hints, create dicts, use helpers _add_entity, _add_relationship\n",
        "    # Use map_to_funnel_stage for events\n",
        "    # --- End Placeholder ---\n",
        "\n",
        "    logger.info(f\"GA4 transform yielded {len(entities)} entities, {len(relationships)} relationships, {len(errors)} row errors.\")\n",
        "    if errors: logger.warning(f\"GA4 transform errors: {errors[:5]}\")\n",
        "    return entities, relationships\n",
        "\n",
        "# --- TODO: Implement other _transform_* functions similarly ---\n",
        "def _transform_google_ads_reformed(df: pd.DataFrame, config: EnhancedConfig) -> Tuple[List[Dict], List[Dict]]:\n",
        "    logger.warning(\"Google Ads transform not implemented.\"); return [], []\n",
        "def _transform_klaviyo_reformed(df: pd.DataFrame, config: EnhancedConfig) -> Tuple[List[Dict], List[Dict]]:\n",
        "    logger.warning(\"Klaviyo transform not implemented.\"); return [], []\n",
        "def _transform_support_logs_reformed(df: pd.DataFrame, config: EnhancedConfig) -> Tuple[List[Dict], List[Dict]]:\n",
        "    logger.warning(\"Support Logs transform (likely semantic) not implemented.\"); return [], []\n",
        "\n",
        "def _transform_generic_reformed(df: pd.DataFrame, data_type: str, config: EnhancedConfig) -> Tuple[List[Dict], List[Dict]]:\n",
        "    \"\"\"Generic transformation for unknown structured data types.\"\"\"\n",
        "    entities = []; relationships = []; errors = []; id_col = None\n",
        "    if df.empty: return [], []\n",
        "    common_id_names = ['id', 'uuid', 'key', 'identifier', 'record_id', 'objectid', 'pk']\n",
        "    df_cols_lower = {c.lower(): c for c in df.columns}\n",
        "    for potential_id in common_id_names:\n",
        "        if potential_id in df_cols_lower: id_col = df_cols_lower[potential_id]; break\n",
        "    if not id_col and len(df.columns) > 0: id_col = df.columns[0] # Fallback to first column\n",
        "\n",
        "    if id_col:\n",
        "         logger.info(f\"Using column '{id_col}' as primary identifier for generic transform of '{data_type}'.\")\n",
        "         for index, row in df.iterrows():\n",
        "              row_info = f\"Row Index: {index}\"\n",
        "              try:\n",
        "                  original_id_val = _safe_get(row, id_col)\n",
        "                  original_id = str(original_id_val) if original_id_val is not None and str(original_id_val).strip() != '' else f\"generic_{data_type}_{index}\"\n",
        "                  hints = {\"data_type\": data_type, \"type\": \"GenericData\", \"original_id\": original_id}\n",
        "                  # Include all non-null properties\n",
        "                  entity_data = {\"type\": \"GenericData\", \"_resolution_hints\": hints, \"original_id\": original_id}\n",
        "                  for col, val in row.items():\n",
        "                      if pd.notna(val): entity_data[col] = val # Let _add_entity handle type conversion\n",
        "                  _add_entity(entities, entity_data)\n",
        "              except Exception as e: errors.append({\"row_info\": row_info, \"error\": str(e)})\n",
        "    else: logger.error(f\"Cannot perform generic transform for '{data_type}': No suitable ID column found.\")\n",
        "    logger.info(f\"Generic Transform for '{data_type}' yielded {len(entities)} entities, {len(errors)} row errors.\")\n",
        "    if errors: logger.warning(f\"Generic transform errors: {errors[:5]}\")\n",
        "    return entities, relationships\n",
        "\n",
        "# --- Main Transformation Function (Reworked for MIZ OKI I/O) ---\n",
        "async def extract_and_transform_for_kg_reformed(input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Reads, validates, transforms data into KG-ready format asynchronously.\n",
        "    Expects input_data to be a dict representing the MIZ OKI payload from a workflow step.\n",
        "    Returns a dict representing the MIZ OKI response payload.\n",
        "    \"\"\"\n",
        "    # --- MIZ OKI Payload Parsing ---\n",
        "    miz_oki_version = input_data.get(\"miz_oki_version\", \"unknown\")\n",
        "    request_id = input_data.get(\"request_id\", f\"req_{uuid.uuid4().hex[:8]}\")\n",
        "    trace_id = input_data.get(\"trace_id\", f\"trace_{uuid.uuid4().hex[:8]}\")\n",
        "    workflow_execution_id = input_data.get(\"workflow_execution_id\")\n",
        "    step_id = input_data.get(\"step_id\")\n",
        "    source_component = input_data.get(\"source_component\")\n",
        "    payload = input_data.get(\"payload\", {})\n",
        "    source_type = payload.get(\"source_type\") # e.g., 'gcs', 'bq'\n",
        "    source_uri_or_query = payload.get(\"source_uri_or_query\") # e.g., 'gs://bucket/file.csv', 'SELECT * ...'\n",
        "    data_type_hint = payload.get(\"data_type_hint\") # e.g., 'facebook_ads', 'shopify', 'ga4'\n",
        "    bq_schema_list = payload.get(\"bq_schema\") # Optional: List of dicts for schema validation\n",
        "\n",
        "    task_id = f\"extract_{uuid.uuid4().hex[:8]}\"\n",
        "    start_time_task = time.monotonic()\n",
        "    logger.info(f\"Starting KG extraction/transformation async (TaskID: {task_id}, TraceID: {trace_id}, WorkflowExec: {workflow_execution_id}, Step: {step_id}): Type='{source_type}', Source='{source_uri_or_query[:100]}...'\")\n",
        "\n",
        "    # --- Prepare MIZ OKI Response Structure ---\n",
        "    response = {\n",
        "        \"miz_oki_version\": _config_obj.miz_oki_schema_version if _config_obj else \"unknown\",\n",
        "        \"request_id\": request_id, \"trace_id\": trace_id, \"workflow_execution_id\": workflow_execution_id, \"step_id\": step_id,\n",
        "        \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "        \"source_component\": \"DataExtractionTool\", # This component's name\n",
        "        \"target_component\": source_component, # Responding to the caller\n",
        "        \"status\": \"unknown\", # Will be updated: success, partial_success, failed_validation, skipped_no_data, failed_transform, critical_failure\n",
        "        \"payload\": {\"kg_entities\": [], \"kg_relationships\": [], \"log\": {}},\n",
        "        \"error_details\": None,\n",
        "        \"metadata\": {}\n",
        "    }\n",
        "    run_log = response[\"payload\"][\"log\"] # Reference for easier logging\n",
        "    run_log.update({\"task_id\": task_id, \"source_type\": source_type, \"source\": source_uri_or_query, \"status\": \"started\", \"start_time_iso\": response[\"timestamp\"], \"steps\": {}})\n",
        "    errors = [] # Local error list for non-critical issues\n",
        "\n",
        "    # --- Dependency Check ---\n",
        "    if not _config_obj or not _fm_client_tool:\n",
        "        error_msg = \"Critical configuration or FM Client Tool dependency missing.\"\n",
        "        logger.critical(f\"Task {task_id}: {error_msg}\")\n",
        "        response[\"status\"] = \"critical_failure\"; response[\"error_details\"] = [{\"code\": \"DEPENDENCY_MISSING\", \"message\": error_msg}]\n",
        "        run_log[\"status\"] = \"critical_failure\"; run_log[\"error\"] = error_msg\n",
        "        run_log[\"total_duration_ms\"] = (time.monotonic() - start_time_task) * 1000\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = run_log[\"total_duration_ms\"]\n",
        "        return response\n",
        "    # --- End Dependency Check ---\n",
        "\n",
        "    try:\n",
        "        if not source_type or not source_uri_or_query:\n",
        "            raise ValueError(\"Missing 'source_type' or 'source_uri_or_query' in input payload.\")\n",
        "\n",
        "        # Convert BQ schema dict list to SchemaField objects if provided\n",
        "        bq_schema_obj: Optional[List[bigquery.SchemaField]] = None\n",
        "        if bq_schema_list and isinstance(bq_schema_list, list) and GCP_SDK_AVAILABLE and hasattr(bigquery, 'SchemaField'):\n",
        "            try:\n",
        "                bq_schema_obj = [bigquery.SchemaField.from_api_repr(field_dict) for field_dict in bq_schema_list]\n",
        "                logger.debug(f\"Task {task_id}: Successfully parsed BQ schema.\")\n",
        "            except Exception as schema_e:\n",
        "                logger.warning(f\"Task {task_id}: Failed to parse BQ schema: {schema_e}. Proceeding without schema validation for CSV.\")\n",
        "                errors.append({\"code\": \"SCHEMA_PARSE_ERROR\", \"message\": f\"Failed to parse BQ schema: {schema_e}\"})\n",
        "\n",
        "        # 1. Read Data Async\n",
        "        step_start = time.monotonic()\n",
        "        df = await read_data_source_reformed(source_type, source_uri_or_query, _config_obj, bq_schema_obj)\n",
        "        read_duration = (time.monotonic() - step_start) * 1000\n",
        "        run_log[\"steps\"][\"read_data\"] = {\"duration_ms\": read_duration, \"source_type\": source_type}\n",
        "\n",
        "        if df is None: # Handles read errors OR validation failures during read\n",
        "            response[\"status\"] = \"failed_validation\" if bq_schema_obj and source_type == 'gcs' and source_uri_or_query.endswith('.csv') else \"skipped_no_data\"\n",
        "            run_log[\"status\"] = response[\"status\"]\n",
        "            run_log[\"steps\"][\"read_data\"][\"status\"] = \"no_data_or_validation_failed\"\n",
        "            error_msg = \"No data read from source or schema validation failed during read.\"\n",
        "            logger.warning(f\"Task {task_id}: {error_msg}\")\n",
        "            errors.append({\"code\": \"NO_DATA_OR_VALIDATION\", \"message\": error_msg})\n",
        "            # No critical failure here, just no data to process\n",
        "        elif df.empty:\n",
        "            response[\"status\"] = \"skipped_no_data\"\n",
        "            run_log[\"status\"] = response[\"status\"]\n",
        "            run_log[\"steps\"][\"read_data\"][\"status\"] = \"empty_dataframe\"\n",
        "            logger.info(f\"Task {task_id}: Source contained no data rows.\")\n",
        "        else:\n",
        "            run_log[\"rows_read\"] = len(df)\n",
        "            run_log[\"steps\"][\"read_data\"][\"rows_read\"] = len(df)\n",
        "            run_log[\"steps\"][\"read_data\"][\"status\"] = \"success\"\n",
        "            logger.info(f\"Task {task_id}: Read {len(df)} rows in {read_duration:.2f} ms.\")\n",
        "\n",
        "            # 2. Determine Data Type\n",
        "            data_type = data_type_hint\n",
        "            if not data_type: # Auto-detect based on URI/query content\n",
        "                src_lower = source_uri_or_query.lower()\n",
        "                if 'facebook' in src_lower or 'meta' in src_lower: data_type = 'facebook_ads'\n",
        "                elif 'shopify' in src_lower: data_type = 'shopify'\n",
        "                elif 'googleads' in src_lower or 'adwords' in src_lower: data_type = 'google_ads'\n",
        "                elif 'analytics' in src_lower or 'ga4' in src_lower: data_type = 'ga4'\n",
        "                elif 'klaviyo' in src_lower: data_type = 'klaviyo'\n",
        "                elif 'support' in src_lower or 'ticket' in src_lower or 'log' in src_lower: data_type = 'support_logs' # Could be semantic\n",
        "                # Add more specific detections\n",
        "                else: data_type = \"generic\"; logger.warning(f\"Task {task_id}: Could not auto-detect data type from '{source_uri_or_query[:50]}...'. Defaulting to 'generic'.\")\n",
        "            run_log[\"data_type\"] = data_type\n",
        "            run_log[\"steps\"][\"validation\"] = {\"data_type\": data_type, \"status\": \"performed_during_read\" if source_type=='gcs' and source_uri_or_query.endswith('.csv') and bq_schema_obj else \"skipped_no_schema_or_not_csv\"}\n",
        "            df_validated = df # Already validated or cleaned during read\n",
        "\n",
        "            # 3. Transform Data\n",
        "            step_start = time.monotonic()\n",
        "            entities, relationships = [], []\n",
        "            # Define types likely needing semantic processing\n",
        "            unstructured_types = {'text_log', 'support_transcript', 'external_news', 'returns_data', 'web_page_content', 'support_logs'}\n",
        "            use_semantic = (data_type in unstructured_types)\n",
        "            run_log[\"steps\"][\"transform\"] = {\"type\": \"semantic\" if use_semantic else \"rule_based\", \"status\": \"pending\"}\n",
        "\n",
        "            if use_semantic:\n",
        "                logger.info(f\"Task {task_id}: Applying SEMANTIC processing via FMClientTool API for: {data_type}\")\n",
        "                model_alias = _config_obj.foundation_models.defaults.llama4_maverick # Or choose based on data_type\n",
        "                # --- Semantic processing logic (using _fm_client_tool proxy) ---\n",
        "                text_col = 'text_content' if 'text_content' in df_validated.columns else 'content' if 'content' in df_validated.columns else None # Find text column\n",
        "                if text_col and not df_validated.empty:\n",
        "                    batch_size = 10 # Adjust batch size based on API limits and content size\n",
        "                    extraction_tasks = []\n",
        "                    # Ensure unique ID for context passing\n",
        "                    if 'miz_internal_id' not in df_validated.columns: df_validated['miz_internal_id'] = [f\"row_{i}\" for i in range(len(df_validated))]\n",
        "\n",
        "                    for i in range(0, len(df_validated), batch_size):\n",
        "                        batch_df = df_validated.iloc[i:i+batch_size]\n",
        "                        batch_content = batch_df[text_col].astype(str).tolist()\n",
        "                        batch_ids = batch_df['miz_internal_id'].astype(str).tolist()\n",
        "                        batch_tasks_inner = []\n",
        "                        for idx, content_item in enumerate(batch_content):\n",
        "                            if not content_item or pd.isna(content_item) or not str(content_item).strip(): continue\n",
        "                            original_id = batch_ids[idx]\n",
        "                            # Prepare MIZ OKI payload for FM Client Tool API call\n",
        "                            fm_request_payload = {\n",
        "                                \"payload\": {\n",
        "                                    \"content\": content_item,\n",
        "                                    \"data_type\": data_type,\n",
        "                                    \"model_alias\": model_alias,\n",
        "                                    \"context\": {\"original_id\": original_id, \"source_uri\": source_uri_or_query}\n",
        "                                },\n",
        "                                \"trace_id\": trace_id, \"request_id\": f\"{request_id}_fm_{original_id}\"\n",
        "                            }\n",
        "                            # Call FM Client Tool API (via proxy)\n",
        "                            task = _fm_client_tool.extract_kg_data_from_content(input_data=fm_request_payload)\n",
        "                            batch_tasks_inner.append(task)\n",
        "                        if batch_tasks_inner: extraction_tasks.append(asyncio.gather(*batch_tasks_inner, return_exceptions=True))\n",
        "\n",
        "                    if extraction_tasks:\n",
        "                        all_batch_results = await asyncio.gather(*extraction_tasks)\n",
        "                        for batch_results in all_batch_results:\n",
        "                            for fm_response in batch_results: # fm_response is the MIZ OKI response dict from the FM tool\n",
        "                                if isinstance(fm_response, Exception):\n",
        "                                    err_detail = f\"Semantic extraction API call failed: {fm_response}\"; logger.error(err_detail); errors.append({\"code\": \"SEMANTIC_API_ERROR\", \"message\": err_detail})\n",
        "                                elif isinstance(fm_response, dict) and fm_response.get(\"status\") == \"success\":\n",
        "                                     fm_payload = fm_response.get(\"payload\", {})\n",
        "                                     # Use helpers to add entities/rels, ensuring validation\n",
        "                                     for e_dict in fm_payload.get(\"entities\", []): _add_entity(entities, e_dict)\n",
        "                                     for r_dict in fm_payload.get(\"relationships\", []): _add_relationship(relationships, r_dict)\n",
        "                                elif isinstance(fm_response, dict): # Handle errors reported by FM tool\n",
        "                                     err_detail = f\"Semantic extraction failed in FM Tool: {fm_response.get('error_details')}\"\n",
        "                                     logger.error(err_detail); errors.append({\"code\": \"SEMANTIC_TOOL_ERROR\", \"message\": err_detail})\n",
        "                                else: logger.warning(f\"Task {task_id}: Invalid format received from semantic extraction API: {type(fm_response)}.\")\n",
        "                else: logger.warning(f\"Task {task_id}: Suitable text column ('{text_col}') not found or DataFrame empty for semantic extraction.\")\n",
        "                # --- End Semantic processing ---\n",
        "            else: # Rule-Based Transformation\n",
        "                 logger.info(f\"Task {task_id}: Applying RULE-BASED transformation for: {data_type}\")\n",
        "                 transform_func_map = {\n",
        "                     'facebook_ads': _transform_facebook_ads_reformed,\n",
        "                     'shopify': _transform_shopify_reformed,\n",
        "                     'ga4': _transform_ga4_reformed,\n",
        "                     'google_ads': _transform_google_ads_reformed,\n",
        "                     'klaviyo': _transform_klaviyo_reformed,\n",
        "                     'support_logs': _transform_support_logs_reformed, # Could also be semantic\n",
        "                     'generic': _transform_generic_reformed,\n",
        "                 }\n",
        "                 # Get the appropriate function, default to generic\n",
        "                 transform_func = transform_func_map.get(data_type, _transform_generic_reformed)\n",
        "                 try:\n",
        "                      loop = asyncio.get_running_loop()\n",
        "                      # Pass config object for access to parameters like funnel stages\n",
        "                      if transform_func == _transform_generic_reformed:\n",
        "                          # Generic function needs data_type explicitly\n",
        "                          func_args = (df_validated.copy(), data_type, _config_obj)\n",
        "                      else:\n",
        "                          func_args = (df_validated.copy(), _config_obj)\n",
        "                      # Run the synchronous transform function in a thread\n",
        "                      entities_t, relationships_t = await loop.run_in_executor(None, functools.partial(transform_func, *func_args))\n",
        "                      entities.extend(entities_t); relationships.extend(relationships_t)\n",
        "                 except Exception as transform_e:\n",
        "                      logger.error(f\"Task {task_id}: Critical failure during rule-based transformation '{data_type}': {transform_e}\", exc_info=True)\n",
        "                      errors.append({\"code\": \"TRANSFORM_CRITICAL\", \"message\": f\"Critical transform error: {transform_e}\"})\n",
        "                      response[\"status\"] = \"failed_transform\" # Mark as failed if transform crashes\n",
        "\n",
        "            run_log[\"steps\"][\"transform\"][\"duration_ms\"] = (time.monotonic() - step_start) * 1000\n",
        "            run_log[\"steps\"][\"transform\"][\"status\"] = \"success\" if response[\"status\"] != \"failed_transform\" else \"failed\"\n",
        "            run_log[\"entities_extracted\"] = len(entities)\n",
        "            run_log[\"relationships_extracted\"] = len(relationships)\n",
        "\n",
        "            # Final processing and status determination only if read was successful\n",
        "            response[\"payload\"][\"kg_entities\"] = entities\n",
        "            response[\"payload\"][\"kg_relationships\"] = relationships\n",
        "            entity_count = len(entities); relationship_count = len(relationships); error_count = len(errors)\n",
        "\n",
        "            if response[\"status\"] == \"unknown\": # Check if status was set during transformation errors\n",
        "                if error_count == 0:\n",
        "                    response[\"status\"] = \"success\" if entity_count > 0 or relationship_count > 0 else \"success_no_results\"\n",
        "                else:\n",
        "                    # Partial success if some entities/rels were generated despite row/API errors\n",
        "                    response[\"status\"] = \"partial_success\" if entity_count > 0 or relationship_count > 0 else \"failed_transform\"\n",
        "\n",
        "        # Update final status in log\n",
        "        run_log[\"status\"] = response[\"status\"]\n",
        "        run_log[\"error_count\"] = len(errors)\n",
        "        run_log[\"total_duration_ms\"] = (time.monotonic() - start_time_task) * 1000\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = run_log[\"total_duration_ms\"]\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "\n",
        "        logger.info(f\"Task {task_id} FINISHED. Status: {response['status']}. Entities: {run_log.get('entities_extracted', 0)}, Rels: {run_log.get('relationships_extracted', 0)}, Errors: {run_log.get('error_count', 0)}. Total Time: {run_log['total_duration_ms']:.2f} ms\")\n",
        "        return response\n",
        "\n",
        "    except Exception as outer_e:\n",
        "        # Catch any unexpected critical errors in the main flow\n",
        "        logger.critical(f\"CRITICAL failure in ETL Task (TaskID: {task_id}, TraceID: {trace_id}): {outer_e}\", exc_info=True)\n",
        "        response[\"status\"] = \"critical_failure\"\n",
        "        errors.append({\"code\": \"CRITICAL\", \"message\": str(outer_e)})\n",
        "        response[\"error_details\"] = errors\n",
        "        run_log[\"status\"] = \"critical_failure\"; run_log[\"error\"] = str(outer_e)\n",
        "        run_log[\"total_duration_ms\"] = (time.monotonic() - start_time_task) * 1000\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = run_log[\"total_duration_ms\"]\n",
        "        return response\n",
        "\n",
        "# --- Generic Data Cleaning Function (Improved) ---\n",
        "def clean_generic_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Applies generic cleaning to DataFrames: handles common nulls, attempts type inference.\"\"\"\n",
        "    if df.empty: return df\n",
        "    df_cleaned = df.copy()\n",
        "    logger.debug(f\"Starting generic cleaning for DataFrame with shape {df.shape}...\")\n",
        "    try:\n",
        "        # Define common null representations\n",
        "        null_values = ['null', 'NULL', '', '#N/A', 'N/A', 'NaN', 'nan', 'None', '<NA>', 'undefined', 'missing']\n",
        "        df_cleaned = df_cleaned.replace(null_values, pd.NA) # Use pandas NA consistently\n",
        "\n",
        "        for col in df_cleaned.columns:\n",
        "            original_dtype = df_cleaned[col].dtype\n",
        "            # Skip if already a specific desired type (like datetime from BQ)\n",
        "            if pd.api.types.is_datetime64_any_dtype(original_dtype) or pd.api.types.is_numeric_dtype(original_dtype) or pd.api.types.is_bool_dtype(original_dtype):\n",
        "                continue\n",
        "\n",
        "            # Attempt numeric conversion if object type\n",
        "            if pd.api.types.is_object_dtype(original_dtype) or pd.api.types.is_string_dtype(original_dtype):\n",
        "                try:\n",
        "                    # Try converting to numeric, coercing errors to NA\n",
        "                    converted_numeric = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "                    # If mostly numeric (e.g., >80% non-NA after conversion), keep it numeric\n",
        "                    if converted_numeric.notna().sum() / len(df_cleaned[col]) > 0.8:\n",
        "                        # Use Int64 if possible (no decimals and fits in int64)\n",
        "                        if converted_numeric.dropna().apply(lambda x: x == int(x) if pd.notna(x) else True).all():\n",
        "                             df_cleaned[col] = converted_numeric.astype(pd.Int64Dtype())\n",
        "                        else:\n",
        "                             df_cleaned[col] = converted_numeric.astype(pd.Float64Dtype())\n",
        "                        logger.debug(f\"Column '{col}': Inferred as numeric ({df_cleaned[col].dtype}).\")\n",
        "                        continue # Move to next column\n",
        "                except (ValueError, TypeError):\n",
        "                    pass # Ignore errors if conversion fails, proceed to other types\n",
        "\n",
        "            # Attempt datetime conversion if object type and not successfully converted to numeric\n",
        "            if pd.api.types.is_object_dtype(df_cleaned[col].dtype) or pd.api.types.is_string_dtype(df_cleaned[col].dtype):\n",
        "                try:\n",
        "                    converted_datetime = pd.to_datetime(df_cleaned[col], errors='coerce', infer_datetime_format=True)\n",
        "                    # If mostly datetime, keep it\n",
        "                    if converted_datetime.notna().sum() / len(df_cleaned[col]) > 0.8:\n",
        "                        df_cleaned[col] = converted_datetime\n",
        "                        logger.debug(f\"Column '{col}': Inferred as datetime.\")\n",
        "                        continue\n",
        "                except (ValueError, TypeError):\n",
        "                     pass # Ignore errors\n",
        "\n",
        "            # Attempt boolean conversion if object type and not converted yet\n",
        "            if pd.api.types.is_object_dtype(df_cleaned[col].dtype) or pd.api.types.is_string_dtype(df_cleaned[col].dtype):\n",
        "                 unique_vals = df_cleaned[col].dropna().unique()\n",
        "                 if len(unique_vals) <= 5: # Limit boolean check to few unique values\n",
        "                      lowered_unique = {str(v).lower() for v in unique_vals}\n",
        "                      bool_representations = {'true', 'false', '1', '0', 'yes', 'no', 't', 'f', 'y', 'n', '1.0', '0.0'}\n",
        "                      if lowered_unique.issubset(bool_representations):\n",
        "                           bool_map = {'true': True, 'false': False, '1': True, '0': False, 'yes': True, 'no': False, 't': True, 'f': False, 'y': True, 'n': False, '1.0': True, '0.0': False}\n",
        "                           df_cleaned[col] = df_cleaned[col].astype(str).str.lower().map(bool_map).astype(pd.BooleanDtype())\n",
        "                           logger.debug(f\"Column '{col}': Inferred as boolean.\")\n",
        "                           continue\n",
        "\n",
        "            # If still object/string, ensure it's nullable string type\n",
        "            if pd.api.types.is_object_dtype(df_cleaned[col].dtype) or pd.api.types.is_string_dtype(df_cleaned[col].dtype):\n",
        "                 df_cleaned[col] = df_cleaned[col].astype(pd.StringDtype())\n",
        "                 logger.debug(f\"Column '{col}': Kept as string.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during generic cleaning: {e}\", exc_info=True)\n",
        "        return df # Return original df on error\n",
        "    logger.debug(\"Generic cleaning finished.\")\n",
        "    return df_cleaned\n",
        "\n",
        "\n",
        "# --- Example Invocation (Conceptual - within a deployed service/function) ---\n",
        "# async def handle_workflow_request(request_json: Dict):\n",
        "#     # 1. Parse request_json (assume it's the MIZ OKI input_data dict)\n",
        "#     # 2. Call the main processing function\n",
        "#     response_dict = await extract_and_transform_for_kg_reformed(request_json)\n",
        "#     # 3. Return response_dict (e.g., as JSON response)\n",
        "#     return response_dict\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 Data Extraction Logic (Cell 2 - Reworked) ---\")\n",
        "print(\"Refactored as Tool/Service logic using MIZ OKI I/O structure.\")\n",
        "print(\"Uses async I/O and calls FM Client Tool API proxy.\")\n",
        "print(\"Requires implementation of specific _transform_* functions.\")\n",
        "print(\"-------------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "MtgsDfKVQcIk",
        "outputId": "b415b4a8-3e8e-4ee5-882d-6bba78dc186d"
      },
      "id": "MtgsDfKVQcIk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:aio-gcsfs/gcsfs not installed. GCS reads will use less efficient sync methods in threads.\n",
            "CRITICAL:root:CRITICAL DEPENDENCY ERROR (CONFIG_OBJ not found or is None). Cannot proceed with Cell 2 logic.\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-2-6d37864f042f>\", line 43, in <cell line: 41>\n",
            "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
            "NameError: CONFIG_OBJ not found or is None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'EnhancedConfig' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6d37864f042f>\u001b[0m in \u001b[0;36m<cell line: 141>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# --- Core Logic Functions (Callable by ADK Agent/Tool - Reworked) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data_source_reformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_uri_or_query\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEnhancedConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_schema\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchemaField\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;34m\"\"\" Reads data from GCS (CSV, JSONL, Parquet) or BQ asynchronously. Includes validation for CSV. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcp\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EnhancedConfig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "4. Updated Cells (Continued)\n",
        "\n",
        "Cell 3: Knowledge Graph Layer Implementation (Reworked)\n",
        "\n",
        "Original Purpose: Synchronous Neo4j adapter, no Vector DB, basic XAI storage, no MIZ OKI API layer.\n",
        "Key Changes: Neo4jAsyncAdapter uses async driver. Includes routing logic for Vector DB (Vertex AI Vector Search via client proxy). Implements structured XAI storage (save/retrieve_decision_record). Adds DataPseudonymizer. Wraps adapter logic in a conceptual KnowledgeGraphToolService with FastAPI-style endpoint methods (*_endpoint) handling MIZ OKI request/response payloads. All DB/API operations are now async. Added robust error handling and dependency checks.\n",
        "Reworked Code:\n",
        "# Cell 3: Knowledge Graph Layer Implementation (Reworked)\n",
        "# Status: Neo4jAsyncAdapter includes Vector DB routing & structured XAI storage.\n",
        "#         Conceptual FastAPI service layer added, handling MIZ OKI payloads & pseudonymization.\n",
        "#         Uses async operations for DB and external Vector DB calls.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import uuid\n",
        "import hashlib\n",
        "from collections import defaultdict, deque\n",
        "from contextlib import contextmanager, asynccontextmanager\n",
        "from typing import Dict, Any, Optional, List, Union, Tuple, Set, Type, Protocol, Callable\n",
        "from abc import ABC, abstractmethod\n",
        "import asyncio\n",
        "\n",
        "# --- Framework for API Service (Conceptual) ---\n",
        "# These imports are needed if deploying as a FastAPI service\n",
        "try:\n",
        "    from fastapi import FastAPI, HTTPException, Body, Depends, Request, Response, status\n",
        "    from pydantic import BaseModel, Field, validator\n",
        "    FASTAPI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FASTAPI_AVAILABLE = False\n",
        "    # Dummy classes if FastAPI not installed (won't run, just for structure)\n",
        "    def Depends(dep): return None; class FastAPI: pass; class HTTPException(Exception): pass; def Body(...): return ...; class BaseModel: pass; def Field(...): return ...; class Request: pass; class Response: pass; class status: pass\n",
        "    logging.warning(\"FastAPI or Pydantic not installed. API service layer cannot be fully defined.\")\n",
        "\n",
        "# --- Neo4j Integration ---\n",
        "try:\n",
        "    # Use the async driver\n",
        "    from neo4j import AsyncGraphDatabase, basic_auth, exceptions as neo4j_exceptions, AsyncSession, AsyncTransaction, AsyncDriver\n",
        "    NEO4J_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NEO4J_AVAILABLE = False\n",
        "    # Dummy classes if Neo4j async driver not installed\n",
        "    class AsyncDriver: pass; class AsyncSession: pass; class AsyncTransaction: pass; class AsyncGraphDatabase: pass\n",
        "    if 'neo4j_exceptions' not in locals(): class neo4j_exceptions: class ClientError(Exception): pass; class TransientError(Exception): pass; class AuthError(Exception): pass; class ServiceUnavailable(Exception): pass\n",
        "    logging.warning(\"Neo4j async driver not found. Neo4j functionality unavailable.\")\n",
        "\n",
        "# --- Vector DB Integration ---\n",
        "VECTOR_DB_AVAILABLE = False\n",
        "try:\n",
        "    # Using Vertex AI Vector Search client library\n",
        "    from google.cloud import aiplatform\n",
        "    from google.cloud.aiplatform.matching_engine import MatchingEngineIndexEndpoint, Namespace\n",
        "    from google.cloud.aiplatform_v1.types import FindNeighborsRequest, FindNeighborsResponse # For type hints\n",
        "    # Ensure Vertex AI SDK is initialized (should happen in Cell 1)\n",
        "    if hasattr(aiplatform, 'initializer') and getattr(aiplatform.initializer.global_config, 'project', None):\n",
        "        VECTOR_DB_AVAILABLE = True\n",
        "        logging.info(\"Vertex AI Matching Engine SDK found and Vertex AI initialized.\")\n",
        "    else:\n",
        "        logging.warning(\"Vertex AI SDK not initialized. Vertex Vector Search unavailable.\")\n",
        "except ImportError:\n",
        "    logging.warning(\"google-cloud-aiplatform not found or version mismatch. Vertex Vector Search unavailable.\")\n",
        "    # Dummy classes for type hinting if SDK missing\n",
        "    class MatchingEngineIndexEndpoint: pass; class Namespace: pass; class FindNeighborsRequest: pass; class FindNeighborsResponse: pass\n",
        "\n",
        "# --- Assume Real Tool/Client Dependencies ---\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real CONFIG_OBJ in Cell 3 (Reworked).\")\n",
        "except NameError as e:\n",
        "    logger.critical(f\"CRITICAL DEPENDENCY ERROR ({e}). Cannot proceed with Cell 3 logic.\", exc_info=True)\n",
        "    # Define minimal mocks to prevent immediate crashes, but functionality is broken\n",
        "    _config_obj = None; _real_dependencies = False\n",
        "\n",
        "logger = logging.getLogger('MIZ-OKI.KGAdapterTool')\n",
        "\n",
        "# --- Data Pseudonymizer (Improved version from Cell 3 analysis) ---\n",
        "class DataPseudonymizer:\n",
        "    \"\"\"Handles pseudonymization of sensitive data fields using SHA-256 hashing.\"\"\"\n",
        "    def __init__(self, salt: Optional[str]):\n",
        "        if not salt or salt == \"default_insecure_salt_replace_me_!!\":\n",
        "            raise ValueError(\"CRITICAL: Cannot initialize DataPseudonymizer without a valid salt. Set MIZ_SALT env var or secret.\")\n",
        "        self.salt = salt.encode('utf-8')\n",
        "        # Define sensitive fields (case-insensitive matching recommended in practice)\n",
        "        self.sensitive_fields = {\n",
        "            \"email\", \"phone\", \"ip_address\", \"name\", \"user_id\", \"firstName\",\n",
        "            \"lastName\", \"customer_name\", \"address\", \"contact\", \"customer_email\",\n",
        "            \"user_pseudo_id\", # Added from GA4 example\n",
        "            # Add other potentially sensitive fields based on data sources\n",
        "        }\n",
        "        self.logger = logging.getLogger('MIZ-OKI.Pseudonymizer')\n",
        "        self.logger.info(\"DataPseudonymizer initialized.\")\n",
        "\n",
        "    def _hash(self, value: str) -> str:\n",
        "        \"\"\"Generates SHA-256 hash with salt.\"\"\"\n",
        "        return hashlib.sha256(self.salt + str(value).strip().lower().encode('utf-8')).hexdigest()\n",
        "\n",
        "    def pseudonymize_value(self, key: str, value: Any) -> Any:\n",
        "        \"\"\"Pseudonymizes a single value if the key indicates sensitivity.\"\"\"\n",
        "        # Check key (case-insensitive recommended for robustness)\n",
        "        is_sensitive = isinstance(key, str) and key.lower() in self.sensitive_fields\n",
        "        # Pseudonymize only non-empty strings\n",
        "        if is_sensitive and isinstance(value, str) and value.strip():\n",
        "            return f\"pseudo_{self._hash(value)[:16]}\" # Truncated hash\n",
        "        return value # Return original value otherwise\n",
        "\n",
        "    def pseudonymize_dict(self, data: Dict) -> Dict:\n",
        "        \"\"\"Recursively pseudonymizes sensitive fields in a dictionary.\"\"\"\n",
        "        if not isinstance(data, dict):\n",
        "            return data # Return non-dicts as is\n",
        "\n",
        "        pseudonymized = {}\n",
        "        for key, value in data.items():\n",
        "            if isinstance(value, dict):\n",
        "                # Recurse into nested dictionaries\n",
        "                pseudonymized[key] = self.pseudonymize_dict(value)\n",
        "            elif isinstance(value, list):\n",
        "                # Process items in lists\n",
        "                pseudonymized[key] = [\n",
        "                    self.pseudonymize_dict(item) if isinstance(item, dict)\n",
        "                    else self.pseudonymize_value(key, item) # Apply to list items based on parent key's sensitivity (might need refinement based on data structure)\n",
        "                    for item in value\n",
        "                ]\n",
        "            else:\n",
        "                # Apply pseudonymization to the value based on the key\n",
        "                pseudonymized[key] = self.pseudonymize_value(key, value)\n",
        "        return pseudonymized\n",
        "\n",
        "# --- Graph Storage Adapter Interface (Protocol - Remains same) ---\n",
        "class GraphStorageAdapter(Protocol):\n",
        "    \"\"\"Defines the interface for interacting with a graph storage backend.\"\"\"\n",
        "    async def connect(self) -> None: ...\n",
        "    async def close(self) -> None: ...\n",
        "    async def execute_query(self, query: str, parameters: Optional[Dict] = None, *, database: Optional[str] = None) -> List[Dict]: ...\n",
        "    @asynccontextmanager\n",
        "    async def transaction(self, database: Optional[str] = None) -> Any: ... # Yields AsyncSession or equivalent\n",
        "    async def add_entity(self, entity_dict: Dict, source: str, transaction: Optional[Any] = None) -> Dict: ...\n",
        "    async def add_relationship(self, rel_dict: Dict, transaction: Optional[Any] = None) -> bool: ...\n",
        "    async def add_entities_bulk(self, entities: List[Dict], source: str, transaction: Optional[Any] = None) -> Dict: ...\n",
        "    async def add_relationships_bulk(self, relationships: List[Dict], transaction: Optional[Any] = None) -> Dict: ...\n",
        "    async def get_entity(self, mizId: str) -> Optional[Dict]: ...\n",
        "    async def find_entity_by_hints(self, hints: Dict, transaction: Optional[Any] = None) -> Optional[str]: ...\n",
        "    async def get_neighbors(self, mizId: str, relationship_type: Optional[str] = None, direction: str = \"both\", limit: int = 250) -> List[Dict]: ...\n",
        "    async def find_path(self, start_node_hints: Dict, end_node_hints: Dict, relationship_types: Optional[List[str]] = None, max_depth: int = 5) -> Optional[List[Dict]]: ...\n",
        "    async def get_schema(self) -> Dict: ...\n",
        "    async def get_stats(self) -> Dict: ...\n",
        "    # Vector Ops\n",
        "    async def upsert_vector(self, vector_id: str, vector: List[float], metadata: Dict, namespace: Optional[str] = None) -> bool: ...\n",
        "    async def search_vector_index(self, query_vector: List[float], k: int, namespace: Optional[str] = None, filter_dict: Optional[Dict] = None) -> List[Tuple[str, float, Dict]]: ...\n",
        "    async def create_vector_namespace(self, namespace: str) -> bool: ... # May not be applicable to all DBs\n",
        "    # XAI Storage\n",
        "    async def save_decision_record(self, record: Dict) -> bool: ...\n",
        "    async def retrieve_decision_record(self, decision_id: str) -> Optional[Dict]: ...\n",
        "\n",
        "# --- Neo4j Async Adapter Implementation (Reworked) ---\n",
        "class Neo4jAsyncAdapter(GraphStorageAdapter):\n",
        "    \"\"\"Async Adapter for Neo4j, including Vector DB routing and structured XAI storage.\"\"\"\n",
        "    def __init__(self, config: EnhancedConfig):\n",
        "        if not config: raise ValueError(\"Configuration object is required for Neo4jAsyncAdapter.\")\n",
        "        self.config = config\n",
        "        self._driver: Optional[AsyncDriver] = None\n",
        "        self._vector_index_endpoint_client: Optional[MatchingEngineIndexEndpoint] = None # For Vertex Vector Search\n",
        "        self.logger = logging.getLogger('MIZ-OKI.Neo4jAsyncAdapter')\n",
        "\n",
        "        # Validate required config sections\n",
        "        if not config.kg or not config.kg.neo4j:\n",
        "            raise ConfigurationError(\"KG or Neo4j configuration missing in EnhancedConfig.\")\n",
        "        if config.kg.storage_type == \"neo4j\" and not NEO4J_AVAILABLE:\n",
        "             raise ImportError(\"Neo4j configured as storage_type but 'neo4j' async driver not installed.\")\n",
        "        if config.kg.vector_db_type == \"vertex_vector_search\" and not VECTOR_DB_AVAILABLE:\n",
        "             logger.warning(\"Vertex Vector Search configured but SDK is unavailable.\")\n",
        "        if config.kg.vector_db_type == \"vertex_vector_search\" and not config.kg.vector_db_endpoint:\n",
        "             logger.warning(\"Vertex Vector Search configured but endpoint name (VERTEX_VECTOR_INDEX_ENDPOINT_NAME) is missing.\")\n",
        "\n",
        "        self.logger.info(f\"Neo4jAsyncAdapter instance created. KG Storage: {config.kg.storage_type}, Vector DB: {config.kg.vector_db_type}\")\n",
        "\n",
        "    async def connect(self) -> None:\n",
        "        \"\"\" Establish connection to Neo4j & potentially Vector DB asynchronously. \"\"\"\n",
        "        # Neo4j Connection\n",
        "        if self.config.kg.storage_type == \"neo4j\" or self.config.kg.vector_db_type == \"neo4j\":\n",
        "            if not self._driver or self._driver.closed:\n",
        "                if not NEO4J_AVAILABLE: raise ConnectionError(\"Neo4j async driver not available.\")\n",
        "                neo4j_cfg = self.config.db.neo4j # Use db config section\n",
        "                if not neo4j_cfg.uri or not neo4j_cfg.user or not neo4j_cfg.password:\n",
        "                    raise ConnectionError(\"Neo4j connection details (URI, User, Password) missing in configuration.\")\n",
        "                try:\n",
        "                    self.logger.info(f\"Connecting to Neo4j async driver at {neo4j_cfg.uri}...\")\n",
        "                    self._driver = AsyncGraphDatabase.driver(\n",
        "                        neo4j_cfg.uri,\n",
        "                        auth=basic_auth(neo4j_cfg.user, neo4j_cfg.password),\n",
        "                        max_connection_lifetime=neo4j_cfg.max_connection_lifetime,\n",
        "                        connection_timeout=neo4j_cfg.connection_timeout,\n",
        "                        max_connection_pool_size=50 # Example pool size\n",
        "                    )\n",
        "                    await self._driver.verify_connectivity()\n",
        "                    self.logger.info(f\"Neo4j async driver connected and verified for {neo4j_cfg.uri}\")\n",
        "                    await self._ensure_constraints_and_indices() # Ensure schema elements exist\n",
        "                except neo4j_exceptions.AuthError:\n",
        "                    self.logger.critical(f\"Neo4j authentication failed for user '{neo4j_cfg.user}'. Check credentials.\")\n",
        "                    self._driver = None; raise ConnectionError(\"Neo4j authentication failed.\")\n",
        "                except neo4j_exceptions.ServiceUnavailable:\n",
        "                     self.logger.critical(f\"Neo4j service unavailable at {neo4j_cfg.uri}. Check DB status and URI.\")\n",
        "                     self._driver = None; raise ConnectionError(\"Neo4j service unavailable.\")\n",
        "                except Exception as e:\n",
        "                    self.logger.critical(f\"Neo4j async connection failed: {e}\", exc_info=True)\n",
        "                    self._driver = None; raise ConnectionError(f\"Neo4j connection failed: {e}\") from e\n",
        "\n",
        "        # Vector DB Connection (Vertex AI Vector Search)\n",
        "        if self.config.kg.vector_db_type == \"vertex_vector_search\":\n",
        "            if self._vector_index_endpoint_client is None and VECTOR_DB_AVAILABLE:\n",
        "                endpoint_name = self.config.kg.vector_db_endpoint\n",
        "                if endpoint_name:\n",
        "                    try:\n",
        "                        self.logger.info(f\"Connecting to Vertex AI Vector Search Index Endpoint: {endpoint_name}\")\n",
        "                        # Instantiation is synchronous\n",
        "                        self._vector_index_endpoint_client = MatchingEngineIndexEndpoint(index_endpoint_name=endpoint_name)\n",
        "                        # Optional: Add a test call here if needed, e.g., list deployed indexes (sync in thread)\n",
        "                        # await asyncio.to_thread(self._vector_index_endpoint_client.list_deployed_indexes)\n",
        "                        self.logger.info(f\"Connected to Vertex AI Vector Search Index Endpoint.\")\n",
        "                    except Exception as vec_e:\n",
        "                        logger.error(f\"Failed to initialize Vertex AI Vector Search client: {vec_e}\", exc_info=True)\n",
        "                        self._vector_index_endpoint_client = None # Ensure client is None on failure\n",
        "                else:\n",
        "                    logger.warning(\"vertex_vector_search configured but endpoint name missing in config.kg.vector_db_endpoint.\")\n",
        "            elif not VECTOR_DB_AVAILABLE:\n",
        "                logger.warning(\"Vertex AI Matching Engine SDK not available. Cannot connect.\")\n",
        "\n",
        "        elif self.config.kg.vector_db_type not in [\"neo4j\", \"none\"]:\n",
        "           logger.warning(f\"Vector DB type '{self.config.kg.vector_db_type}' configured but connection logic not implemented.\")\n",
        "\n",
        "    async def close(self) -> None:\n",
        "        \"\"\"Closes Neo4j driver connection asynchronously.\"\"\"\n",
        "        if self._driver:\n",
        "            try:\n",
        "                await self._driver.close()\n",
        "                self.logger.info(\"Neo4j async connection closed.\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error closing Neo4j async connection: {e}\")\n",
        "            finally:\n",
        "                self._driver = None\n",
        "        # No explicit close needed for Vertex AI client library instances generally\n",
        "\n",
        "    async def _ensure_constraints_and_indices(self) -> None:\n",
        "        \"\"\"Ensure necessary constraints and indices exist in Neo4j.\"\"\"\n",
        "        if not self._driver: return\n",
        "        queries = [\n",
        "            # Unique constraint on the primary identifier for Entities\n",
        "            \"CREATE CONSTRAINT unique_entity_mizId IF NOT EXISTS FOR (n:Entity) REQUIRE n.mizId IS UNIQUE\",\n",
        "            # Unique constraint for Decision Logs\n",
        "            \"CREATE CONSTRAINT unique_decision_id IF NOT EXISTS FOR (d:DecisionLog) REQUIRE d.decision_id IS UNIQUE\",\n",
        "            # Composite unique constraint for external entities (platform + original_id) - Adjust label if needed\n",
        "            \"CREATE CONSTRAINT unique_external_id IF NOT EXISTS FOR (e:ExternalEntity) REQUIRE (e.platform, e.original_id) IS UNIQUE\",\n",
        "            # Indices for faster lookups\n",
        "            \"CREATE INDEX entity_type_index IF NOT EXISTS FOR (n:Entity) ON (n.entity_type)\",\n",
        "            \"CREATE INDEX entity_source_index IF NOT EXISTS FOR (n:Entity) ON (n.source)\",\n",
        "            \"CREATE INDEX decision_timestamp_index IF NOT EXISTS FOR (d:DecisionLog) ON (d.timestamp)\",\n",
        "            \"CREATE INDEX workflow_exec_id_index IF NOT EXISTS FOR (wf:WorkflowExecution) ON (wf.id)\",\n",
        "            \"CREATE INDEX workflow_step_id_index IF NOT EXISTS FOR (ws:WorkflowStep) ON (ws.id)\",\n",
        "        ]\n",
        "        # Add Neo4j vector index creation if configured\n",
        "        if self.config.kg.vector_db_type == \"neo4j\":\n",
        "             idx_name = self.config.kg.vector_index_name\n",
        "             dims = self.config.kg.vector_dimensions\n",
        "             if idx_name and dims > 0:\n",
        "                 # Assuming embeddings are on :Entity nodes, property 'embedding'\n",
        "                 queries.append(f\"CREATE VECTOR INDEX {idx_name} IF NOT EXISTS FOR (n:Entity) ON (n.embedding) OPTIONS {{indexConfig: {{`vector.dimensions`: {dims}, `vector.similarity_function`: 'cosine'}}}}\")\n",
        "             else:\n",
        "                 logger.warning(\"Neo4j vector index configured but name or dimensions missing/invalid.\")\n",
        "\n",
        "        try:\n",
        "            async with self.transaction() as session: # Use the transaction context manager\n",
        "                 async with session.begin_transaction() as tx: # Start a transaction\n",
        "                     for query in queries:\n",
        "                         try:\n",
        "                             await tx.run(query)\n",
        "                             logger.info(f\"Applied/verified schema async: {query.split(' FOR')[0]}...\")\n",
        "                         except neo4j_exceptions.ClientError as e:\n",
        "                             # Ignore errors indicating the constraint/index already exists\n",
        "                             if \"already exists\" in str(e).lower() or \"Constraint already created\" in str(e) or \"index already exists\" in str(e).lower():\n",
        "                                 logger.debug(f\"Schema item likely exists: {query.split(' FOR')[0]}...\")\n",
        "                             else:\n",
        "                                 raise # Re-raise other client errors\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to ensure schema constraints/indices async: {e}\", exc_info=True)\n",
        "\n",
        "    async def execute_query(self, query: str, parameters: Optional[Dict] = None, *, database: Optional[str] = \"neo4j\") -> List[Dict]:\n",
        "        \"\"\"Executes a read Cypher query asynchronously.\"\"\"\n",
        "        if not self._driver or self._driver.closed: await self.connect()\n",
        "        if not self._driver: raise ConnectionError(\"Neo4j async driver not connected.\")\n",
        "        parameters = parameters or {}\n",
        "        try:\n",
        "            # Use execute_query for potentially simpler read operations or when explicit transaction management isn't needed\n",
        "            results, summary, keys = await self._driver.execute_query(query, parameters, database_=database)\n",
        "            # Convert Neo4j Records to dictionaries\n",
        "            return [r.data() for r in results]\n",
        "        except neo4j_exceptions.ClientError as e:\n",
        "            logger.error(f\"Cypher query syntax error async: {e}\\nQuery: {query}\\nParams: {parameters}\")\n",
        "            raise\n",
        "        except neo4j_exceptions.TransientError as e:\n",
        "            logger.warning(f\"Neo4j transient error async (retrying might help): {e}\\nQuery: {query}\")\n",
        "            raise # Or implement retry logic here/caller\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Cypher query failed async: {e}\\nQuery: {query}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    @asynccontextmanager\n",
        "    async def transaction(self, database: Optional[str] = \"neo4j\") -> AsyncSession:\n",
        "        \"\"\"Provides an asynchronous transaction context.\"\"\"\n",
        "        if not self._driver or self._driver.closed: await self.connect()\n",
        "        if not self._driver: raise ConnectionError(\"Neo4j async driver not connected.\")\n",
        "        session: Optional[AsyncSession] = None\n",
        "        try:\n",
        "            session = self._driver.session(database=database)\n",
        "            yield session # Yield the session; user manages tx with 'async with session.begin_transaction() as tx:'\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Exception creating/yielding Neo4j async session: {e}\", exc_info=True)\n",
        "            raise\n",
        "        finally:\n",
        "            if session:\n",
        "                await session.close()\n",
        "\n",
        "    def _build_merge_clause(self, hints: Dict, variable: str = 'n') -> Tuple[str, Dict, str]:\n",
        "        \"\"\"Builds MERGE clause based on hints (sync helper).\"\"\"\n",
        "        params = {}\n",
        "        merge_parts = []\n",
        "        node_label = hints.get('type', 'Entity') # Default label\n",
        "        # Ensure label safety\n",
        "        safe_node_label = f\"`{node_label}`\" if not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', node_label) else node_label\n",
        "\n",
        "        # Prioritize mizId if available\n",
        "        if 'mizId' in hints and hints['mizId']:\n",
        "            prop = 'mizId'\n",
        "            param_name = f\"{variable}_{prop}_hint\"\n",
        "            merge_parts.append(f\"{variable}.{prop} = ${param_name}\")\n",
        "            params[param_name] = hints[prop]\n",
        "        # Composite key for external entities\n",
        "        elif hints.get('platform') and hints.get('original_id'):\n",
        "            # Use a more specific label if possible, e.g., ExternalEntity\n",
        "            if node_label == 'Entity': node_label = 'ExternalEntity'; safe_node_label = 'ExternalEntity'\n",
        "\n",
        "            prop_plat = 'platform'\n",
        "            param_plat = f\"{variable}_{prop_plat}_hint\"\n",
        "            prop_id = 'original_id'\n",
        "            param_id = f\"{variable}_{prop_id}_hint\"\n",
        "            merge_parts.append(f\"{variable}.{prop_plat} = ${param_plat}\")\n",
        "            merge_parts.append(f\"{variable}.{prop_id} = ${param_id}\")\n",
        "            params[param_plat] = hints['platform']\n",
        "            params[param_id] = hints['original_id']\n",
        "        # Other potential unique identifiers\n",
        "        elif hints.get('email'):\n",
        "            prop = 'email'\n",
        "            param_name = f\"{variable}_{prop}_hint\"; merge_parts.append(f\"{variable}.{prop} = ${param_name}\"); params[param_name] = hints[prop]\n",
        "        elif hints.get('type') == 'Product' and hints.get('sku'):\n",
        "            prop = 'sku'\n",
        "            param_name = f\"{variable}_{prop}_hint\"; merge_parts.append(f\"{variable}.{prop} = ${param_name}\"); params[param_name] = hints[prop]\n",
        "        else:\n",
        "            raise ValueError(f\"Insufficient hints for MERGE clause: {hints}. Need mizId, (platform, original_id), email, or (type=Product, sku).\")\n",
        "\n",
        "        merge_clause = f\"MERGE ({variable}:{safe_node_label} {{ {', '.join(merge_parts)} }})\"\n",
        "        return merge_clause, params, node_label # Return the label used\n",
        "\n",
        "    async def find_entity_by_hints(self, hints: Dict, transaction: Optional[AsyncSession] = None) -> Optional[str]:\n",
        "        \"\"\"Finds an entity's mizId based on hints using MATCH.\"\"\"\n",
        "        if not hints: return None\n",
        "        match_parts = []; params = {}; node_label = hints.get('type'); safe_node_label = f\"`{node_label}`\" if node_label and not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', node_label) else node_label or 'Entity'\n",
        "\n",
        "        # Build MATCH clause based on available hints (similar logic to _build_merge_clause)\n",
        "        prop = None\n",
        "        if 'mizId' in hints and hints['mizId']: prop = 'mizId'\n",
        "        elif hints.get('platform') and hints.get('original_id'): prop = None # Composite handled below\n",
        "        elif hints.get('email'): prop = 'email'\n",
        "        elif hints.get('type') == 'Product' and hints.get('sku'): prop = 'sku'\n",
        "\n",
        "        if prop:\n",
        "            param_name = f\"find_{prop}_hint\"; match_parts.append(f\"n.{prop} = ${param_name}\"); params[param_name] = hints[prop]\n",
        "        elif prop is None and hints.get('platform') and hints.get('original_id'):\n",
        "            if node_label == 'Entity': safe_node_label = 'ExternalEntity' # Assume specific label for composite key\n",
        "            prop_plat='platform'; param_plat=f\"find_{prop_plat}_hint\"; prop_id='original_id'; param_id=f\"find_{prop_id}_hint\"\n",
        "            match_parts.append(f\"n.{prop_plat} = ${param_plat}\"); match_parts.append(f\"n.{prop_id} = ${param_id}\")\n",
        "            params[param_plat] = hints['platform']; params[param_id] = hints['original_id']\n",
        "        else:\n",
        "            logger.warning(f\"Insufficient hints for MATCH: {hints}. Cannot find entity.\")\n",
        "            return None\n",
        "\n",
        "        query = f\"MATCH (n:{safe_node_label} {{ {', '.join(match_parts)} }}) RETURN n.mizId AS mizId LIMIT 1\"\n",
        "        try:\n",
        "            async def _run_find(tx_or_session):\n",
        "                result = await tx_or_session.run(query, params)\n",
        "                record = await result.single()\n",
        "                return record['mizId'] if record and record['mizId'] else None\n",
        "\n",
        "            if transaction: # If session is passed, run query directly\n",
        "                 return await _run_find(transaction)\n",
        "            else: # Manage session internally\n",
        "                 async with self.transaction() as session:\n",
        "                      return await _run_find(session)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error finding entity async by hints {hints}: {e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    async def add_entity(self, entity_dict: Dict, source: str, transaction: Optional[AsyncSession] = None) -> Dict:\n",
        "        \"\"\"Adds or updates an entity in Neo4j asynchronously using MERGE.\"\"\"\n",
        "        if not isinstance(entity_dict, dict): raise TypeError(\"entity_dict must be a dictionary.\")\n",
        "        hints = entity_dict.get('_resolution_hints')\n",
        "        entity_type = entity_dict.get('type') or (hints.get('type') if hints else 'Entity')\n",
        "        if not hints: hints = {\"type\": entity_type} # Ensure hints exist\n",
        "\n",
        "        # Ensure mizId exists, generate if needed\n",
        "        mizId = hints.get('mizId') or entity_dict.get('mizId') or f\"{entity_type}:{uuid.uuid4()}\"\n",
        "        hints['mizId'] = mizId # Ensure hints have the final mizId\n",
        "        entity_dict['mizId'] = mizId # Ensure entity data has the final mizId\n",
        "\n",
        "        # Prepare properties, excluding internal keys like _resolution_hints\n",
        "        properties_to_set = {k: v for k, v in entity_dict.items() if not k.startswith('_')}\n",
        "        properties_to_set['source'] = source\n",
        "        now_iso = datetime.now(datetime.timezone.utc).isoformat()\n",
        "        properties_to_set['updated_at'] = now_iso\n",
        "        properties_to_set['entity_type'] = entity_type # Ensure type is set as property\n",
        "\n",
        "        try:\n",
        "            merge_clause, merge_params, used_label = self._build_merge_clause(hints, 'n')\n",
        "            params = {**merge_params, 'props': properties_to_set, 'now': now_iso}\n",
        "            # Ensure the primary label and potentially 'Entity' label are set\n",
        "            labels_to_set = {f\"`{used_label}`\", \"`Entity`\"} # Use backticks for safety\n",
        "            set_labels_clause = \" SET n\" + \":\".join(labels_to_set)\n",
        "\n",
        "            # Use ON CREATE and ON MATCH for efficient updates\n",
        "            query = f\"\"\"\n",
        "            {merge_clause}\n",
        "            ON CREATE SET n = $props, n.created_at = $now\n",
        "            ON MATCH SET n += $props\n",
        "            {set_labels_clause}\n",
        "            RETURN n.mizId AS mizId, n.created_at = $now AS isNew\n",
        "            \"\"\"\n",
        "\n",
        "            async def _run_merge_in_tx(tx: AsyncTransaction) -> Dict:\n",
        "                 result = await tx.run(query, params)\n",
        "                 record = await result.single()\n",
        "                 summary = await result.consume() # Consume result to get summary\n",
        "                 nodes_created = summary.counters.nodes_created\n",
        "                 props_set = summary.counters.properties_set\n",
        "                 if record:\n",
        "                     return {\"success\": True, \"mizId\": record[\"mizId\"], \"is_new\": record[\"isNew\"], \"_nodes_created\": nodes_created, \"_props_set\": props_set}\n",
        "                 else:\n",
        "                     self.logger.error(f\"MERGE async op for {hints} returned no result. Query: {query}, Params: {params}\")\n",
        "                     return {\"success\": False, \"error\": \"MERGE async op returned no result.\"}\n",
        "\n",
        "            if transaction: # If session is passed, use it to begin transaction\n",
        "                 async with transaction.begin_transaction() as tx:\n",
        "                      return await _run_merge_in_tx(tx)\n",
        "            else: # Manage session and transaction internally\n",
        "                 async with self.transaction() as session:\n",
        "                      # Use session.write_transaction for automatic retry on transient errors\n",
        "                      return await session.write_transaction(_run_merge_in_tx)\n",
        "\n",
        "        except ValueError as ve: # Catch errors from _build_merge_clause\n",
        "             logger.error(f\"Error preparing entity merge for {hints}: {ve}\")\n",
        "             return {\"success\": False, \"mizId\": mizId, \"error\": str(ve), \"hints\": hints}\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Error adding/updating entity async {hints}: {e}\", exc_info=True)\n",
        "             return {\"success\": False, \"mizId\": mizId, \"error\": str(e), \"hints\": hints}\n",
        "\n",
        "    async def add_relationship(self, rel_dict: Dict, transaction: Optional[AsyncSession] = None) -> bool:\n",
        "        \"\"\"Adds or updates a relationship between two entities asynchronously.\"\"\"\n",
        "        source_hints = rel_dict.get('source_hints')\n",
        "        target_hints = rel_dict.get('target_hints')\n",
        "        rel_type = rel_dict.get('type')\n",
        "        if not source_hints or not target_hints or not rel_type:\n",
        "            raise ValueError(\"Missing 'source_hints', 'target_hints', or 'type' for relationship.\")\n",
        "\n",
        "        properties_to_set = {k: v for k, v in rel_dict.items() if k not in ['source_hints', 'target_hints', 'type']}\n",
        "        now_iso = datetime.now(datetime.timezone.utc).isoformat()\n",
        "        properties_to_set['updated_at'] = now_iso\n",
        "\n",
        "        try:\n",
        "            source_merge, source_params, source_label = self._build_merge_clause(source_hints, 'a')\n",
        "            target_merge, target_params, target_label = self._build_merge_clause(target_hints, 'b')\n",
        "            params = {**source_params, **target_params, 'rel_props': properties_to_set, 'now': now_iso}\n",
        "\n",
        "            # Ensure labels are safe\n",
        "            safe_rel_type = f\"`{rel_type}`\" if not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', rel_type) else rel_type\n",
        "            safe_source_label = f\"`{source_label}`\" if not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', source_label) else source_label\n",
        "            safe_target_label = f\"`{target_label}`\" if not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', target_label) else target_label\n",
        "\n",
        "            # MERGE nodes first (ensures they exist) and set their primary labels, then MERGE relationship\n",
        "            query = f\"\"\"\n",
        "            {source_merge} SET a:{safe_source_label}\n",
        "            {target_merge} SET b:{safe_target_label}\n",
        "            MERGE (a)-[r:{safe_rel_type}]->(b)\n",
        "            ON CREATE SET r = $rel_props, r.created_at = $now\n",
        "            ON MATCH SET r += $rel_props, r.updated_at = $now\n",
        "            RETURN count(r) as rel_count\n",
        "            \"\"\"\n",
        "\n",
        "            async def _run_rel_merge_in_tx(tx: AsyncTransaction) -> bool:\n",
        "                 result = await tx.run(query, params)\n",
        "                 record = await result.single()\n",
        "                 # Check if record exists and count is valid (>= 0)\n",
        "                 return record is not None and record[\"rel_count\"] >= 0\n",
        "\n",
        "            if transaction:\n",
        "                 async with transaction.begin_transaction() as tx:\n",
        "                      return await _run_rel_merge_in_tx(tx)\n",
        "            else:\n",
        "                 async with self.transaction() as session:\n",
        "                      return await session.write_transaction(_run_rel_merge_in_tx)\n",
        "\n",
        "        except ValueError as ve: # Catch errors from _build_merge_clause\n",
        "             logger.error(f\"Error preparing relationship merge '{rel_type}' between {source_hints} and {target_hints}: {ve}\")\n",
        "             return False\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Error adding/updating relationship async '{rel_type}' between {source_hints} and {target_hints}: {e}\", exc_info=True)\n",
        "             return False\n",
        "\n",
        "    async def add_entities_bulk(self, entities: List[Dict], source: str, transaction: Optional[AsyncSession] = None) -> Dict:\n",
        "        \"\"\"Adds/updates entities in bulk using UNWIND and MERGE (more robust than APOC).\"\"\"\n",
        "        if not entities: return {\"new\": 0, \"updated\": 0, \"failed\": 0, \"failures\": []}\n",
        "        batch_data = []; failures = []; start_time = time.monotonic(); now_iso = datetime.now(datetime.timezone.utc).isoformat()\n",
        "\n",
        "        for entity_dict in entities:\n",
        "            try:\n",
        "                 if not isinstance(entity_dict, dict): raise TypeError(\"Entity item must be a dictionary.\")\n",
        "                 hints = entity_dict.get('_resolution_hints'); entity_type = entity_dict.get('type') or (hints.get('type') if hints else 'Entity')\n",
        "                 if not hints: hints = {\"type\": entity_type}\n",
        "                 mizId = hints.get('mizId') or entity_dict.get('mizId') or f\"{entity_type}:{uuid.uuid4()}\"; hints['mizId'] = mizId; entity_dict['mizId'] = mizId\n",
        "\n",
        "                 properties_to_set = {k: v for k, v in entity_dict.items() if not k.startswith('_')}\n",
        "                 properties_to_set['source'] = source; properties_to_set['updated_at'] = now_iso; properties_to_set['entity_type'] = entity_type\n",
        "\n",
        "                 # Build merge keys based on hints (similar to _build_merge_clause but just the keys)\n",
        "                 merge_props = {}\n",
        "                 if 'mizId' in hints and hints['mizId']: merge_props['mizId'] = hints['mizId']\n",
        "                 elif hints.get('platform') and hints.get('original_id'): merge_props['platform'] = hints['platform']; merge_props['original_id'] = hints['original_id']\n",
        "                 # Add other unique key combinations if needed\n",
        "                 else: raise ValueError(f\"Insufficient hints for bulk MERGE: {hints}\")\n",
        "\n",
        "                 used_label = hints.get('type', 'Entity'); safe_label = f\"`{used_label}`\" if not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', used_label) else used_label\n",
        "                 labels_to_set = [safe_label, \"`Entity`\"] # Ensure Entity label is also added\n",
        "\n",
        "                 batch_data.append({\n",
        "                     \"merge_props\": merge_props,\n",
        "                     \"set_props\": properties_to_set,\n",
        "                     \"labels\": labels_to_set\n",
        "                 })\n",
        "            except Exception as prep_e:\n",
        "                 failures.append({\"data\": entity_dict, \"error\": str(prep_e)})\n",
        "\n",
        "        if not batch_data:\n",
        "            duration=time.monotonic()-start_time; self.logger.error(f\"Bulk add entities async failed: No valid data prepared ({len(failures)} failures) in {duration:.3f}s.\"); return {\"new\": 0, \"updated\": 0, \"failed\": len(failures), \"failures\": failures}\n",
        "\n",
        "        # Cypher query using UNWIND and MERGE\n",
        "        query = \"\"\"\n",
        "        UNWIND $batch AS item\n",
        "        MERGE (n {mizId: item.merge_props.mizId}) // Assuming mizId is the primary merge key for simplicity here\n",
        "        ON CREATE SET n = item.set_props, n.created_at = $now\n",
        "        ON MATCH SET n += item.set_props\n",
        "        // Dynamically set labels - requires APOC if labels are truly dynamic per item\n",
        "        // Standard Cypher requires labels known at query time. If labels vary, use APOC or multiple queries.\n",
        "        // Assuming labels are consistent for the batch for now, or handle via APOC:\n",
        "        // WITH n, item CALL apoc.create.addLabels(n, item.labels) YIELD node\n",
        "        // RETURN node // If using APOC\n",
        "        SET n:`Entity` // Set base label if not using APOC for dynamic labels\n",
        "        WITH n, item.set_props.created_at = $now AS is_new\n",
        "        RETURN sum(CASE WHEN is_new THEN 1 ELSE 0 END) AS newCount,\n",
        "               sum(CASE WHEN NOT is_new THEN 1 ELSE 0 END) AS updatedCount\n",
        "        \"\"\"\n",
        "        # Note: The standard Cypher SET n:Label syntax doesn't work well with dynamic labels from the list.\n",
        "        # If labels truly vary per item in the batch, APOC's `apoc.create.addLabels` is needed, or run separate queries per label combination.\n",
        "        # The query above assumes `Entity` is the main label and merges primarily on `mizId`. Adjust merge key if needed.\n",
        "\n",
        "        params = {\"batch\": batch_data, \"now\": now_iso}; new_count = 0; updated_count = 0\n",
        "        try:\n",
        "            async def _run_bulk_entities_in_tx(tx: AsyncTransaction) -> Tuple[int, int]:\n",
        "                 result = await tx.run(query, params)\n",
        "                 record = await result.single()\n",
        "                 # Handle potential null counts if the query returns nothing\n",
        "                 nc = record[\"newCount\"] if record and record[\"newCount\"] is not None else 0\n",
        "                 uc = record[\"updatedCount\"] if record and record[\"updatedCount\"] is not None else 0\n",
        "                 return nc, uc\n",
        "\n",
        "            if transaction:\n",
        "                 async with transaction.begin_transaction() as tx:\n",
        "                      new_count, updated_count = await _run_bulk_entities_in_tx(tx)\n",
        "            else:\n",
        "                 async with self.transaction() as session:\n",
        "                      new_count, updated_count = await session.write_transaction(_run_bulk_entities_in_tx)\n",
        "\n",
        "            duration = time.monotonic() - start_time\n",
        "            self.logger.info(f\"Bulk add entities async completed ({duration:.3f}s). New: {new_count}, Updated: {updated_count}, Prep Failed: {len(failures)}\")\n",
        "            return {\"new\": new_count, \"updated\": updated_count, \"failed\": len(failures), \"failures\": failures}\n",
        "        except Exception as e:\n",
        "            duration = time.monotonic() - start_time\n",
        "            self.logger.error(f\"Bulk add entities async failed critically ({duration:.3f}s): {e}\", exc_info=True)\n",
        "            # Add all items intended for the batch to failures on critical DB error\n",
        "            for item in batch_data: failures.append({\"data\": item.get(\"set_props\", {}), \"error\": f\"Bulk DB operation failed: {e}\"})\n",
        "            return {\"new\": 0, \"updated\": 0, \"failed\": len(failures), \"failures\": failures}\n",
        "\n",
        "    async def add_relationships_bulk(self, relationships: List[Dict], transaction: Optional[AsyncSession] = None) -> Dict:\n",
        "        \"\"\"Adds/updates relationships in bulk using UNWIND and MERGE.\"\"\"\n",
        "        if not relationships: return {\"loaded\": 0, \"failed\": 0, \"failures\": []}\n",
        "        batch_data = []; failures = []; start_time = time.monotonic(); now_iso = datetime.now(datetime.timezone.utc).isoformat()\n",
        "\n",
        "        for rel_dict in relationships:\n",
        "            try:\n",
        "                source_hints = rel_dict.get('source_hints'); target_hints = rel_dict.get('target_hints'); rel_type = rel_dict.get('type')\n",
        "                if not source_hints or not target_hints or not rel_type: raise ValueError(\"Missing hints or type.\")\n",
        "\n",
        "                # Assume hints contain mizId for bulk operations\n",
        "                source_mizId = source_hints.get('mizId'); target_mizId = target_hints.get('mizId')\n",
        "                if not source_mizId or not target_mizId: raise ValueError(\"Hints must contain mizId for bulk relationship merge.\")\n",
        "\n",
        "                properties_to_set = {k: v for k, v in rel_dict.items() if k not in ['source_hints', 'target_hints', 'type']}\n",
        "                properties_to_set['updated_at'] = now_iso\n",
        "\n",
        "                safe_rel_type = f\"`{rel_type}`\" if not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', rel_type) else rel_type\n",
        "\n",
        "                batch_data.append({\n",
        "                    \"source_mizId\": source_mizId,\n",
        "                    \"target_mizId\": target_mizId,\n",
        "                    \"rel_type\": safe_rel_type,\n",
        "                    \"rel_props\": properties_to_set\n",
        "                })\n",
        "            except Exception as e:\n",
        "                failures.append({\"data\": rel_dict, \"error\": str(e)})\n",
        "\n",
        "        if not batch_data:\n",
        "            duration=time.monotonic()-start_time; self.logger.error(f\"Bulk add relationships async failed: No valid data prepared ({len(failures)} failures) in {duration:.3f}s.\"); return {\"loaded\": 0, \"failed\": len(failures), \"failures\": failures}\n",
        "\n",
        "        # Use standard MERGE within UNWIND\n",
        "        query = \"\"\"\n",
        "        UNWIND $batch AS item\n",
        "        MATCH (a:Entity {mizId: item.source_mizId})\n",
        "        MATCH (b:Entity {mizId: item.target_mizId})\n",
        "        MERGE (a)-[r:`\"\"\" + safe_rel_type + \"\"\"`]->(b) // Use dynamic relationship type safely\n",
        "        ON CREATE SET r = item.rel_props, r.created_at = $now\n",
        "        ON MATCH SET r += item.rel_props, r.updated_at = $now\n",
        "        RETURN count(r) as total_processed_rels_in_batch\n",
        "        \"\"\"\n",
        "        # Note: This query assumes all relationships in the batch have the SAME type (`safe_rel_type`).\n",
        "        # If types vary within the batch, APOC's `apoc.merge.relationship` is needed, or run separate batches per type.\n",
        "        # Example using APOC (if types vary):\n",
        "        # query_apoc = \"\"\"\n",
        "        # UNWIND $batch AS item\n",
        "        # MATCH (a:Entity {mizId: item.source_mizId})\n",
        "        # MATCH (b:Entity {mizId: item.target_mizId})\n",
        "        # CALL apoc.merge.relationship(a, item.rel_type, {}, item.rel_props, b, {}) YIELD rel\n",
        "        # // Add ON CREATE/MATCH logic if needed via separate SET clauses with WHERE rel.created_at IS NULL etc.\n",
        "        # SET rel.updated_at = $now\n",
        "        # RETURN count(rel) as total_processed_rels_in_batch\n",
        "        # \"\"\"\n",
        "\n",
        "        params = {\"batch\": batch_data, \"now\": now_iso}; processed_rels_count = 0\n",
        "        try:\n",
        "            async def _run_bulk_rels_in_tx(tx: AsyncTransaction) -> int:\n",
        "                 result = await tx.run(query, params) # Use standard query or query_apoc\n",
        "                 # Aggregate counts if query returns multiple rows (unlikely with count)\n",
        "                 total = 0\n",
        "                 async for record in result:\n",
        "                     total += record[\"total_processed_rels_in_batch\"] or 0\n",
        "                 return total\n",
        "\n",
        "            if transaction:\n",
        "                 async with transaction.begin_transaction() as tx:\n",
        "                      processed_rels_count = await _run_bulk_rels_in_tx(tx)\n",
        "            else:\n",
        "                 async with self.transaction() as session:\n",
        "                      processed_rels_count = await session.write_transaction(_run_bulk_rels_in_tx)\n",
        "\n",
        "            duration = time.monotonic() - start_time\n",
        "            self.logger.info(f\"Bulk add relationships async completed ({duration:.3f}s). Loaded/Updated: {processed_rels_count}, Prep Failed: {len(failures)}\")\n",
        "            return {\"loaded\": processed_rels_count, \"failed\": len(failures), \"failures\": failures}\n",
        "        except Exception as e:\n",
        "            duration = time.monotonic() - start_time\n",
        "            self.logger.error(f\"Bulk add relationships async failed critically ({duration:.3f}s): {e}\", exc_info=True)\n",
        "            for item_data in batch_data: failures.append({\"data\": item_data.get(\"rel_props\",{}), \"error\": f\"Bulk DB operation failed: {e}\"})\n",
        "            return {\"loaded\": 0, \"failed\": len(failures), \"failures\": failures}\n",
        "\n",
        "    async def get_entity(self, mizId: str) -> Optional[Dict]:\n",
        "        \"\"\"Retrieves a single entity by its mizId.\"\"\"\n",
        "        query = \"MATCH (n:Entity {mizId: $mizId}) RETURN properties(n) AS props\"\n",
        "        result = await self.execute_query(query, {\"mizId\": mizId})\n",
        "        return result[0]['props'] if result else None\n",
        "\n",
        "    async def get_neighbors(self, mizId: str, relationship_type: Optional[str] = None, direction: str = \"both\", limit: int = 250) -> List[Dict]:\n",
        "        \"\"\"Retrieves neighbors of an entity.\"\"\"\n",
        "        if direction == \"outgoing\": arrow = \"-[r]->\"\n",
        "        elif direction == \"incoming\": arrow = \"<-[r]-\"\n",
        "        else: arrow = \"-[r]-\" # Default to both\n",
        "\n",
        "        params = {\"mizId\": mizId, \"limit\": limit}\n",
        "        rel_match = \"\"\n",
        "        if relationship_type:\n",
        "            safe_rel_type = f\"`{relationship_type}`\" if not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', relationship_type) else relationship_type\n",
        "            rel_match = f\":{safe_rel_type}\"\n",
        "\n",
        "        arrow = arrow.replace('[r]', f'[r{rel_match}]')\n",
        "\n",
        "        query = f\"\"\"\n",
        "        MATCH (a:Entity {{mizId: $mizId}}){arrow}(b:Entity)\n",
        "        RETURN b.mizId AS neighborId, properties(b) AS neighborProps, type(r) AS relationshipType, properties(r) AS relationshipProps\n",
        "        ORDER BY relationshipType, neighborId\n",
        "        LIMIT $limit\n",
        "        \"\"\"\n",
        "        return await self.execute_query(query, params)\n",
        "\n",
        "    async def find_path(self, start_node_hints: Dict, end_node_hints: Dict, relationship_types: Optional[List[str]] = None, max_depth: int = 5) -> Optional[List[Dict]]:\n",
        "        \"\"\"Finds the shortest path between two nodes.\"\"\"\n",
        "        try:\n",
        "            # Resolve hints to mizIds asynchronously\n",
        "            start_mizId_task = self.find_entity_by_hints(start_node_hints)\n",
        "            end_mizId_task = self.find_entity_by_hints(end_node_hints)\n",
        "            start_mizId, end_mizId = await asyncio.gather(start_mizId_task, end_mizId_task)\n",
        "\n",
        "            if not start_mizId or not end_mizId:\n",
        "                logger.warning(f\"Cannot find path async: Start/end node hints unresolved. Start: {start_node_hints} -> {start_mizId}, End: {end_node_hints} -> {end_mizId}.\")\n",
        "                return None\n",
        "            if start_mizId == end_mizId:\n",
        "                logger.info(f\"Start and end node are the same ({start_mizId}). Path length is 0.\")\n",
        "                return [] # Path of length 0\n",
        "\n",
        "            # Build relationship filter string\n",
        "            rel_filter = \"*\" # Default: any relationship type\n",
        "            if relationship_types:\n",
        "                safe_rel_types = [f\"`{rt}`\" if not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', rt) else rt for rt in relationship_types]\n",
        "                rel_filter = \"|\".join([f\":{srt}\" for srt in safe_rel_types])\n",
        "\n",
        "            query = f\"\"\"\n",
        "            MATCH (a:Entity {{mizId: $start_mizId}}), (b:Entity {{mizId: $end_mizId}})\n",
        "            MATCH p = shortestPath((a)-[{rel_filter}*1..{max_depth}]-(b))\n",
        "            RETURN [node in nodes(p) | properties(node)] AS nodes,\n",
        "                   [rel in relationships(p) | {{type: type(rel), props: properties(rel)}}] AS relationships\n",
        "            LIMIT 1\n",
        "            \"\"\"\n",
        "            params = {\"start_mizId\": start_mizId, \"end_mizId\": end_mizId}\n",
        "            result = await self.execute_query(query, params)\n",
        "\n",
        "            if result:\n",
        "                # Return the path structure containing list of nodes and relationships\n",
        "                return result[0]\n",
        "            else:\n",
        "                logger.info(f\"No path found async between {start_mizId} and {end_mizId} (Max Depth: {max_depth}, Types: {relationship_types}).\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error finding path async: {e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    async def get_schema(self) -> Dict:\n",
        "        \"\"\"Retrieves the graph schema (labels, relationship types, property keys).\"\"\"\n",
        "        schema = {\"labels\": [], \"relationship_types\": [], \"property_keys\": []}\n",
        "        try:\n",
        "            labels_query = \"CALL db.labels() YIELD label RETURN collect(label) AS labels\"\n",
        "            rels_query = \"CALL db.relationshipTypes() YIELD relationshipType RETURN collect(relationshipType) AS relationshipTypes\"\n",
        "            props_query = \"CALL db.propertyKeys() YIELD propertyKey RETURN collect(propertyKey) AS propertyKeys\"\n",
        "\n",
        "            labels_result, rels_result, props_result = await asyncio.gather(\n",
        "                self.execute_query(labels_query),\n",
        "                self.execute_query(rels_query),\n",
        "                self.execute_query(props_query),\n",
        "                return_exceptions=True\n",
        "            )\n",
        "\n",
        "            if not isinstance(labels_result, Exception) and labels_result: schema[\"labels\"] = labels_result[0].get('labels', [])\n",
        "            if not isinstance(rels_result, Exception) and rels_result: schema[\"relationship_types\"] = rels_result[0].get('relationshipTypes', [])\n",
        "            if not isinstance(props_result, Exception) and props_result: schema[\"property_keys\"] = props_result[0].get('propertyKeys', [])\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting graph schema async: {e}\", exc_info=True)\n",
        "            schema[\"error\"] = str(e)\n",
        "        return schema\n",
        "\n",
        "    async def get_stats(self) -> Dict:\n",
        "        \"\"\"Retrieves basic graph statistics.\"\"\"\n",
        "        stats = {\"nodes\": -1, \"edges\": -1}\n",
        "        try:\n",
        "            # Use count store for potentially faster results if available and accurate\n",
        "            # stats_query = \"CALL db.stats.retrieve('GRAPH_COUNTS') YIELD nodes, relationships RETURN nodes, relationships\"\n",
        "            # result = await self.execute_query(stats_query)\n",
        "            # if result: stats = {\"nodes\": result[0].get('nodes', -1), \"edges\": result[0].get('relationships', -1)}\n",
        "\n",
        "            # Fallback to MATCH COUNT if stats procedure fails or gives invalid results\n",
        "            # if stats[\"nodes\"] <= 0 or stats[\"edges\"] <= 0:\n",
        "            logger.debug(\"Falling back to MATCH COUNT for graph stats.\")\n",
        "            nodes_query = \"MATCH (n) RETURN count(n) AS nodeCount\"\n",
        "            rels_query = \"MATCH ()-[r]->() RETURN count(r) AS relationshipCount\"\n",
        "            node_result, rel_result = await asyncio.gather(\n",
        "                self.execute_query(nodes_query),\n",
        "                self.execute_query(rels_query),\n",
        "                return_exceptions=True\n",
        "            )\n",
        "            if not isinstance(node_result, Exception) and node_result: stats[\"nodes\"] = node_result[0].get('nodeCount', -1)\n",
        "            if not isinstance(rel_result, Exception) and rel_result: stats[\"edges\"] = rel_result[0].get('relationshipCount', -1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting graph stats async: {e}\", exc_info=True)\n",
        "            stats[\"error\"] = str(e)\n",
        "        return stats\n",
        "\n",
        "    # --- Vector Operations (Reworked) ---\n",
        "    async def upsert_vector(self, vector_id: str, vector: List[float], metadata: Dict, namespace: Optional[str] = None) -> bool:\n",
        "        \"\"\"Upserts a vector into the configured vector database.\"\"\"\n",
        "        db_type = self.config.kg.vector_db_type\n",
        "        namespace = namespace or \"default\" # Default namespace if not provided\n",
        "        self.logger.debug(f\"Upserting vector async. ID: {vector_id}, DB: {db_type}, Namespace: {namespace}\")\n",
        "\n",
        "        if db_type == \"vertex_vector_search\":\n",
        "            if self._vector_index_endpoint_client and VECTOR_DB_AVAILABLE:\n",
        "                try:\n",
        "                    # Format metadata for Vertex AI Vector Search restricts\n",
        "                    # Note: Vertex AI uses 'allow_list' for filtering. Metadata keys become restrict namespaces.\n",
        "                    restricts = []\n",
        "                    if metadata:\n",
        "                        for key, value in metadata.items():\n",
        "                             # Ensure value is a list of strings for allow_list\n",
        "                             allow_list_values = [str(v) for v in value] if isinstance(value, list) else [str(value)]\n",
        "                             restricts.append(Namespace(name=key, allow_list=allow_list_values))\n",
        "\n",
        "                    datapoint = aiplatform.matching_engine.Datapoint(\n",
        "                        datapoint_id=vector_id,\n",
        "                        feature_vector=vector,\n",
        "                        restricts=restricts\n",
        "                    )\n",
        "                    # Use upsert_datapoints (sync SDK method) in a thread\n",
        "                    await asyncio.to_thread(\n",
        "                        self._vector_index_endpoint_client.upsert_datapoints,\n",
        "                        datapoints=[datapoint]\n",
        "                    )\n",
        "                    self.logger.info(f\"Vertex Vector Search upsert successful for ID: {vector_id}\")\n",
        "                    return True\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Vertex Vector Search upsert failed for ID {vector_id}: {e}\", exc_info=True)\n",
        "                    return False\n",
        "            else:\n",
        "                logger.error(\"Vertex Vector Search client not initialized or SDK unavailable for upsert.\")\n",
        "                return False\n",
        "\n",
        "        elif db_type == \"neo4j\":\n",
        "            # Assumes vector_id corresponds to a mizId of an :Entity node\n",
        "            return await self.add_node_embedding(mizId=vector_id, embedding=vector, embedding_property=metadata.get(\"embedding_property\", \"embedding\"))\n",
        "\n",
        "        else:\n",
        "            logger.error(f\"Vector upsert not supported for db_type: '{db_type}'\")\n",
        "            return False\n",
        "\n",
        "    async def search_vector_index(self, query_vector: List[float], k: int, namespace: Optional[str] = None, filter_dict: Optional[Dict] = None) -> List[Tuple[str, float, Dict]]:\n",
        "        \"\"\"Searches the vector index asynchronously.\"\"\"\n",
        "        db_type = self.config.kg.vector_db_type\n",
        "        namespace = namespace or \"default\"\n",
        "        self.logger.debug(f\"Searching vector index async. K={k}, DB: {db_type}, Namespace: {namespace}, Filter: {filter_dict}\")\n",
        "\n",
        "        if db_type == \"vertex_vector_search\":\n",
        "            if self._vector_index_endpoint_client and VECTOR_DB_AVAILABLE:\n",
        "                try:\n",
        "                    # Prepare filters for Vertex AI ('restricts')\n",
        "                    search_restricts = []\n",
        "                    if filter_dict:\n",
        "                         for key, value in filter_dict.items():\n",
        "                              allow_list_values = [str(v) for v in value] if isinstance(value, list) else [str(value)]\n",
        "                              search_restricts.append(Namespace(name=key, allow_list=allow_list_values))\n",
        "\n",
        "                    # Use find_neighbors (sync SDK method) in a thread\n",
        "                    # Note: Specify deployed_index_id if multiple indexes are deployed to the endpoint\n",
        "                    deployed_index_id = os.getenv(\"VERTEX_VECTOR_DEPLOYED_INDEX_ID\") # Get from env if needed\n",
        "                    if not deployed_index_id:\n",
        "                        logger.warning(\"VERTEX_VECTOR_DEPLOYED_INDEX_ID not set, assuming only one index deployed to endpoint.\")\n",
        "\n",
        "                    response: FindNeighborsResponse = await asyncio.to_thread(\n",
        "                        self._vector_index_endpoint_client.find_neighbors,\n",
        "                        queries=[query_vector],\n",
        "                        deployed_index_id=deployed_index_id, # Pass deployed index ID\n",
        "                        num_neighbors=k,\n",
        "                        filter=search_restricts\n",
        "                    )\n",
        "\n",
        "                    # Parse response: response[0] contains neighbors for the first query\n",
        "                    neighbors = response[0].neighbors if response and response[0].neighbors else []\n",
        "                    # Return format: List[Tuple[vector_id, score, metadata_dict]]\n",
        "                    # Vertex AI find_neighbors doesn't return metadata directly. Metadata is usually stored elsewhere (like KG).\n",
        "                    # Score is 1.0 - distance for cosine similarity.\n",
        "                    results = [(match.datapoint.datapoint_id, 1.0 - match.distance, {\"source\": \"vertex_vector_search\"}) for match in neighbors]\n",
        "                    self.logger.info(f\"Vertex Vector Search successful. Found {len(results)} neighbors.\")\n",
        "                    return results\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Vertex Vector Search failed: {e}\", exc_info=True)\n",
        "                    return []\n",
        "            else:\n",
        "                logger.error(\"Vertex Vector Search client not initialized or SDK unavailable for search.\")\n",
        "                return []\n",
        "\n",
        "        elif db_type == \"neo4j\":\n",
        "             index_name = self.config.kg.vector_index_name\n",
        "             if index_name and self._driver:\n",
        "                 # Call internal Neo4j vector search helper\n",
        "                 results_neo = await self._neo4j_vector_search(query_vector, index_name, k, filter_dict)\n",
        "                 # Neo4j vector search typically returns node IDs and scores. Metadata needs separate fetch.\n",
        "                 # Fetch metadata separately if needed, e.g., using get_entity\n",
        "                 # For now, return placeholder metadata\n",
        "                 return [(nid, score, {\"source\": \"neo4j\"}) for nid, score in results_neo]\n",
        "             else:\n",
        "                 logger.error(\"Neo4j vector index name not configured or driver unavailable.\")\n",
        "                 return []\n",
        "        else:\n",
        "            logger.error(f\"Vector search not supported for db_type: '{db_type}'\")\n",
        "            return []\n",
        "\n",
        "    async def create_vector_namespace(self, namespace: str) -> bool:\n",
        "        \"\"\"Creates a namespace (if applicable to the vector DB). Placeholder.\"\"\"\n",
        "        db_type = self.config.kg.vector_db_type\n",
        "        self.logger.info(f\"Placeholder: Creating vector namespace '{namespace}' for DB type '{db_type}'.\")\n",
        "        # Implementation depends heavily on the specific vector DB.\n",
        "        # Vertex AI Vector Search uses restricts, not explicit namespaces in this way.\n",
        "        # Neo4j doesn't have namespaces in the same way either.\n",
        "        return True # Simulate success\n",
        "\n",
        "    # --- Neo4j Specific Vector Methods (Internal Helpers - Async) ---\n",
        "    async def add_node_embedding(self, mizId: str, embedding: List[float], embedding_property: str = \"embedding\", transaction: Optional[AsyncSession] = None) -> bool:\n",
        "        \"\"\"Sets the embedding property on a specific node asynchronously.\"\"\"\n",
        "        if not self._driver: return False\n",
        "        query = f\"MATCH (n:Entity {{mizId: $mizId}}) SET n.`{embedding_property}` = $embedding\"\n",
        "        params = {\"mizId\": mizId, \"embedding\": embedding}\n",
        "        try:\n",
        "            async def _run_set_embedding(tx: AsyncTransaction) -> bool:\n",
        "                 result = await tx.run(query, params)\n",
        "                 summary = await result.consume()\n",
        "                 return summary.counters.properties_set > 0\n",
        "\n",
        "            if transaction:\n",
        "                 async with transaction.begin_transaction() as tx: return await _run_set_embedding(tx)\n",
        "            else:\n",
        "                 async with self.transaction() as session: return await session.write_transaction(_run_set_embedding)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to set embedding for node {mizId}: {e}\", exc_info=True)\n",
        "            return False\n",
        "\n",
        "    async def _neo4j_vector_search(self, query_vector: List[float], index_name: str, k: int = 5, filter_dict: Optional[Dict] = None) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Performs vector similarity search using Neo4j's vector index.\"\"\"\n",
        "        if not self._driver: return []\n",
        "        # Basic query structure - filtering needs to be added based on filter_dict\n",
        "        # Example filter: WHERE n.category = 'some_category'\n",
        "        filter_clause = \"\"\n",
        "        params = {\"queryVector\": query_vector, \"k\": k}\n",
        "        if filter_dict:\n",
        "            filter_parts = []\n",
        "            for i, (key, value) in enumerate(filter_dict.items()):\n",
        "                param_name = f\"filter_val_{i}\"\n",
        "                filter_parts.append(f\"n.`{key}` = ${param_name}\")\n",
        "                params[param_name] = value\n",
        "            if filter_parts:\n",
        "                filter_clause = \"WHERE \" + \" AND \".join(filter_parts)\n",
        "\n",
        "        # Ensure index_name is safe if needed, though usually predefined\n",
        "        safe_index_name = index_name # Add validation/escaping if index names can be dynamic/unsafe\n",
        "\n",
        "        query = f\"\"\"\n",
        "        CALL db.index.vector.queryNodes('{safe_index_name}', $k, $queryVector) YIELD node, score\n",
        "        {filter_clause}\n",
        "        RETURN node.mizId AS mizId, score\n",
        "        \"\"\"\n",
        "        try:\n",
        "            results = await self.execute_query(query, params)\n",
        "            return [(r['mizId'], r['score']) for r in results]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Neo4j vector search failed for index '{safe_index_name}': {e}\", exc_info=True)\n",
        "            return []\n",
        "\n",
        "    # --- XAI Storage Methods (Reworked for Structured Storage - Async) ---\n",
        "    async def save_decision_record(self, record: Dict) -> bool:\n",
        "        \"\"\"Saves a decision record with structured nodes and relationships in Neo4j asynchronously.\"\"\"\n",
        "        decision_id = record.get('decision_id')\n",
        "        if not decision_id:\n",
        "            logger.error(\"Cannot save decision log: missing 'decision_id'.\")\n",
        "            return False\n",
        "        if not self._driver:\n",
        "             logger.error(f\"Cannot save decision log {decision_id}: Neo4j driver unavailable.\")\n",
        "             return False\n",
        "\n",
        "        logger.debug(f\"Saving structured decision log async for ID: {decision_id}\")\n",
        "        try:\n",
        "            # Prepare parameters, ensuring JSON serializability for complex fields\n",
        "            def safe_json_dumps(data):\n",
        "                try: return json.dumps(data, default=str)\n",
        "                except TypeError: return json.dumps(str(data)) # Fallback\n",
        "\n",
        "            params = {\n",
        "                \"decision_id\": decision_id,\n",
        "                \"component\": record.get(\"component\"),\n",
        "                \"timestamp_iso\": record.get(\"timestamp\", datetime.now(datetime.timezone.utc).isoformat()),\n",
        "                \"model_used\": record.get(\"model_used\"),\n",
        "                \"workflow_exec_id\": record.get(\"workflow_execution_id\"),\n",
        "                \"workflow_step_id\": record.get(\"workflow_step_id\"),\n",
        "                \"decision_json\": safe_json_dumps(record.get(\"decision\", {})),\n",
        "                \"inputs_json\": safe_json_dumps(record.get(\"inputs\", {})),\n",
        "                \"outputs_json\": safe_json_dumps(record.get(\"outputs\", {})),\n",
        "                \"context_json\": safe_json_dumps(record.get(\"context\", {})),\n",
        "                \"cot\": record.get(\"chain_of_thought\", []) # Store list directly\n",
        "            }\n",
        "\n",
        "            # Cypher query to create/update nodes and relationships\n",
        "            query = \"\"\"\n",
        "            MERGE (log:DecisionLog {decision_id: $decision_id})\n",
        "            ON CREATE SET log.created_at = datetime($timestamp_iso)\n",
        "            SET log.component = $component,\n",
        "                log.timestamp = datetime($timestamp_iso),\n",
        "                log.model_used = $model_used,\n",
        "                log.decision_json = $decision_json,\n",
        "                log.inputs_json = $inputs_json,\n",
        "                log.outputs_json = $outputs_json,\n",
        "                log.context_json = $context_json,\n",
        "                log.chain_of_thought = $cot\n",
        "\n",
        "            // Link to Workflow Execution if ID provided\n",
        "            WITH log\n",
        "            WHERE $workflow_exec_id IS NOT NULL AND $workflow_exec_id <> \"\"\n",
        "            MERGE (wfExec:WorkflowExecution {id: $workflow_exec_id})\n",
        "              ON CREATE SET wfExec.first_seen = datetime()\n",
        "            MERGE (log)-[:PART_OF_EXECUTION]->(wfExec)\n",
        "\n",
        "            // Link to Workflow Step if ID provided\n",
        "            WITH log, wfExec\n",
        "            WHERE $workflow_step_id IS NOT NULL AND $workflow_step_id <> \"\" AND wfExec IS NOT NULL\n",
        "            MERGE (wfStep:WorkflowStep {id: $workflow_step_id})\n",
        "              ON CREATE SET wfStep.first_seen = datetime()\n",
        "            // Ensure step is linked to execution (might be created elsewhere)\n",
        "            MERGE (wfStep)-[:PART_OF_EXECUTION]->(wfExec)\n",
        "            MERGE (log)-[:EXECUTED_STEP]->(wfStep)\n",
        "\n",
        "            RETURN count(log) as count\n",
        "            \"\"\"\n",
        "\n",
        "            async def _run_save_decision(tx: AsyncTransaction) -> bool:\n",
        "                 result = await tx.run(query, params)\n",
        "                 record = await result.single()\n",
        "                 summary = await result.consume()\n",
        "                 # Check if the node was created or properties were set\n",
        "                 success = (record is not None and record[\"count\"] == 1) or summary.counters.properties_set > 0\n",
        "                 return success\n",
        "\n",
        "            async with self.transaction() as session:\n",
        "                success = await session.write_transaction(_run_save_decision)\n",
        "\n",
        "            if success: logger.debug(f\"Saved/Updated structured decision log {decision_id}.\")\n",
        "            else: logger.error(f\"Failed to save/update structured decision log {decision_id}.\")\n",
        "            return success\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save structured decision record {decision_id} to KG async: {e}\", exc_info=True)\n",
        "            return False\n",
        "\n",
        "    async def retrieve_decision_record(self, decision_id: str) -> Optional[Dict]:\n",
        "        \"\"\"Retrieves a structured decision record from Neo4j asynchronously.\"\"\"\n",
        "        if not self._driver:\n",
        "             logger.error(f\"Cannot retrieve decision log {decision_id}: Neo4j driver unavailable.\")\n",
        "             return None\n",
        "        logger.info(f\"Retrieving structured decision log async for ID: {decision_id}\")\n",
        "        query = \"\"\"\n",
        "        MATCH (log:DecisionLog {decision_id: $decision_id})\n",
        "        OPTIONAL MATCH (log)-[:EXECUTED_STEP]->(step:WorkflowStep)\n",
        "        OPTIONAL MATCH (log)-[:PART_OF_EXECUTION]->(exec:WorkflowExecution)\n",
        "        RETURN\n",
        "            properties(log) AS log_props,\n",
        "            properties(step) AS step_props,\n",
        "            properties(exec) AS exec_props\n",
        "        LIMIT 1\n",
        "        \"\"\"\n",
        "        params = {\"decision_id\": decision_id}\n",
        "        try:\n",
        "            results = await self.execute_query(query, params)\n",
        "            if not results:\n",
        "                logger.warning(f\"Decision log {decision_id} not found in KG.\")\n",
        "                return None\n",
        "\n",
        "            log_data = results[0].get(\"log_props\", {})\n",
        "            step_data = results[0].get(\"step_props\")\n",
        "            exec_data = results[0].get(\"exec_props\")\n",
        "\n",
        "            # Reconstruct the record, parsing JSON properties safely\n",
        "            record = {\n",
        "                \"decision_id\": log_data.get(\"decision_id\"),\n",
        "                \"timestamp\": log_data.get(\"timestamp\"), # Neo4j driver handles datetime conversion\n",
        "                \"component\": log_data.get(\"component\"),\n",
        "                \"model_used\": log_data.get(\"model_used\"),\n",
        "                \"chain_of_thought\": log_data.get(\"chain_of_thought\", []),\n",
        "                \"workflow_execution_id\": exec_data.get(\"id\") if exec_data else log_data.get(\"workflow_execution_id\"),\n",
        "                \"workflow_step_id\": step_data.get(\"id\") if step_data else log_data.get(\"workflow_step_id\"),\n",
        "            }\n",
        "            for json_field in [\"decision\", \"inputs\", \"outputs\", \"context\"]:\n",
        "                json_str = log_data.get(f\"{json_field}_json\")\n",
        "                if json_str:\n",
        "                    try: record[json_field] = json.loads(json_str)\n",
        "                    except json.JSONDecodeError: record[json_field] = {\"error\": f\"Failed to parse {json_field} JSON\", \"raw\": json_str}\n",
        "                else: record[json_field] = {} # Default to empty dict if field missing\n",
        "\n",
        "            return record\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to retrieve structured decision record {decision_id} from KG async: {e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "\n",
        "# --- KnowledgeGraphToolService API Layer (Conceptual Reworked Async) ---\n",
        "# This class wraps the adapter and provides the API endpoints (e.g., for FastAPI)\n",
        "# It handles MIZ OKI payload parsing, validation, pseudonymization, and response formatting.\n",
        "\n",
        "# --- Pydantic Models for API Payloads (Conceptual - Requires FastAPI/Pydantic) ---\n",
        "if FASTAPI_AVAILABLE:\n",
        "    class MizOkiBase(BaseModel):\n",
        "        miz_oki_version: str = Field(default_factory=lambda: _config_obj.miz_oki_schema_version if _config_obj else \"unknown\")\n",
        "        request_id: str = Field(default_factory=lambda: f\"req_{uuid.uuid4().hex[:8]}\")\n",
        "        trace_id: Optional[str] = None\n",
        "        workflow_execution_id: Optional[str] = None\n",
        "        step_id: Optional[str] = None\n",
        "        timestamp: str = Field(default_factory=lambda: datetime.now(datetime.timezone.utc).isoformat())\n",
        "        source_component: Optional[str] = None\n",
        "        target_component: Optional[str] = None\n",
        "        security_context: Optional[Dict[str, Any]] = None # For potential auth info\n",
        "        metadata: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    class MizOkiRequest(MizOkiBase):\n",
        "        payload: Dict[str, Any]\n",
        "\n",
        "    class MizOkiResponse(MizOkiBase):\n",
        "        status: str # e.g., \"success\", \"error\", \"partial_success\", \"not_found\", \"bad_request\"\n",
        "        payload: Optional[Dict[str, Any]] = None\n",
        "        error_details: Optional[List[Dict[str, Any]]] = None\n",
        "\n",
        "    # Example specific payload models (can be added for stricter validation)\n",
        "    class EntityPayload(BaseModel):\n",
        "        entity_data: Dict[str, Any]\n",
        "        source: Optional[str] = \"api\"\n",
        "\n",
        "    class BulkEntitiesPayload(BaseModel):\n",
        "        entities: List[Dict[str, Any]]\n",
        "        source: Optional[str] = \"bulk_api\"\n",
        "\n",
        "    class RelationshipPayload(BaseModel):\n",
        "        relationship_data: Dict[str, Any]\n",
        "\n",
        "    class BulkRelationshipsPayload(BaseModel):\n",
        "        relationships: List[Dict[str, Any]]\n",
        "\n",
        "    class HintsPayload(BaseModel):\n",
        "        hints: Dict[str, Any]\n",
        "\n",
        "    class NeighborsPayload(BaseModel):\n",
        "        mizId: str\n",
        "        relationship_type: Optional[str] = None\n",
        "        direction: Optional[str] = \"both\"\n",
        "        limit: Optional[int] = 250\n",
        "\n",
        "    class PathPayload(BaseModel):\n",
        "        start_node_hints: Dict[str, Any]\n",
        "        end_node_hints: Dict[str, Any]\n",
        "        relationship_types: Optional[List[str]] = None\n",
        "        max_depth: Optional[int] = 5\n",
        "\n",
        "    class VectorUpsertPayload(BaseModel):\n",
        "        vector_id: str\n",
        "        vector: List[float]\n",
        "        metadata: Dict[str, Any]\n",
        "        namespace: Optional[str] = None\n",
        "\n",
        "    class VectorSearchPayload(BaseModel):\n",
        "        query_vector: List[float]\n",
        "        k: int\n",
        "        namespace: Optional[str] = None\n",
        "        filter_dict: Optional[Dict[str, Any]] = None\n",
        "        index_name: Optional[str] = None # Allow overriding default index\n",
        "\n",
        "    class DecisionRecordPayload(BaseModel):\n",
        "        record: Dict[str, Any]\n",
        "\n",
        "# --- End Pydantic Models ---\n",
        "\n",
        "class KnowledgeGraphToolService:\n",
        "    \"\"\" Conceptual Service wrapping the GraphStorageAdapter. Handles MIZ OKI Payloads. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig):\n",
        "        if not config: raise InitializationError(\"EnhancedConfig is required for KnowledgeGraphToolService.\")\n",
        "        self.config = config\n",
        "        self.adapter: Optional[GraphStorageAdapter] = None\n",
        "        self.pseudonymizer: Optional[DataPseudonymizer] = None\n",
        "        self.logger = logging.getLogger('MIZ-OKI.KGToolService')\n",
        "        self._adapter_initialized = False\n",
        "        self._initialization_lock = asyncio.Lock()\n",
        "\n",
        "    async def initialize_adapter(self):\n",
        "        \"\"\"Initializes the backend adapter (Neo4j) and pseudonymizer.\"\"\"\n",
        "        async with self._initialization_lock:\n",
        "            if self._adapter_initialized: return\n",
        "            try:\n",
        "                # Initialize Adapter\n",
        "                if self.config.kg.storage_type == \"neo4j\":\n",
        "                    if not NEO4J_AVAILABLE: raise InitializationError(\"Neo4j driver not available, cannot initialize Neo4j adapter.\")\n",
        "                    self.adapter = Neo4jAsyncAdapter(self.config)\n",
        "                # elif self.config.kg.storage_type == \"other\": self.adapter = OtherDbAsyncAdapter(self.config) # Example\n",
        "                else:\n",
        "                    raise InitializationError(f\"Unsupported kg.storage_type: {self.config.kg.storage_type}\")\n",
        "\n",
        "                await self.adapter.connect() # Connect during initialization\n",
        "\n",
        "                # Initialize Pseudonymizer\n",
        "                if not self.config.miz_salt: raise InitializationError(\"MIZ_SALT is missing, cannot initialize pseudonymizer.\")\n",
        "                self.pseudonymizer = DataPseudonymizer(self.config.miz_salt)\n",
        "\n",
        "                self._adapter_initialized = True\n",
        "                self.logger.info(f\"KG Tool Service adapter initialized: {type(self.adapter).__name__}\")\n",
        "            except Exception as e:\n",
        "                self.logger.critical(f\"KG Tool Service: CRITICAL Error initializing adapter: {e}\", exc_info=True)\n",
        "                self.adapter = None # Ensure adapter is None on failure\n",
        "                self.pseudonymizer = None\n",
        "                self._adapter_initialized = False\n",
        "                raise InitializationError(f\"Failed to initialize KG Adapter: {e}\") from e\n",
        "\n",
        "    async def close_adapter(self):\n",
        "        \"\"\"Closes the backend adapter connection.\"\"\"\n",
        "        if hasattr(self, 'adapter') and self.adapter:\n",
        "            await self.adapter.close()\n",
        "            self._adapter_initialized = False\n",
        "            self.logger.info(\"KG Tool Service adapter closed.\")\n",
        "\n",
        "    async def ensure_adapter(self):\n",
        "        \"\"\"Ensures the adapter is initialized, calling initialize_adapter if needed.\"\"\"\n",
        "        if not self._adapter_initialized or self.adapter is None or self.pseudonymizer is None:\n",
        "            await self.initialize_adapter()\n",
        "        # Check again after attempting initialization\n",
        "        if self.adapter is None or self.pseudonymizer is None:\n",
        "             raise RuntimeError(\"KG Adapter or Pseudonymizer failed to initialize and is required.\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"),\n",
        "            \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"),\n",
        "            \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"KnowledgeGraphToolService\",\n",
        "            \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status,\n",
        "            \"payload\": payload,\n",
        "            \"error_details\": errors,\n",
        "            \"metadata\": {} # Add processing time later if needed\n",
        "        }\n",
        "\n",
        "    # --- API Endpoint Handlers (Conceptual FastAPI Structure - Reworked Async) ---\n",
        "    # These methods would be decorated with @app.post, @app.get etc. in a FastAPI app.\n",
        "    # They accept the MIZ OKI request dict and return the MIZ OKI response dict.\n",
        "\n",
        "    # @app.post(\"/entities\", response_model=MizOkiResponse, status_code=201)\n",
        "    async def add_entity_endpoint(self, request: Dict = Body(...)) -> Dict:\n",
        "        \"\"\" API endpoint to add/update an entity, handling MIZ OKI payload and pseudonymization. \"\"\"\n",
        "        start_time = time.monotonic(); response_payload = {}; status = \"error\"; errors = []\n",
        "        try:\n",
        "            await self.ensure_adapter() # Ensure adapter and pseudonymizer are ready\n",
        "            # TODO: Add permission check based on request.get(\"security_context\")\n",
        "            # Validate input payload structure (basic check, Pydantic model preferred)\n",
        "            if not isinstance(request.get(\"payload\"), dict) or \"entity_data\" not in request[\"payload\"]:\n",
        "                 raise ValueError(\"Invalid payload: 'payload.entity_data' is required.\")\n",
        "\n",
        "            entity_data = request[\"payload\"][\"entity_data\"]\n",
        "            source = request[\"payload\"].get(\"source\", \"api\") # Default source if not provided\n",
        "\n",
        "            # Apply pseudonymization BEFORE sending to adapter\n",
        "            processed_data = self.pseudonymizer.pseudonymize_dict(entity_data)\n",
        "\n",
        "            # Call the adapter method\n",
        "            result = await self.adapter.add_entity(processed_data, source)\n",
        "\n",
        "            if result.get(\"success\"):\n",
        "                status = \"success\"; response_payload = result\n",
        "            else:\n",
        "                status = \"failed\"; errors.append({\"code\": \"KG_ADD_FAILED\", \"message\": result.get(\"error\", \"Unknown KG error\"), \"details\": result.get(\"hints\")})\n",
        "\n",
        "        except (ValueError, TypeError) as ve:\n",
        "            status = \"bad_request\"; errors.append({\"code\": \"VALIDATION_ERROR\", \"message\": str(ve)})\n",
        "        except InitializationError as ie:\n",
        "             status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "        except ConnectionError as ce:\n",
        "             status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "        except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error adding entity: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(request, status, response_payload if status == \"success\" else None, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    # @app.post(\"/entities/bulk\", response_model=MizOkiResponse)\n",
        "    async def add_entities_bulk_endpoint(self, request: Dict = Body(...)) -> Dict:\n",
        "        \"\"\" API endpoint for bulk entity addition/update. \"\"\"\n",
        "        start_time = time.monotonic(); response_payload = {}; status = \"error\"; errors = []\n",
        "        try:\n",
        "            await self.ensure_adapter()\n",
        "            # TODO: Permission check\n",
        "            if not isinstance(request.get(\"payload\"), dict) or \"entities\" not in request[\"payload\"]:\n",
        "                 raise ValueError(\"Invalid payload: 'payload.entities' (list) is required.\")\n",
        "\n",
        "            entities = request[\"payload\"][\"entities\"]\n",
        "            source = request[\"payload\"].get(\"source\", \"bulk_api\")\n",
        "            if not isinstance(entities, list): raise ValueError(\"'payload.entities' must be a list.\")\n",
        "\n",
        "            # Pseudonymize each entity in the list\n",
        "            processed_entities = [self.pseudonymizer.pseudonymize_dict(e) for e in entities if isinstance(e, dict)]\n",
        "            invalid_items = len(entities) - len(processed_entities)\n",
        "            if invalid_items > 0: logger.warning(f\"{invalid_items} items in bulk entity request were not dictionaries and were skipped.\")\n",
        "\n",
        "            if not processed_entities: raise ValueError(\"No valid entity data provided in the list.\")\n",
        "\n",
        "            # Call adapter method\n",
        "            result = await self.adapter.add_entities_bulk(processed_entities, source)\n",
        "\n",
        "            # Result includes counts and specific failures from the adapter\n",
        "            response_payload = result\n",
        "            failed_count = result.get(\"failed\", 0)\n",
        "            if failed_count == 0: status = \"success\"\n",
        "            elif failed_count < len(processed_entities): status = \"partial_success\"\n",
        "            else: status = \"failed\"\n",
        "            if failed_count > 0: errors = result.get(\"failures\", []) # Pass adapter failures back\n",
        "\n",
        "        except (ValueError, TypeError) as ve:\n",
        "            status = \"bad_request\"; errors.append({\"code\": \"VALIDATION_ERROR\", \"message\": str(ve)})\n",
        "        except InitializationError as ie:\n",
        "             status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "        except ConnectionError as ce:\n",
        "             status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "        except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error adding entities bulk: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    # --- Other Endpoints (Implement similarly, calling corresponding adapter methods) ---\n",
        "\n",
        "    # @app.post(\"/relationships\", response_model=MizOkiResponse, status_code=201)\n",
        "    async def add_relationship_endpoint(self, request: Dict = Body(...)) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             if not isinstance(request.get(\"payload\"), dict) or \"relationship_data\" not in request[\"payload\"]:\n",
        "                  raise ValueError(\"Invalid payload: 'payload.relationship_data' is required.\")\n",
        "             rel_data = request[\"payload\"][\"relationship_data\"]\n",
        "             # Pseudonymize hints if they contain sensitive info (depends on hint structure)\n",
        "             # processed_rel_data = self.pseudonymizer.pseudonymize_dict(rel_data) # Apply carefully based on expected hint content\n",
        "             processed_rel_data = rel_data # Assuming hints don't need it for now\n",
        "             success = await self.adapter.add_relationship(processed_rel_data)\n",
        "             if success: status = \"success\"; response_payload = {\"relationship_added\": True}\n",
        "             else: status = \"failed\"; errors.append({\"code\": \"KG_REL_ADD_FAILED\", \"message\": \"Failed to add/update relationship.\"})\n",
        "         except (ValueError, TypeError) as ve: status = \"bad_request\"; errors.append({\"code\": \"VALIDATION_ERROR\", \"message\": str(ve)})\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error adding relationship: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.post(\"/relationships/bulk\", response_model=MizOkiResponse)\n",
        "    async def add_relationships_bulk_endpoint(self, request: Dict = Body(...)) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             if not isinstance(request.get(\"payload\"), dict) or \"relationships\" not in request[\"payload\"]:\n",
        "                  raise ValueError(\"Invalid payload: 'payload.relationships' (list) is required.\")\n",
        "             relationships = request[\"payload\"][\"relationships\"]\n",
        "             if not isinstance(relationships, list): raise ValueError(\"'payload.relationships' must be a list.\")\n",
        "             # processed_rels = [self.pseudonymizer.pseudonymize_dict(r) for r in relationships if isinstance(r, dict)] # Apply pseudonymization carefully\n",
        "             processed_rels = [r for r in relationships if isinstance(r, dict)]\n",
        "             if not processed_rels: raise ValueError(\"No valid relationship data provided.\")\n",
        "             result = await self.adapter.add_relationships_bulk(processed_rels)\n",
        "             response_payload = result; failed_count = result.get(\"failed\", 0)\n",
        "             if failed_count == 0: status = \"success\"\n",
        "             elif failed_count < len(processed_rels): status = \"partial_success\"\n",
        "             else: status = \"failed\"\n",
        "             if failed_count > 0: errors = result.get(\"failures\", [])\n",
        "         except (ValueError, TypeError) as ve: status = \"bad_request\"; errors.append({\"code\": \"VALIDATION_ERROR\", \"message\": str(ve)})\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error adding relationships bulk: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.get(\"/entities/{mizId}\", response_model=MizOkiResponse)\n",
        "    async def get_entity_endpoint(self, request: Request, mizId: str) -> Dict: # Pass request for context\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         request_data = {\"request_id\": request.headers.get(\"X-Request-ID\"), \"trace_id\": request.headers.get(\"X-Trace-ID\")} # Example: Get IDs from headers\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             entity_data = await self.adapter.get_entity(mizId)\n",
        "             if entity_data:\n",
        "                  # Apply pseudonymization if needed based on request context/role (optional)\n",
        "                  # processed_entity_data = self.pseudonymizer.pseudonymize_dict(entity_data)\n",
        "                  processed_entity_data = entity_data # Assume raw data return for now\n",
        "                  status = \"success\"; response_payload = {\"entity_data\": processed_entity_data}\n",
        "             else: status = \"not_found\"; errors.append({\"code\": \"ENTITY_NOT_FOUND\", \"message\": f\"Entity with mizId '{mizId}' not found.\"})\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error getting entity {mizId}: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request_data, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.post(\"/entities/find\", response_model=MizOkiResponse)\n",
        "    async def find_entity_endpoint(self, request: Dict = Body(...)) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             if not isinstance(request.get(\"payload\"), dict) or \"hints\" not in request[\"payload\"]:\n",
        "                  raise ValueError(\"Invalid payload: 'payload.hints' (dict) is required.\")\n",
        "             hints = request[\"payload\"][\"hints\"]\n",
        "             mizId = await self.adapter.find_entity_by_hints(hints)\n",
        "             if mizId: status = \"success\"; response_payload = {\"mizId\": mizId}\n",
        "             else: status = \"not_found\"; errors.append({\"code\": \"ENTITY_NOT_FOUND\", \"message\": f\"Entity not found for hints: {hints}\"})\n",
        "         except (ValueError, TypeError) as ve: status = \"bad_request\"; errors.append({\"code\": \"VALIDATION_ERROR\", \"message\": str(ve)})\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error finding entity by hints: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.get(\"/entities/{mizId}/neighbors\", response_model=MizOkiResponse)\n",
        "    async def get_neighbors_endpoint(self, request: Request, mizId: str, relationship_type: Optional[str] = None, direction: str = \"both\", limit: int = 250) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         request_data = {\"request_id\": request.headers.get(\"X-Request-ID\"), \"trace_id\": request.headers.get(\"X-Trace-ID\")}\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             neighbors = await self.adapter.get_neighbors(mizId, relationship_type, direction, limit)\n",
        "             # Apply pseudonymization to neighborProps if needed\n",
        "             # processed_neighbors = [...]\n",
        "             status = \"success\"; response_payload = {\"neighbors\": neighbors}\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error getting neighbors for {mizId}: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request_data, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.post(\"/paths/find\", response_model=MizOkiResponse)\n",
        "    async def find_path_endpoint(self, request: Dict = Body(...)) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             payload = request.get(\"payload\", {})\n",
        "             if not all(k in payload for k in [\"start_node_hints\", \"end_node_hints\"]):\n",
        "                  raise ValueError(\"Invalid payload: 'start_node_hints' and 'end_node_hints' are required.\")\n",
        "             path_data = await self.adapter.find_path(\n",
        "                 payload[\"start_node_hints\"], payload[\"end_node_hints\"],\n",
        "                 payload.get(\"relationship_types\"), payload.get(\"max_depth\", 5)\n",
        "             )\n",
        "             if path_data is not None: # Could be empty list [] for path length 0\n",
        "                  status = \"success\"; response_payload = {\"path\": path_data}\n",
        "             else: status = \"not_found\"; errors.append({\"code\": \"PATH_NOT_FOUND\", \"message\": \"No path found between the specified nodes.\"})\n",
        "         except (ValueError, TypeError) as ve: status = \"bad_request\"; errors.append({\"code\": \"VALIDATION_ERROR\", \"message\": str(ve)})\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error finding path: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.get(\"/schema\", response_model=MizOkiResponse)\n",
        "    async def get_schema_endpoint(self, request: Request) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         request_data = {\"request_id\": request.headers.get(\"X-Request-ID\"), \"trace_id\": request.headers.get(\"X-Trace-ID\")}\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             schema_data = await self.adapter.get_schema()\n",
        "             status = \"success\"; response_payload = {\"schema\": schema_data}\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error getting schema: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request_data, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.get(\"/stats\", response_model=MizOkiResponse)\n",
        "    async def get_stats_endpoint(self, request: Request) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         request_data = {\"request_id\": request.headers.get(\"X-Request-ID\"), \"trace_id\": request.headers.get(\"X-Trace-ID\")}\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             stats_data = await self.adapter.get_stats()\n",
        "             status = \"success\"; response_payload = {\"stats\": stats_data}\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error getting stats: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request_data, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.post(\"/vectors\", response_model=MizOkiResponse, status_code=201)\n",
        "    async def upsert_vector_endpoint(self, request: Dict = Body(...)) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             payload = request.get(\"payload\", {})\n",
        "             # Basic validation (Pydantic model preferred)\n",
        "             if not all(k in payload for k in [\"vector_id\", \"vector\", \"metadata\"]):\n",
        "                  raise ValueError(\"Invalid payload: 'vector_id', 'vector', and 'metadata' are required.\")\n",
        "             success = await self.adapter.upsert_vector(\n",
        "                 payload[\"vector_id\"], payload[\"vector\"], payload[\"metadata\"], payload.get(\"namespace\")\n",
        "             )\n",
        "             if success: status = \"success\"; response_payload = {\"upserted\": True, \"vector_id\": payload[\"vector_id\"]}\n",
        "             else: status = \"failed\"; errors.append({\"code\": \"VECTOR_UPSERT_FAILED\", \"message\": \"Failed to upsert vector.\"})\n",
        "         except (ValueError, TypeError) as ve: status = \"bad_request\"; errors.append({\"code\": \"VALIDATION_ERROR\", \"message\": str(ve)})\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error upserting vector: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.post(\"/vectors/search\", response_model=MizOkiResponse)\n",
        "    async def search_vector_endpoint(self, request: Dict = Body(...)) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             payload = request.get(\"payload\", {})\n",
        "             if not all(k in payload for k in [\"query_vector\", \"k\"]):\n",
        "                  raise ValueError(\"Invalid payload: 'query_vector' and 'k' are required.\")\n",
        "             results = await self.adapter.search_vector_index(\n",
        "                 payload[\"query_vector\"], payload[\"k\"],\n",
        "                 payload.get(\"namespace\"), payload.get(\"filter_dict\"),\n",
        "                 # payload.get(\"index_name\") # Allow overriding index if needed\n",
        "             )\n",
        "             status = \"success\"; response_payload = {\"results\": results}\n",
        "         except (ValueError, TypeError) as ve: status = \"bad_request\"; errors.append({\"code\": \"VALIDATION_ERROR\", \"message\": str(ve)})\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error searching vectors: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.post(\"/xai/decisions\", response_model=MizOkiResponse, status_code=201)\n",
        "    async def save_decision_endpoint(self, request: Dict = Body(...)) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             if not isinstance(request.get(\"payload\"), dict) or \"record\" not in request[\"payload\"]:\n",
        "                  raise ValueError(\"Invalid payload: 'payload.record' (dict) is required.\")\n",
        "             record = request[\"payload\"][\"record\"]\n",
        "             success = await self.adapter.save_decision_record(record)\n",
        "             if success: status = \"success\"; response_payload = {\"decision_id\": record.get(\"decision_id\"), \"saved\": True}\n",
        "             else: status = \"failed\"; errors.append({\"code\": \"XAI_SAVE_FAILED\", \"message\": \"Failed to save decision record.\"})\n",
        "         except (ValueError, TypeError) as ve: status = \"bad_request\"; errors.append({\"code\": \"VALIDATION_ERROR\", \"message\": str(ve)})\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error saving decision record: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.get(\"/xai/decisions/{decision_id}\", response_model=MizOkiResponse)\n",
        "    async def retrieve_decision_endpoint(self, request: Request, decision_id: str) -> Dict:\n",
        "         start_time = time.monotonic(); status = \"error\"; errors = []; response_payload = None\n",
        "         request_data = {\"request_id\": request.headers.get(\"X-Request-ID\"), \"trace_id\": request.headers.get(\"X-Trace-ID\")}\n",
        "         try:\n",
        "             await self.ensure_adapter()\n",
        "             # TODO: Permission check\n",
        "             record = await self.adapter.retrieve_decision_record(decision_id)\n",
        "             if record: status = \"success\"; response_payload = {\"decision_record\": record}\n",
        "             else: status = \"not_found\"; errors.append({\"code\": \"XAI_LOG_NOT_FOUND\", \"message\": f\"Decision record '{decision_id}' not found.\"})\n",
        "         except InitializationError as ie: status = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(ie)})\n",
        "         except ConnectionError as ce: status = \"service_unavailable\"; errors.append({\"code\": \"DB_CONNECTION_ERROR\", \"message\": str(ce)})\n",
        "         except Exception as e: status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_SERVER_ERROR\", \"message\": str(e)}); self.logger.error(f\"API Error retrieving decision record {decision_id}: {e}\", exc_info=True)\n",
        "         response = self._create_miz_oki_response(request_data, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "    # @app.get(\"/health\")\n",
        "    async def health_check_endpoint(self, request: Request) -> Dict:\n",
        "        \"\"\"Basic health check endpoint.\"\"\"\n",
        "        # Simple check, can be expanded to check DB connection status etc.\n",
        "        status = \"ok\" if self._adapter_initialized and self.adapter else \"error\"\n",
        "        return {\"status\": status, \"timestamp\": datetime.now(datetime.timezone.utc).isoformat()}\n",
        "\n",
        "\n",
        "# --- Initialization (Conceptual - Service deployed separately) ---\n",
        "# This would happen in the main application entry point (e.g., main.py for FastAPI)\n",
        "\n",
        "# kg_tool_service_instance: Optional[KnowledgeGraphToolService] = None\n",
        "\n",
        "# async def startup_kg_service():\n",
        "#     global kg_tool_service_instance\n",
        "#     if _config_obj:\n",
        "#         try:\n",
        "#             kg_tool_service_instance = KnowledgeGraphToolService(_config_obj)\n",
        "#             await kg_tool_service_instance.initialize_adapter() # Initialize on startup\n",
        "#             logger.info(\"KnowledgeGraphToolService initialized successfully.\")\n",
        "#         except Exception as e:\n",
        "#             logger.critical(f\"Failed to initialize KnowledgeGraphToolService: {e}\", exc_info=True)\n",
        "#             # Optionally prevent app startup if KG is critical\n",
        "#     else:\n",
        "#         logger.critical(\"Cannot initialize KnowledgeGraphToolService: CONFIG_OBJ not loaded.\")\n",
        "\n",
        "# async def shutdown_kg_service():\n",
        "#      if kg_tool_service_instance:\n",
        "#          await kg_tool_service_instance.close_adapter()\n",
        "#          logger.info(\"KnowledgeGraphToolService shut down.\")\n",
        "\n",
        "# Example FastAPI app setup (conceptual)\n",
        "# if FASTAPI_AVAILABLE:\n",
        "#     app = FastAPI(title=\"MIZ OKI 3.0 Knowledge Graph Tool Service\")\n",
        "#     app.add_event_handler(\"startup\", startup_kg_service)\n",
        "#     app.add_event_handler(\"shutdown\", shutdown_kg_service)\n",
        "#\n",
        "#     # Dependency function for FastAPI endpoints\n",
        "#     async def get_kg_service() -> KnowledgeGraphToolService:\n",
        "#         if kg_tool_service_instance is None or not kg_tool_service_instance._adapter_initialized:\n",
        "#              raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=\"KG Service not ready\")\n",
        "#         return kg_tool_service_instance\n",
        "#\n",
        "#     # Example endpoint definition using the service\n",
        "#     @app.post(\"/entities\", response_model=MizOkiResponse, status_code=status.HTTP_201_CREATED)\n",
        "#     async def add_entity(request: MizOkiRequest, service: KnowledgeGraphToolService = Depends(get_kg_service)):\n",
        "#         return await service.add_entity_endpoint(request.dict()) # Pass MIZ OKI dict\n",
        "#\n",
        "#     @app.get(\"/health\")\n",
        "#     async def health_check(service: KnowledgeGraphToolService = Depends(get_kg_service)):\n",
        "#          return await service.health_check_endpoint(None) # Pass dummy request or modify endpoint signature\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 KG Layer (Cell 3 - Reworked) ---\")\n",
        "print(\"Neo4jAsyncAdapter includes Vector DB routing & structured XAI storage.\")\n",
        "print(\"Conceptual FastAPI service layer added, handling MIZ OKI payloads.\")\n",
        "print(\"Requires full implementation of API endpoints and Vector DB client logic.\")\n",
        "print(\"Uses async operations for DB/API calls.\")\n",
        "print(\"-------------------------------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "U9vCReRfUIO1",
        "outputId": "a0cc6491-933d-49fa-a19c-f8d5db5a305d"
      },
      "id": "U9vCReRfUIO1",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-361a23d22eb2>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-361a23d22eb2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    4. Updated Cells (Continued)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Foundational Layer Implementation (Reworked)\n",
        "# Status: AKA logic structured for ADK/Tool deployment. Uses real dependencies/proxies.\n",
        "#         B.O.S.S/Experimentation loops trigger real clients (Vertex Workflows, Pub/Sub).\n",
        "#         Placeholders remain for specific KG queries, external data fetch details, MoE API client.\n",
        "#         State management (monitors, experiments) needs persistent backing in a real deployment.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "import asyncio\n",
        "from typing import Dict, Any, Optional, List, Union, Tuple, Callable\n",
        "import uuid\n",
        "from collections import deque, defaultdict, Counter # Added Counter\n",
        "import aiohttp # For MoE Registry API call\n",
        "\n",
        "# --- Assume Real Tool/Client Dependencies are Injected/Available ---\n",
        "# These would be provided during the initialization of the service/agent running this logic.\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    # Proxies for other MIZ OKI Tool APIs (representing deployed services)\n",
        "    if 'kg_tool_service_instance' not in globals(): raise NameError(\"kg_tool_service_instance proxy not found\") # Cell 3 Proxy\n",
        "    if 'foundation_model_client' not in globals(): raise NameError(\"foundation_model_client proxy not found\") # Cell 18 Proxy\n",
        "    if 'kd_tool' not in globals(): raise NameError(\"kd_tool (KnowledgeDistillationTool instance/proxy) not found\") # Cell 8 Proxy\n",
        "\n",
        "    # Real/Mock Clients for GCP Services\n",
        "    if '_workflow_executions_client' not in globals(): raise NameError(\"_workflow_executions_client not found\") # Cell 16 Client\n",
        "    if '_pubsub_client' not in globals(): raise NameError(\"_pubsub_client not found\") # Cell 8 Client\n",
        "\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _kg_tool_proxy = kg_tool_service_instance\n",
        "    _fm_client_proxy = foundation_model_client\n",
        "    _kd_tool_proxy = kd_tool\n",
        "    _workflow_client_proxy = _workflow_executions_client # Use real/mock client from Cell 16\n",
        "    _pubsub_client_proxy = _pubsub_client # Use real/mock client from Cell 8\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real/conceptual dependencies in Cell 4 (Reworked).\")\n",
        "\n",
        "except NameError as e:\n",
        "    logger.warning(f\"Dependency Error in Cell 4 ({e}). Using Mocks/Placeholders.\")\n",
        "    _real_dependencies = False\n",
        "    # --- Mock/Placeholder Setup ---\n",
        "    class MockKGTool:\n",
        "        async def execute_query(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"results\": [{'description': 'gap desc', 'domain': 'mock_domain'}]}}\n",
        "        async def add_entities_bulk_endpoint(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"new\": 1, \"updated\": 0, \"failed\": 0}}\n",
        "    class MockFMClientTool:\n",
        "        async def summarize(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"summary\": \"Mock summary.\"}}\n",
        "        async def extract_entities(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"entities\": [{\"entity\": \"mock\"}]}}\n",
        "    class MockKnowledgeDistillationTool:\n",
        "        async def distill_knowledge(self, request): await asyncio.sleep(0.1); return {\"status\": \"success\", \"payload\": {\"teacher_output_path\": \"gs://mock/kd_output.jsonl\"}}\n",
        "    class MockVertexWorkflowClient:\n",
        "        async def start_workflow(self, project, location, workflow_id, miz_oki_input): return f\"projects/{project}/locations/{location}/workflows/{workflow_id}/executions/exec_{uuid.uuid4().hex[:8]}\" # Return full name\n",
        "        async def get_execution(self, request): return MagicMock(state=ExecutionState.SUCCEEDED) # Needs refinement if state logic tested\n",
        "    class MockPubSubClient:\n",
        "        async def publish(self, topic, data_bytes): return f\"msg_{uuid.uuid4().hex[:8]}\"\n",
        "\n",
        "    # Define minimal config if needed\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ:\n",
        "        from dataclasses import dataclass, field\n",
        "        @dataclass class MockGcpConfig: project_id:Optional[str]=\"mock-proj\"; region:str=\"mock-region\"; gcs_bucket_name:Optional[str]=\"mock-bucket\"\n",
        "        @dataclass class MockAdkConfig: tool_registry_location: Optional[str] = 'gs://mock/tools'\n",
        "        @dataclass class MockFmDefaults: llama4_maverick: str = \"mock-llama\"; llama4_scout: str = \"mock-scout\"\n",
        "        @dataclass class MockFmConfig: defaults: MockFmDefaults = field(default_factory=MockFmDefaults)\n",
        "        @dataclass class MockVertexAIConfig: experiment_execution_workflow_id: str = \"mock-exp-exec\"; experiment_analysis_workflow_id: str = \"mock-exp-analysis\"\n",
        "        @dataclass class MockConfig: gcp: MockGcpConfig = field(default_factory=MockGcpConfig); mlops_trigger_topic:str=\"mock-topic\"; mlops_rl_train_topic:str=\"mock-rl\"; service_endpoints: ServiceEndpointsConfig = field(default_factory=ServiceEndpointsConfig); adk: MockAdkConfig = field(default_factory=MockAdkConfig); foundation_models: MockFmConfig = field(default_factory=MockFmConfig); vertex_ai: MockVertexAIConfig = field(default_factory=MockVertexAIConfig); miz_oki_schema_version: str = \"3.0\"; def get(self, key, default=None): parts=key.split('.'); val=self; try: [val := getattr(val, p) for p in parts]; return val; except: return default\n",
        "        _config_obj = MockConfig()\n",
        "\n",
        "    _kg_tool_proxy = MockKGTool()\n",
        "    _fm_client_proxy = MockFMClientTool()\n",
        "    _kd_tool_proxy = MockKnowledgeDistillationTool()\n",
        "    _workflow_client_proxy = MockVertexWorkflowClient()\n",
        "    _pubsub_client_proxy = MockPubSubClient()\n",
        "    # --- End Mock/Placeholder Setup ---\n",
        "\n",
        "logger = logging.getLogger('MIZ-OKI.FoundationalLayer')\n",
        "\n",
        "# --- Mixture of Experts Registry Manager (Metadata Focus - Reworked Async) ---\n",
        "class MixtureOfExpertsRegistryManager:\n",
        "    \"\"\" Manages expert model metadata via API (preferred) or static source. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig):\n",
        "        self.config = config\n",
        "        self.registry_api_endpoint = config.service_endpoints.moe_registry_api_endpoint\n",
        "        # Fallback to ADK tool registry location if API not set (less ideal for dynamic MoE)\n",
        "        self.static_registry_location = config.adk.tool_registry_location if not self.registry_api_endpoint else None\n",
        "        self.registry_source = self.registry_api_endpoint or self.static_registry_location\n",
        "        self.expert_registry: Dict[str, Dict] = {} # In-memory cache\n",
        "        self._cache_ttl_seconds = 300 # 5 minutes\n",
        "        self._last_cache_update_time = 0\n",
        "        self._lock = asyncio.Lock() # Prevent race conditions during refresh\n",
        "        self.session: Optional[aiohttp.ClientSession] = None\n",
        "        self.logger = logging.getLogger('MIZ-OKI.MoERegistryManager')\n",
        "        if not self.registry_source:\n",
        "            self.logger.warning(\"No MoE Registry API endpoint or static location configured! Expert routing will be disabled.\")\n",
        "        self.logger.info(f\"MoE Registry Manager initialized. Source: {self.registry_source}\")\n",
        "\n",
        "    async def initialize(self):\n",
        "        \"\"\"Initialize async session.\"\"\"\n",
        "        self.session = aiohttp.ClientSession()\n",
        "        await self._load_registry(force_refresh=True) # Initial load\n",
        "\n",
        "    async def cleanup(self):\n",
        "        \"\"\"Close async session.\"\"\"\n",
        "        if self.session: await self.session.close()\n",
        "\n",
        "    async def _load_registry(self, force_refresh: bool = False) -> None:\n",
        "        \"\"\" Loads or refreshes the registry cache from the source (API preferred) asynchronously. \"\"\"\n",
        "        now = time.monotonic()\n",
        "        async with self._lock:\n",
        "            if not force_refresh and (now - self._last_cache_update_time < self._cache_ttl_seconds):\n",
        "                return # Cache is still valid\n",
        "\n",
        "            self.logger.info(f\"Refreshing MoE registry cache from {self.registry_source}...\")\n",
        "            new_registry = {}\n",
        "            try:\n",
        "                if self.registry_api_endpoint:\n",
        "                    if not self.session: self.session = aiohttp.ClientSession() # Ensure session exists\n",
        "                    api_url = f\"{self.registry_api_endpoint.rstrip('/')}/experts\" # Assuming a '/experts' endpoint\n",
        "                    # TODO: Add authentication headers if the API requires it (e.g., API key, OIDC token)\n",
        "                    headers = {\"Accept\": \"application/json\"}\n",
        "                    async with self.session.get(api_url, headers=headers, timeout=10) as response:\n",
        "                        response.raise_for_status() # Raise exception for bad status codes\n",
        "                        experts_list = await response.json()\n",
        "                        if isinstance(experts_list, list):\n",
        "                            new_registry = {expert.get('expert_id'): expert for expert in experts_list if expert.get('expert_id')}\n",
        "                        else:\n",
        "                            self.logger.error(f\"Invalid format from MoE Registry API: Expected list, got {type(experts_list)}\")\n",
        "                elif self.static_registry_location and self.static_registry_location.startswith(\"gs://\"):\n",
        "                    # --- Placeholder: Implement GCS load using aio-gcsfs or sync thread ---\n",
        "                    # Example using sync thread:\n",
        "                    # def _load_from_gcs_sync():\n",
        "                    #     client = storage.Client(project=self.config.gcp.project_id)\n",
        "                    #     bucket_name = self.static_registry_location.split('/')[2]\n",
        "                    #     blob_name = '/'.join(self.static_registry_location.split('/')[3:])\n",
        "                    #     blob = client.bucket(bucket_name).blob(blob_name)\n",
        "                    #     content = blob.download_as_text()\n",
        "                    #     return json.loads(content)\n",
        "                    # registry_data = await asyncio.to_thread(_load_from_gcs_sync)\n",
        "                    # if isinstance(registry_data, dict): new_registry = registry_data # Assuming GCS file stores the dict directly\n",
        "                    self.logger.warning(f\"MoE Registry loading from GCS ({self.static_registry_location}) not implemented - using empty cache.\") # Placeholder\n",
        "                else:\n",
        "                    self.logger.warning(f\"Invalid or missing MoE registry source: {self.registry_source}\")\n",
        "\n",
        "                self.expert_registry = new_registry\n",
        "                self._last_cache_update_time = time.monotonic()\n",
        "                self.logger.info(f\"MoE registry cache refreshed. Found {len(self.expert_registry)} experts.\")\n",
        "            except aiohttp.ClientError as http_err:\n",
        "                self.logger.error(f\"HTTP error fetching MoE registry from API: {http_err}\")\n",
        "                self.expert_registry = {} # Clear cache on error\n",
        "            except json.JSONDecodeError as json_err:\n",
        "                 self.logger.error(f\"Failed to parse JSON from MoE registry source: {json_err}\")\n",
        "                 self.expert_registry = {}\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Failed to refresh MoE registry cache: {e}\", exc_info=True)\n",
        "                self.expert_registry = {} # Clear cache on error\n",
        "\n",
        "    async def get_expert_details(self, expert_id: str) -> Optional[Dict]:\n",
        "        \"\"\"Gets details for a specific expert from the cache.\"\"\"\n",
        "        await self._load_registry() # Ensure cache is reasonably fresh\n",
        "        details = self.expert_registry.get(expert_id)\n",
        "        if not details:\n",
        "            self.logger.warning(f\"Expert '{expert_id}' not found in registry cache.\")\n",
        "            # Optionally trigger a forced refresh if not found\n",
        "            # await self._load_registry(force_refresh=True)\n",
        "            # details = self.expert_registry.get(expert_id)\n",
        "        return details\n",
        "\n",
        "    async def find_expert_for_task(self, task_type: str, domain: Optional[str] = None, context: Optional[Dict] = None) -> Optional[str]:\n",
        "        \"\"\"Finds the best expert ID for a given task, domain, and context.\"\"\"\n",
        "        await self._load_registry() # Ensure cache is reasonably fresh\n",
        "        if not self.expert_registry:\n",
        "            self.logger.warning(\"MoE registry is empty. Cannot find expert.\")\n",
        "            return None\n",
        "\n",
        "        candidates = []\n",
        "        for expert_id, details in self.expert_registry.items():\n",
        "            # Basic filtering (active, task type match)\n",
        "            if details.get('status') != 'active': continue\n",
        "            if details.get('task_type') != task_type: continue\n",
        "\n",
        "            # Domain scoring (exact match = 1.0, partial/no match = lower score)\n",
        "            domain_score = 1.0 if not domain or details.get('domain') == domain else 0.5 # Simple scoring\n",
        "\n",
        "            # Performance scoring (use a primary metric like accuracy or F1)\n",
        "            # Default to 0.5 if metrics are missing\n",
        "            perf_score = details.get('evaluation_metrics', {}).get('accuracy', 0.5)\n",
        "\n",
        "            # Contextual scoring (Placeholder - could involve checking tags, input/output types, etc.)\n",
        "            context_score = 1.0 # Default if no context matching implemented\n",
        "\n",
        "            # Combine scores (example: weighted average)\n",
        "            final_score = (0.4 * domain_score) + (0.4 * perf_score) + (0.2 * context_score)\n",
        "            candidates.append((expert_id, final_score))\n",
        "\n",
        "        if not candidates:\n",
        "            self.logger.warning(f\"No suitable 'active' expert found for task '{task_type}' / domain '{domain}'.\")\n",
        "            return None\n",
        "\n",
        "        # Select the best candidate (highest score)\n",
        "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        best_expert_id = candidates[0][0]\n",
        "        self.logger.info(f\"Routed task '{task_type}'/{domain or '*'} to expert '{best_expert_id}' (Score: {candidates[0][1]:.2f}).\")\n",
        "        return best_expert_id\n",
        "\n",
        "# --- Conceptual ADK Agent Base ---\n",
        "class AdkAgentBase: # Placeholder Base Class for structure\n",
        "    \"\"\"Conceptual base for agents intended to run within ADK/Vertex Agent Engine.\"\"\"\n",
        "    def __init__(self, agent_id, config, tools=None):\n",
        "        self.agent_id = agent_id\n",
        "        self.config = config\n",
        "        # Tools would be injected proxies to other services/APIs\n",
        "        self.tools = tools or {}\n",
        "        # State management needs integration with the chosen persistence layer (e.g., Firestore, Workflow state)\n",
        "        self.state: Dict[str, Any] = {}\n",
        "        self.logger = logging.getLogger(f'MIZ-OKI.AdkAgent.{agent_id}')\n",
        "        self.logger.info(f\"ADK Agent '{agent_id}' logic initialized.\")\n",
        "\n",
        "    async def setup(self, initial_state: Optional[Dict] = None):\n",
        "        \"\"\"Load initial state. In real ADK, this might be handled by the framework.\"\"\"\n",
        "        self.state = initial_state or {}\n",
        "        # TODO: Implement loading state from persistent store (e.g., Firestore, DB) if needed\n",
        "        self.logger.info(f\"Agent {self.agent_id} setup complete.\")\n",
        "\n",
        "    async def invoke(self, input_data: Dict, context: Dict) -> Dict:\n",
        "        \"\"\"Core agent logic. Expects MIZ OKI input, returns MIZ OKI output.\"\"\"\n",
        "        raise NotImplementedError # Subclasses must implement this\n",
        "\n",
        "    async def save_state(self):\n",
        "        \"\"\"Persist agent state. In real ADK, this might be handled by the framework.\"\"\"\n",
        "        # TODO: Implement saving state to persistent store\n",
        "        logger.warning(f\"Agent {self.agent_id}: save_state not implemented - state is ephemeral.\")\n",
        "        pass\n",
        "\n",
        "# --- Autonomous Knowledge Agent (Reworked Logic for ADK/Tool) ---\n",
        "class AutonomousKnowledgeAgentLogic(AdkAgentBase):\n",
        "    \"\"\" Implements AKA logic. Runs B.O.S.S. loop & experimentation asynchronously. Designed as a deployable Tool/Service. \"\"\"\n",
        "    def __init__(self, agent_id: str, config: EnhancedConfig, kg_tool_proxy: Any, fm_client_proxy: Any, kd_tool_proxy: Any,\n",
        "                 moe_registry: MixtureOfExpertsRegistryManager, workflow_client_proxy: Any, pubsub_client_proxy: Any):\n",
        "        # Inject proxies/clients for dependencies\n",
        "        tools = {\"kg\": kg_tool_proxy, \"fm\": fm_client_proxy, \"kd\": kd_tool_proxy}\n",
        "        super().__init__(agent_id, config, tools=tools)\n",
        "        self.moe_registry = moe_registry\n",
        "        self.workflow_client = workflow_client_proxy # Use the injected client proxy\n",
        "        self.pubsub_client = pubsub_client_proxy     # Use the injected client proxy\n",
        "        self.project = config.gcp.project_id\n",
        "        self.location = config.gcp.region\n",
        "        self.mlops_trigger_topic_name = config.mlops_trigger_topic\n",
        "        self.mlops_rl_train_topic_name = config.mlops_rl_train_topic\n",
        "        # State attributes - NEED PERSISTENCE\n",
        "        self.discovery_monitors: Dict[str, Dict] = {}\n",
        "        self.experiments: Dict[str, Dict] = {}\n",
        "        # In-memory log, consider persistent logging via Cloud Logging\n",
        "        self.agent_history = deque(maxlen=500)\n",
        "\n",
        "    async def setup(self, initial_state: Optional[Dict] = None):\n",
        "        \"\"\" Load monitors, experiments state from persistent store or initial config. \"\"\"\n",
        "        await super().setup(initial_state)\n",
        "        # --- TODO: Implement loading state from persistent store ---\n",
        "        # Example: Load from Firestore or state passed via workflow input\n",
        "        self.discovery_monitors = self.state.get(\"discovery_monitors\", {})\n",
        "        self.experiments = self.state.get(\"experiments\", {})\n",
        "        # --- End TODO ---\n",
        "        # Add a default monitor if none exist (for demonstration)\n",
        "        if not self.discovery_monitors:\n",
        "            self.add_discovery_monitor(\"default_news_monitor\", \"api\", \"market_news_api\", 6, ['summarize', 'extract_entities'])\n",
        "        self.logger.info(\"AutonomousKnowledgeAgent setup complete.\")\n",
        "\n",
        "    def add_discovery_monitor(self, monitor_id, source_type, query_or_key, frequency_hours, processing_pipeline):\n",
        "        \"\"\" Adds/Updates a discovery monitor config. State needs persistence. \"\"\"\n",
        "        self.discovery_monitors[monitor_id] = {\n",
        "            \"source_type\": source_type,\n",
        "            \"query_or_key\": query_or_key,\n",
        "            \"frequency_hours\": frequency_hours,\n",
        "            \"processing_pipeline\": processing_pipeline,\n",
        "            \"last_checked\": None,\n",
        "            \"status\": \"active\"\n",
        "        }\n",
        "        self.state[\"discovery_monitors\"] = self.discovery_monitors # Update conceptual state\n",
        "        # TODO: Call self.save_state() or rely on ADK framework persistence\n",
        "        self.logger.info(f\"Added/Updated discovery monitor: {monitor_id}\")\n",
        "\n",
        "    async def invoke(self, input_data: Dict, context: Optional[Dict]=None) -> Dict:\n",
        "        \"\"\"\n",
        "        Handles tasks based on MIZ OKI input payload. Returns MIZ OKI response.\n",
        "        This is the main entry point when called as a Tool/Service.\n",
        "        \"\"\"\n",
        "        # --- MIZ OKI Payload Parsing ---\n",
        "        miz_oki_version = input_data.get(\"miz_oki_version\", \"unknown\")\n",
        "        request_id = input_data.get(\"request_id\", f\"req_aka_{uuid.uuid4().hex[:8]}\")\n",
        "        trace_id = input_data.get(\"trace_id\", f\"trace_aka_{uuid.uuid4().hex[:8]}\")\n",
        "        workflow_execution_id = input_data.get(\"workflow_execution_id\") # Passed by Vertex Workflow\n",
        "        step_id = input_data.get(\"step_id\") # Passed by Vertex Workflow\n",
        "        source_component = input_data.get(\"source_component\")\n",
        "        payload = input_data.get(\"payload\", {})\n",
        "        task_type = payload.get(\"task_type\", \"default_cycle\") # e.g., \"run_discovery_cycle\", \"run_boss_cycle\"\n",
        "\n",
        "        self.logger.info(f\"AKA invoke started. Task: '{task_type}', Trace: {trace_id}, Request: {request_id}\")\n",
        "\n",
        "        # --- Prepare MIZ OKI Response Structure ---\n",
        "        response = {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_id, \"trace_id\": trace_id,\n",
        "            \"workflow_execution_id\": workflow_execution_id, \"step_id\": step_id,\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": self.agent_id, # This agent's ID\n",
        "            \"target_component\": source_component, # Respond to caller\n",
        "            \"status\": \"unknown\", # Will be updated\n",
        "            \"payload\": {\"results\": None},\n",
        "            \"error_details\": None,\n",
        "            \"metadata\": {}\n",
        "        }\n",
        "        start_time = time.monotonic()\n",
        "        results = None; errors = []\n",
        "\n",
        "        try:\n",
        "            # --- Dependency Check ---\n",
        "            if not all([self.tools.get(\"kg\"), self.tools.get(\"fm\"), self.tools.get(\"kd\"),\n",
        "                        self.moe_registry, self.workflow_client, self.pubsub_client]):\n",
        "                raise InitializationError(\"AKA missing critical dependencies (tools or clients).\")\n",
        "\n",
        "            # --- Task Routing ---\n",
        "            if task_type == \"run_discovery_cycle\":\n",
        "                results = await self.run_discovery_cycle(trace_id)\n",
        "            elif task_type == \"run_boss_cycle\":\n",
        "                results = await self.run_boss_cycle(trace_id)\n",
        "            elif task_type == \"run_experimentation_cycle\":\n",
        "                results = await self.run_experimentation_cycle(trace_id)\n",
        "            elif task_type == \"default_cycle\":\n",
        "                 # Example: Run discovery and B.O.S.S. sequentially\n",
        "                 discovery_results = await self.run_discovery_cycle(trace_id)\n",
        "                 boss_results = await self.run_boss_cycle(trace_id)\n",
        "                 # Experimentation might be triggered separately or based on B.O.S.S. outcomes\n",
        "                 results = {\"discovery_summary\": discovery_results, \"boss_summary\": boss_results}\n",
        "            else:\n",
        "                self.logger.warning(f\"Unsupported task type for AKA: {task_type}\")\n",
        "                response[\"status\"] = \"bad_request\"\n",
        "                errors.append({\"code\": \"UNSUPPORTED_TASK\", \"message\": f\"Task type '{task_type}' not supported by {self.agent_id}.\"})\n",
        "\n",
        "            # --- Update Response Status ---\n",
        "            if response[\"status\"] == \"unknown\": # If no error set yet\n",
        "                 response[\"status\"] = \"success\"\n",
        "                 response[\"payload\"][\"results\"] = results\n",
        "\n",
        "        except InitializationError as init_e:\n",
        "             self.logger.critical(f\"AKA Initialization Error during invoke: {init_e}\")\n",
        "             response[\"status\"] = \"service_unavailable\"; errors.append({\"code\": \"INIT_ERROR\", \"message\": str(init_e)})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during AKA invoke for task '{task_type}' (Trace: {trace_id}): {e}\", exc_info=True)\n",
        "            response[\"status\"] = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": str(e)})\n",
        "\n",
        "        # --- Finalize Response ---\n",
        "        # TODO: Call self.save_state() or rely on ADK framework persistence\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        self.logger.info(f\"AKA invoke finished. Status: {response['status']}. Duration: {response['metadata']['processing_duration_ms']:.2f} ms\")\n",
        "        return response\n",
        "\n",
        "    # --- Research & Discovery (Reworked Async) ---\n",
        "    async def run_discovery_cycle(self, trace_id: str) -> Dict:\n",
        "        \"\"\" Runs active discovery monitors asynchronously. \"\"\"\n",
        "        self.logger.info(f\"Starting external discovery cycle (Trace: {trace_id})...\")\n",
        "        now_dt = datetime.datetime.now(datetime.timezone.utc); now_iso = now_dt.isoformat()\n",
        "        triggered_monitors = []; tasks = []\n",
        "        # --- TODO: Load self.discovery_monitors from persistent state ---\n",
        "        for monitor_id, monitor in self.discovery_monitors.items():\n",
        "            if monitor.get(\"status\") != \"active\": continue\n",
        "            frequency = monitor.get(\"frequency_hours\", 6); last_checked_iso = monitor.get(\"last_checked\"); should_run = True\n",
        "            if last_checked_iso:\n",
        "                try: last_checked_dt = datetime.datetime.fromisoformat(last_checked_iso.replace('Z', '+00:00')) # Handle Z timezone\n",
        "                except ValueError: last_checked_dt = None\n",
        "                if last_checked_dt and (now_dt - last_checked_dt < datetime.timedelta(hours=frequency)): should_run = False\n",
        "            if should_run:\n",
        "                tasks.append(self._run_single_monitor(monitor_id, monitor, now_iso, trace_id))\n",
        "                triggered_monitors.append(monitor_id)\n",
        "\n",
        "        run_summary = {\"triggered_count\": len(triggered_monitors), \"triggered_ids\": triggered_monitors, \"results\": []}\n",
        "        if tasks:\n",
        "             results_or_exceptions = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "             processed_results = []\n",
        "             for i, res in enumerate(results_or_exceptions):\n",
        "                 monitor_id = triggered_monitors[i]\n",
        "                 if isinstance(res, Exception):\n",
        "                     logger.error(f\"Monitor {monitor_id} failed: {res}\")\n",
        "                     processed_results.append({\"monitor_id\": monitor_id, \"status\": \"error\", \"error\": str(res)})\n",
        "                 elif isinstance(res, dict):\n",
        "                     processed_results.append(res)\n",
        "                     # Update last_checked only on success\n",
        "                     if res.get(\"status\") == \"success\":\n",
        "                         self.discovery_monitors[monitor_id][\"last_checked\"] = now_iso\n",
        "                 else: # Should not happen if _run_single_monitor returns dict\n",
        "                      processed_results.append({\"monitor_id\": monitor_id, \"status\": \"error\", \"error\": \"Unknown result type\"})\n",
        "             run_summary[\"results\"] = processed_results\n",
        "        # --- TODO: Persist self.discovery_monitors state ---\n",
        "        self.logger.info(f\"External discovery cycle finished (Trace: {trace_id}). Triggered {len(triggered_monitors)} monitors.\")\n",
        "        return run_summary\n",
        "\n",
        "    async def _run_single_monitor(self, monitor_id: str, monitor: Dict, run_time_iso: str, trace_id: str) -> Dict:\n",
        "        \"\"\" Executes a single monitor asynchronously using Tool API proxies. \"\"\"\n",
        "        run_log = {\"monitor_id\": monitor_id, \"trace_id\": trace_id, \"timestamp\": run_time_iso, \"status\": \"started\", \"steps\": {}}\n",
        "        kg_tool = self.tools.get(\"kg\")\n",
        "        fm_tool = self.tools.get(\"fm\")\n",
        "        if not kg_tool or not fm_tool:\n",
        "            return {**run_log, \"status\": \"error\", \"error\": \"KG Tool or FM Client Tool proxy unavailable\"}\n",
        "\n",
        "        try:\n",
        "            # 1. Fetch External Data (Placeholder - Needs dedicated Tool)\n",
        "            step_start = time.monotonic()\n",
        "            # --- TODO: Replace placeholder with API call to ExternalDataFetcherTool ---\n",
        "            # fetch_request = {\"payload\": {\"source_type\": monitor[\"source_type\"], \"query_or_key\": monitor[\"query_or_key\"]}, \"trace_id\": trace_id}\n",
        "            # fetch_response = await external_data_fetcher_proxy(request=fetch_request)\n",
        "            # raw_findings = fetch_response.get(\"payload\", {}).get(\"findings\", []) if fetch_response.get(\"status\") == \"success\" else []\n",
        "            raw_findings = await self._fetch_external_data(monitor[\"source_type\"], monitor[\"query_or_key\"]) # Using placeholder\n",
        "            # --- End TODO ---\n",
        "            run_log[\"steps\"][\"fetch_data\"] = {\"duration_ms\": (time.monotonic() - step_start) * 1000, \"findings_count\": len(raw_findings)}\n",
        "            if not raw_findings: run_log[\"status\"] = \"no_new_findings\"; return run_log\n",
        "\n",
        "            # 2. Process Findings (via FM Client Tool API Proxy)\n",
        "            step_start = time.monotonic()\n",
        "            processed_insights = await self._process_findings(raw_findings, monitor.get(\"processing_pipeline\", []), fm_tool, trace_id)\n",
        "            run_log[\"steps\"][\"process_findings\"] = {\"duration_ms\": (time.monotonic() - step_start) * 1000, \"processed_count\": len(processed_insights)}\n",
        "            if not processed_insights: run_log[\"status\"] = \"no_insights_processed\"; return run_log\n",
        "\n",
        "            # 3. Integrate Insights (via KG Tool API Proxy)\n",
        "            step_start = time.monotonic()\n",
        "            integration_results = await self._integrate_insights(processed_insights, f\"discovery:{monitor_id}\", kg_tool, trace_id)\n",
        "            run_log[\"steps\"][\"integrate_insights\"] = {\"duration_ms\": (time.monotonic() - step_start) * 1000, **integration_results}\n",
        "            run_log[\"status\"] = \"success\" if integration_results.get(\"integrated_count\", 0) > 0 else \"integration_failed\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error running monitor {monitor_id} async: {e}\", exc_info=True)\n",
        "            run_log[\"status\"] = \"error\"; run_log[\"error\"] = str(e)\n",
        "\n",
        "        self.agent_history.append(run_log) # Append to in-memory history\n",
        "        return run_log\n",
        "\n",
        "    async def _fetch_external_data(self, source_type: str, query_or_key: str) -> List[Dict]:\n",
        "        \"\"\" Placeholder: Should call a dedicated External Data Fetcher Tool/Service via API. \"\"\"\n",
        "        self.logger.debug(f\"Fetching external data async: Type={source_type} (Placeholder - Needs dedicated tool call)\")\n",
        "        # --- Placeholder Logic ---\n",
        "        await asyncio.sleep(random.uniform(0.05, 0.2)) # Simulate API call latency\n",
        "        if source_type == 'api':\n",
        "            return [{\"id\": f\"api_{uuid.uuid4().hex[:6]}\", \"title\": f\"Simulated API Result for {query_or_key}\", \"content\": \"Content from API...\", \"source\": query_or_key, \"publishedAt\": datetime.now().isoformat()}]\n",
        "        if source_type == 'web_search':\n",
        "            return [{\"id\": f\"web_{uuid.uuid4().hex[:6]}\", \"title\": f\"Simulated Web Result for {query_or_key}\", \"snippet\": \"Snippet from web search...\", \"url\": f\"http://example.com/{uuid.uuid4().hex[:6]}\", \"source\": \"web_search\"}]\n",
        "        return []\n",
        "\n",
        "    async def _process_findings(self, findings: List[Dict], pipeline: List[str], fm_tool: Any, trace_id: str) -> List[Dict]:\n",
        "        \"\"\" Process raw findings using FM Client Tool API via proxy. \"\"\"\n",
        "        if not fm_tool: logger.error(\"FM Client Tool proxy unavailable for processing findings.\"); return []\n",
        "        model_alias = self.config.foundation_models.defaults.llama4_scout # Use scout for processing\n",
        "\n",
        "        async def process_single(finding: Dict) -> Optional[Dict]:\n",
        "            text = finding.get('content') or finding.get('snippet') or finding.get('title') or ''\n",
        "            finding_id = finding.get('id') or finding.get('url') or f\"f_{uuid.uuid4().hex[:6]}\"\n",
        "            processed = {\"original_id\": finding_id, **finding, \"processing_error\": None}\n",
        "            if not text: return processed # Return original if no text to process\n",
        "\n",
        "            try:\n",
        "                for step in pipeline:\n",
        "                    fm_request = {\n",
        "                        \"payload\": {\"model_alias\": model_alias},\n",
        "                        \"trace_id\": trace_id, \"request_id\": f\"fm_{step}_{finding_id}\"\n",
        "                    }\n",
        "                    if step == 'summarize':\n",
        "                        fm_request[\"payload\"][\"prompt\"] = f\"Summarize the following content:\\n{text}\"\n",
        "                        fm_request[\"payload\"][\"max_tokens\"] = 150 # Example length\n",
        "                        fm_response = await fm_tool.generate_text(input_data=fm_request) # API Call via proxy\n",
        "                        if fm_response.get(\"status\") == \"success\": processed['summary_ai'] = fm_response.get(\"payload\", {}).get(\"generated_text\")\n",
        "                        else: raise RuntimeError(f\"Summarize API call failed: {fm_response.get('error_details')}\")\n",
        "                    elif step == 'extract_entities':\n",
        "                        fm_request[\"payload\"][\"prompt\"] = f\"Extract key entities (people, orgs, locations, topics) from:\\n{text}\\nOutput as JSON list: [{{'entity': '...', 'type': '...'}}]\"\n",
        "                        fm_response = await fm_tool.generate_text(input_data=fm_request) # API Call via proxy\n",
        "                        if fm_response.get(\"status\") == \"success\":\n",
        "                            try: processed['entities_ai'] = json.loads(fm_response.get(\"payload\", {}).get(\"generated_text\", \"[]\"))\n",
        "                            except json.JSONDecodeError: processed['entities_ai'] = [{\"error\": \"Invalid JSON from LLM\"}]; logger.warning(\"Failed to parse JSON entities from LLM.\")\n",
        "                        else: raise RuntimeError(f\"Extract Entities API call failed: {fm_response.get('error_details')}\")\n",
        "                    # Add other processing steps (e.g., sentiment analysis via fm_tool.analyze)\n",
        "                return processed\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing finding async '{finding_id}': {e}\")\n",
        "                processed['processing_error'] = str(e)\n",
        "                return processed\n",
        "\n",
        "        tasks = [process_single(f) for f in findings if isinstance(f, dict)]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        return [r for r in results if r is not None]\n",
        "\n",
        "    async def _integrate_insights(self, insights: List[Dict], source_prefix: str, kg_tool: Any, trace_id: str) -> Dict:\n",
        "        \"\"\" Integrates insights into KG via KG Tool API proxy. \"\"\"\n",
        "        if not kg_tool or not hasattr(kg_tool, 'add_entities_bulk_endpoint'):\n",
        "            logger.error(\"KG Tool API proxy unavailable or missing 'add_entities_bulk_endpoint' method.\")\n",
        "            return {\"integrated_count\": 0, \"failed_count\": len(insights)}\n",
        "\n",
        "        kg_entities = []\n",
        "        for insight in insights:\n",
        "             # Create a unique ID for the insight node itself\n",
        "             insight_node_id = insight.get('original_id') or f\"{source_prefix}:{uuid.uuid4().hex[:8]}\"\n",
        "             hints = {\"type\": \"ExternalInsight\", \"source\": source_prefix, \"original_id\": insight_node_id}\n",
        "             # Prepare properties for the insight node\n",
        "             entity_dict = {\n",
        "                 \"_resolution_hints\": hints,\n",
        "                 \"mizId\": insight_node_id, # Use the generated ID as mizId\n",
        "                 \"type\": \"ExternalInsight\",\n",
        "                 \"title\": insight.get(\"title\"),\n",
        "                 \"link\": insight.get(\"url\"),\n",
        "                 \"source\": source_prefix,\n",
        "                 \"published_at\": _parse_date(insight.get(\"publishedAt\")), # Use helper\n",
        "                 \"summary_ai\": insight.get(\"summary_ai\"),\n",
        "                 # Store extracted entities as a JSON string or list property\n",
        "                 \"entities_ai_json\": json.dumps(insight.get(\"entities_ai\", []), default=str),\n",
        "                 \"processing_error\": insight.get(\"processing_error\"),\n",
        "                 \"processed_at\": datetime.now(datetime.timezone.utc).isoformat()\n",
        "             }\n",
        "             kg_entities.append(entity_dict)\n",
        "             # TODO: Optionally create nodes for extracted entities_ai and link them to the insight node\n",
        "\n",
        "        if not kg_entities:\n",
        "            return {\"integrated_count\": 0, \"failed_count\": 0}\n",
        "\n",
        "        self.logger.info(f\"Integrating {len(kg_entities)} insights into KG via Tool API from {source_prefix}...\")\n",
        "        try:\n",
        "            # Call KG Tool API endpoint via proxy (MIZ OKI payload)\n",
        "            kg_request = {\n",
        "                \"payload\": {\"entities\": kg_entities, \"source\": source_prefix},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"kg_integrate_{source_prefix}\"\n",
        "            }\n",
        "            kg_response = await kg_tool.add_entities_bulk_endpoint(request=kg_request) # Call proxy method\n",
        "\n",
        "            # Parse MIZ OKI response from KG Tool\n",
        "            if kg_response.get(\"status\") in [\"success\", \"partial_success\"]:\n",
        "                kg_payload = kg_response.get(\"payload\", {})\n",
        "                integrated_count = kg_payload.get(\"new\", 0) + kg_payload.get(\"updated\", 0)\n",
        "                failed_count = kg_payload.get(\"failed\", 0)\n",
        "                if failed_count > 0: logger.warning(f\"KG integration via Tool API: {failed_count} failures reported by KG Tool.\")\n",
        "                self.logger.info(f\"Insight integration via Tool API Complete. Successful: {integrated_count}, Failed: {failed_count}\")\n",
        "                return {\"integrated_count\": integrated_count, \"failed_count\": failed_count}\n",
        "            else:\n",
        "                raise RuntimeError(f\"KG Tool Bulk API call failed: {kg_response.get('error_details')}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to call KG Tool Bulk API via proxy: {e}\", exc_info=True)\n",
        "            return {\"integrated_count\": 0, \"failed_count\": len(kg_entities)}\n",
        "\n",
        "    # --- B.O.S.S. Self-Teaching Loop (Reworked Async) ---\n",
        "    async def run_boss_cycle(self, trace_id: str) -> Dict:\n",
        "        \"\"\" Executes one cycle of the B.O.S.S. loop asynchronously. \"\"\"\n",
        "        self.logger.info(f\"Starting B.O.S.S. cycle (Trace: {trace_id})...\")\n",
        "        cycle_log = {\"cycle_id\": trace_id, \"timestamp_start\": datetime.now(datetime.timezone.utc).isoformat(), \"status\": \"started\", \"steps\": {}}\n",
        "        kg_tool = self.tools.get(\"kg\")\n",
        "        kd_tool = self.tools.get(\"kd\")\n",
        "        if not kg_tool or not kd_tool or not self.pubsub_client:\n",
        "            error_msg = \"Missing dependencies (KG Tool, KD Tool, or PubSub Client proxies).\"\n",
        "            logger.error(f\"B.O.S.S. (Trace: {trace_id}): {error_msg}\")\n",
        "            return {**cycle_log, \"status\": \"error\", \"error\": error_msg}\n",
        "\n",
        "        try:\n",
        "            # 1. Identify Gaps (Call KG Tool API Proxy)\n",
        "            start_step = time.monotonic()\n",
        "            gaps = await self._identify_knowledge_gaps(kg_tool, trace_id)\n",
        "            cycle_log[\"steps\"][\"identify_gaps\"] = {\"duration_ms\": (time.monotonic()-start_step)*1000, \"status\": \"success\", \"gaps_found\": len(gaps)}\n",
        "            if not gaps:\n",
        "                self.logger.info(f\"B.O.S.S. (Trace: {trace_id}): No significant knowledge gaps identified.\"); cycle_log[\"status\"] = \"no_gaps\"; return cycle_log\n",
        "\n",
        "            # 2. Prioritize Gap (Simple: take the first one)\n",
        "            # TODO: Implement more sophisticated prioritization logic\n",
        "            selected_gap = gaps[0]\n",
        "            cycle_log[\"selected_gap\"] = selected_gap\n",
        "            self.logger.info(f\"B.O.S.S. (Trace: {trace_id}): Selected gap: {selected_gap.get('description')}\")\n",
        "\n",
        "            # 3. Research Gap (Call External Fetcher Tool -> FM Client Tool API Proxy)\n",
        "            start_step = time.monotonic()\n",
        "            research_findings = await self._trigger_subagent_research(selected_gap.get('description'), trace_id)\n",
        "            cycle_log[\"steps\"][\"research\"] = {\"duration_ms\": (time.monotonic()-start_step)*1000, \"status\": \"success\", \"findings_count\": len(research_findings)}\n",
        "            if not research_findings:\n",
        "                self.logger.warning(f\"B.O.S.S. (Trace: {trace_id}): Research yielded no findings for gap '{selected_gap.get('description')}'.\"); cycle_log[\"status\"] = \"research_failed\"; return cycle_log\n",
        "\n",
        "            # 4. Synthesize & Trigger Training (Call KD Tool API Proxy -> Pub/Sub Client Proxy)\n",
        "            start_step = time.monotonic()\n",
        "            mini_model_info = await self._generate_and_trigger_mini_model_training(selected_gap, research_findings, kd_tool, trace_id)\n",
        "            cycle_log[\"steps\"][\"synthesize_trigger\"] = {\"duration_ms\": (time.monotonic()-start_step)*1000}\n",
        "            if not mini_model_info or mini_model_info.get(\"status\") != \"training_triggered\":\n",
        "                 self.logger.error(f\"B.O.S.S. (Trace: {trace_id}): Mini-model generation/trigger failed.\"); cycle_log[\"status\"] = \"synthesis_failed\"; cycle_log[\"steps\"][\"synthesize_trigger\"][\"status\"] = \"failed\"; return cycle_log\n",
        "            cycle_log[\"steps\"][\"synthesize_trigger\"].update(mini_model_info)\n",
        "            self.logger.info(f\"B.O.S.S. (Trace: {trace_id}): Triggered mini-model training via Pub/Sub ({mini_model_info.get('pipeline_trigger_message_id')}).\")\n",
        "            cycle_log[\"status\"] = \"training_triggered\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during B.O.S.S. cycle (Trace: {trace_id}): {e}\", exc_info=True)\n",
        "            cycle_log[\"status\"] = \"error\"; cycle_log[\"error\"] = str(e)\n",
        "\n",
        "        self.agent_history.append(cycle_log) # Append to in-memory history\n",
        "        return cycle_log\n",
        "\n",
        "    async def _identify_knowledge_gaps(self, kg_tool: Any, trace_id: str) -> List[Dict]:\n",
        "        \"\"\" Identifies knowledge gaps by querying the KG Tool API proxy. \"\"\"\n",
        "        self.logger.debug(f\"B.O.S.S.: Identifying knowledge gaps via KG Tool API...\")\n",
        "        # Conceptual Query (adjust based on actual KG schema for tasks/models/performance)\n",
        "        # This query looks for TaskDefinition nodes that either have no associated Model\n",
        "        # or where the average performance score of associated Models is below a threshold (e.g., 0.6).\n",
        "        query = \"\"\"\n",
        "        MATCH (t:TaskDefinition)\n",
        "        OPTIONAL MATCH (t)<-[:PERFORMS_TASK]-(m:Model) // Assuming Model nodes have performance_score\n",
        "        WITH t, count(m) AS modelCount, avg(m.performance_score) AS avgModelPerf\n",
        "        WHERE modelCount = 0 OR avgModelPerf < $performance_threshold\n",
        "        RETURN t.description AS description,\n",
        "               t.domain AS domain,\n",
        "               t.required_task_type AS potential_task_type,\n",
        "               // Calculate a gap score (higher for missing models or lower performance)\n",
        "               (CASE WHEN modelCount = 0 THEN 1.0 ELSE ($performance_threshold - avgModelPerf) / $performance_threshold END) AS gap_score\n",
        "        ORDER BY gap_score DESC\n",
        "        LIMIT 5 // Limit the number of gaps returned\n",
        "        \"\"\"\n",
        "        params = {\"performance_threshold\": 0.6} # Example threshold\n",
        "        try:\n",
        "             # Call KG Tool API proxy (MIZ OKI payload)\n",
        "             kg_request = {\n",
        "                 \"payload\": {\"query\": query, \"parameters\": params},\n",
        "                 \"trace_id\": trace_id, \"request_id\": f\"kg_find_gaps_{trace_id}\"\n",
        "             }\n",
        "             kg_response = await kg_tool.execute_query(request=kg_request) # Call proxy method\n",
        "\n",
        "             if kg_response.get(\"status\") == \"success\":\n",
        "                 gaps = kg_response.get(\"payload\", {}).get(\"results\", [])\n",
        "                 self.logger.info(f\"B.O.S.S.: Found {len(gaps)} potential knowledge gaps.\")\n",
        "                 return gaps\n",
        "             else:\n",
        "                 logger.error(f\"KG Tool API query for gaps failed: {kg_response.get('error_details')}\")\n",
        "                 return []\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to query KG Tool API for gaps: {e}\", exc_info=True)\n",
        "            return []\n",
        "\n",
        "    async def _trigger_subagent_research(self, gap_description: str, trace_id: str) -> List[Dict]:\n",
        "        \"\"\" Triggers research via dedicated tools/agents (placeholder) and processes results. \"\"\"\n",
        "        self.logger.debug(f\"B.O.S.S. (Trace: {trace_id}): Triggering research for gap: {gap_description}\")\n",
        "        # --- TODO: Replace placeholder with API call to a dedicated ResearchAgent/Tool ---\n",
        "        # research_request = {\"payload\": {\"query\": f\"Find information and techniques related to: {gap_description}\"}, \"trace_id\": trace_id}\n",
        "        # research_response = await research_agent_proxy(request=research_request)\n",
        "        # findings = research_response.get(\"payload\", {}).get(\"findings\", []) if research_response.get(\"status\") == \"success\" else []\n",
        "        findings = await self._fetch_external_data(source_type='web_search', query_or_key=f\"techniques for {gap_description}\") # Using placeholder\n",
        "        # --- End TODO ---\n",
        "        if not findings: return []\n",
        "        # Process findings using FM Client Tool API Proxy\n",
        "        return await self._process_findings(findings, ['summarize', 'extract_entities'], self.tools.get(\"fm\"), trace_id)\n",
        "\n",
        "    async def _generate_and_trigger_mini_model_training(self, gap_details: Dict, research_findings: List[Dict], kd_tool: Any, trace_id: str) -> Optional[Dict]:\n",
        "        \"\"\" Generates teacher outputs via KD Tool API proxy and triggers MLOps via Pub/Sub client proxy. \"\"\"\n",
        "        task_desc = gap_details.get('description', 'Unknown_Task'); task_type = gap_details.get('potential_task_type', 'classification'); domain = gap_details.get('domain', 'boss_gen')\n",
        "        self.logger.info(f\"B.O.S.S. (Trace: {trace_id}): Synthesizing mini-model for: {task_desc}\")\n",
        "\n",
        "        # Prepare input data for the teacher model (KD Tool)\n",
        "        # Use summaries or relevant parts of research findings\n",
        "        kd_input_items = [f.get('summary_ai', '') or f.get('content', '') or f.get('snippet', '') for f in research_findings]\n",
        "        kd_input_items = [item for item in kd_input_items if item] # Filter out empty inputs\n",
        "        if not kd_input_items:\n",
        "            logger.warning(\"B.O.S.S.: No suitable content from research findings for KD teacher input.\")\n",
        "            return {\"status\": \"failed\", \"error\": \"No input data for teacher model\"}\n",
        "\n",
        "        # --- TODO: Save full input dataset to GCS asynchronously ---\n",
        "        # This requires a robust async GCS write implementation (e.g., using aio-gcsfs)\n",
        "        # For now, we'll use a placeholder path and pass preview data to KD tool.\n",
        "        temp_kd_input_uri = f\"gs://{self.config.gcp.gcs_bucket_name}/kd_inputs/boss_{domain}_{task_type}_{uuid.uuid4().hex[:8]}.jsonl\"\n",
        "        logger.warning(f\"B.O.S.S.: Placeholder - Full KD input data should be saved to {temp_kd_input_uri}\")\n",
        "        # --- End TODO ---\n",
        "\n",
        "        student_model_name = f\"boss_mini_{domain}_{task_type}_{uuid.uuid4().hex[:6]}\"\n",
        "        student_model_details = {\"name\": student_model_name, \"architecture\": self.config.get('learning_flows.kd.default_student_architecture', \"distilbert-base-uncased\"), \"task_type\": task_type}\n",
        "        # Reference to the dataset (using placeholder URI and preview)\n",
        "        dataset_ref_for_kd = {\"gcs_uri\": temp_kd_input_uri, \"inputs_preview\": kd_input_items[:10]} # Pass preview\n",
        "        distillation_params = {\"teacher_model_alias\": self.config.get('learning_flows.kd.teacher_model_alias', self.config.foundation_models.defaults.llama4_maverick)}\n",
        "        teacher_output_uri = None\n",
        "\n",
        "        try: # Call KD Tool API via proxy\n",
        "            kd_request = {\n",
        "                \"payload\": {\n",
        "                    \"student_model_details\": student_model_details,\n",
        "                    \"dataset_ref\": dataset_ref_for_kd,\n",
        "                    \"distillation_params\": distillation_params\n",
        "                },\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"kd_teacher_{student_model_name}\"\n",
        "            }\n",
        "            kd_response = await kd_tool.distill_knowledge(request=kd_request) # Call proxy method\n",
        "\n",
        "            if kd_response.get(\"status\") == \"success\":\n",
        "                teacher_output_uri = kd_response.get(\"payload\", {}).get(\"teacher_output_path\")\n",
        "                if not teacher_output_uri: raise RuntimeError(\"KD Tool API succeeded but did not return teacher output path.\")\n",
        "                self.logger.info(f\"B.O.S.S.: Teacher outputs generated by KD Tool API: {teacher_output_uri}\")\n",
        "            else:\n",
        "                raise RuntimeError(f\"KD Tool API failed: {kd_response.get('error_details')}\")\n",
        "        except Exception as kd_e:\n",
        "            logger.error(f\"B.O.S.S.: Error calling KD Tool API proxy: {kd_e}\", exc_info=True)\n",
        "            return {\"status\": \"failed\", \"error\": f\"KD Tool API Error: {kd_e}\"}\n",
        "\n",
        "        # Trigger MLOps Pipeline via Pub/Sub Client Proxy\n",
        "        pipeline_name = \"miz3_expert_training_pipeline_v1deploy_apireg\" # From Cell 17 (or config)\n",
        "        # Parameters for the Vertex AI Pipeline job\n",
        "        pipeline_params = {\n",
        "            \"project\": self.project,\n",
        "            \"location\": self.location,\n",
        "            \"model_display_name_prefix\": student_model_name,\n",
        "            \"source_uri_or_query\": temp_kd_input_uri, # Input data for student training\n",
        "            \"target_column\": \"teacher_prediction\", # Column containing teacher outputs\n",
        "            \"task_type\": task_type,\n",
        "            \"expert_domain\": domain,\n",
        "            \"epochs\": 10, # Example hyperparameter\n",
        "            \"teacher_output_uri\": teacher_output_uri # Pass teacher outputs if needed by pipeline\n",
        "            # Add other necessary pipeline parameters\n",
        "        }\n",
        "        # MIZ OKI formatted message for Pub/Sub\n",
        "        message_data = {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"event_type\": \"trigger_mlops_pipeline\",\n",
        "            \"payload\": {\"pipeline_name\": pipeline_name, \"parameters\": pipeline_params},\n",
        "            \"metadata\": {\"trace_id\": trace_id, \"source_component\": self.agent_id, \"student_model_name\": student_model_name}\n",
        "        }\n",
        "        message_bytes = json.dumps(message_data).encode('utf-8')\n",
        "        mlops_topic_full_path = f\"projects/{self.project}/topics/{self.mlops_trigger_topic_name}\"\n",
        "\n",
        "        try:\n",
        "             # Call Pub/Sub Client Proxy method\n",
        "             message_id = await self.pubsub_client.publish(mlops_topic_full_path, message_bytes)\n",
        "             self.logger.info(f\"B.O.S.S. (Trace: {trace_id}): Triggered MLOps training via Pub/Sub proxy. Topic: {mlops_topic_full_path}, Message ID: {message_id}\")\n",
        "             return {\"status\": \"training_triggered\", \"student_model_name\": student_model_name, \"pipeline_trigger_message_id\": message_id}\n",
        "        except Exception as pub_e:\n",
        "            logger.error(f\"B.O.S.S. (Trace: {trace_id}): Failed to publish MLOps trigger via Pub/Sub proxy: {pub_e}\", exc_info=True)\n",
        "            return {\"status\": \"failed\", \"error\": f\"PubSub Proxy Error: {pub_e}\"}\n",
        "\n",
        "    # --- Autonomous Experimentation (Reworked Async) ---\n",
        "    async def run_experimentation_cycle(self, trace_id: str) -> Dict:\n",
        "        \"\"\" Designs, launches, and potentially triggers analysis of experiments using Vertex AI Workflows client proxy. \"\"\"\n",
        "        self.logger.info(f\"Starting experimentation cycle (Trace: {trace_id})...\")\n",
        "        # --- TODO: Implement logic to identify opportunities ---\n",
        "        # Example: Query KG Tool API for underperforming campaigns, user segments, or processes\n",
        "        # kg_request = {\"payload\": {\"query\": \"FIND_EXPERIMENT_OPPORTUNITIES_QUERY\"}, \"trace_id\": trace_id}\n",
        "        # kg_response = await self.tools.get(\"kg\").execute_query(request=kg_request)\n",
        "        # opportunities = kg_response.get(\"payload\", {}).get(\"results\", [])\n",
        "        opportunities = [{\"goal\": \"Improve conversion rate for segment X\", \"metric\": \"conversion_rate\"}] # Placeholder\n",
        "        # --- End TODO ---\n",
        "\n",
        "        if opportunities:\n",
        "             opportunity = opportunities[0] # Select first opportunity for now\n",
        "             goal_desc = opportunity.get(\"goal\", \"Improve performance\")\n",
        "             target_metric = opportunity.get(\"metric\", \"unknown_metric\")\n",
        "             # --- TODO: Define control/variations based on opportunity ---\n",
        "             control_query = \"MATCH (u:User {segment:'X'}) RETURN u.mizId\" # Example control group\n",
        "             variations = [{\"type\":\"hp_variant\", \"param\":0.5}, {\"type\":\"hp_variant\", \"param\":0.8}] # Example variations\n",
        "             # --- End TODO ---\n",
        "\n",
        "             exp_id = await self.design_experiment(goal_desc, target_metric, control_query, variations, trace_id)\n",
        "             if exp_id:\n",
        "                  launch_result = await self.launch_experiment(exp_id)\n",
        "                  if launch_result.get(\"status\") == \"running\":\n",
        "                       self.logger.info(f\"Experiment {exp_id} launched (Exec ID: {launch_result.get('execution_id')}). Analysis handled by separate workflow/schedule.\")\n",
        "                       return {\"status\": \"experiment_launched\", \"experiment_id\": exp_id, \"execution_id\": launch_result.get('execution_id')}\n",
        "                  else:\n",
        "                      return {\"status\": \"launch_failed\", \"experiment_id\": exp_id, \"error\": launch_result.get(\"error\")}\n",
        "             else:\n",
        "                 return {\"status\": \"design_failed\"}\n",
        "        else:\n",
        "            self.logger.info(f\"Experimentation cycle (Trace: {trace_id}): No suitable opportunities found.\")\n",
        "            return {\"status\": \"no_experiment_opportunity\"}\n",
        "\n",
        "    async def design_experiment(self, goal_desc: str, target_metric: str, control_query: str, variations: List[Dict], trace_id: str) -> Optional[str]:\n",
        "        \"\"\" Designs experiment structure (Placeholder logic). Needs state persistence. \"\"\"\n",
        "        self.logger.info(f\"Designing experiment (Trace: {trace_id}): {goal_desc}\")\n",
        "        exp_id = f\"exp_{uuid.uuid4().hex[:8]}\"\n",
        "        # --- TODO: Use FM Client API proxy for design refinement if needed ---\n",
        "        # fm_request = {\"payload\": {\"prompt\": f\"Refine experiment design for goal: {goal_desc}...\", ...}, \"trace_id\": trace_id}\n",
        "        # fm_response = await self.tools.get(\"fm\").generate_text(input_data=fm_request)\n",
        "        # refined_design = json.loads(fm_response.get(\"payload\", {}).get(\"generated_text\", \"{}\"))\n",
        "        # --- End TODO ---\n",
        "        design = {\n",
        "            \"id\": exp_id,\n",
        "            \"goal\": goal_desc,\n",
        "            \"metric\": target_metric,\n",
        "            \"status\": \"designed\",\n",
        "            \"control_query\": control_query, # Query to identify control group in KG\n",
        "            \"variants\": variations, # Description of variations to test\n",
        "            \"trace_id\": trace_id,\n",
        "            \"created_at\": datetime.now(datetime.timezone.utc).isoformat()\n",
        "        }\n",
        "        self.experiments[exp_id] = design\n",
        "        # --- TODO: Persist self.experiments state ---\n",
        "        await self.save_state() # Conceptual call\n",
        "        # --- End TODO ---\n",
        "        self.logger.info(f\"Experiment {exp_id} designed (Trace: {trace_id}).\")\n",
        "        return exp_id\n",
        "\n",
        "    async def launch_experiment(self, experiment_id: str) -> Dict:\n",
        "        \"\"\" Triggers the Vertex AI Workflow for experiment execution using the REAL client proxy. \"\"\"\n",
        "        # --- TODO: Load self.experiments state ---\n",
        "        if experiment_id not in self.experiments or self.experiments[experiment_id][\"status\"] != \"designed\":\n",
        "            return {\"status\": \"error\", \"error\": \"Experiment not found or not in designed state.\"}\n",
        "        if not self.workflow_client:\n",
        "            return {\"status\": \"error\", \"error\": \"Workflow client proxy unavailable.\"}\n",
        "\n",
        "        trace_id = self.experiments[experiment_id].get(\"trace_id\", f\"exp_launch_{experiment_id}\")\n",
        "        self.logger.info(f\"Triggering Vertex AI Workflow to launch experiment {experiment_id} (Trace: {trace_id})...\")\n",
        "        try:\n",
        "            workflow_id = self.config.vertex_ai.experiment_execution_workflow_id\n",
        "            if not workflow_id: raise ConfigurationError(\"Experiment execution workflow ID not configured.\")\n",
        "\n",
        "            # Prepare MIZ OKI structured input for the workflow\n",
        "            workflow_input_payload = {\"experiment_id\": experiment_id, \"experiment_design\": self.experiments[experiment_id]}\n",
        "            miz_oki_input = {\n",
        "                \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "                \"request_id\": f\"req_exec_{experiment_id}\",\n",
        "                \"trace_id\": trace_id,\n",
        "                \"source_component\": self.agent_id,\n",
        "                \"target_component\": workflow_id, # Target is the workflow itself\n",
        "                \"payload\": workflow_input_payload\n",
        "            }\n",
        "            # Use real Vertex client proxy method (assuming start_workflow wraps create_execution)\n",
        "            execution_name = await self.workflow_client.start_workflow(\n",
        "                project=self.project,\n",
        "                location=self.location,\n",
        "                workflow_id=workflow_id,\n",
        "                miz_oki_input=miz_oki_input # Pass the structured input\n",
        "            )\n",
        "            if execution_name:\n",
        "                self.experiments[experiment_id][\"status\"] = \"running\"\n",
        "                self.experiments[experiment_id][\"start_date\"] = datetime.now(datetime.timezone.utc).isoformat()\n",
        "                self.experiments[experiment_id][\"execution_id\"] = execution_name\n",
        "                # --- TODO: Persist self.experiments state ---\n",
        "                await self.save_state() # Conceptual call\n",
        "                # --- End TODO ---\n",
        "                self.logger.info(f\"Experiment execution workflow started for {experiment_id}. Vertex Exec Name: {execution_name}\")\n",
        "                return {\"status\": \"running\", \"execution_id\": execution_name}\n",
        "            else:\n",
        "                raise RuntimeError(\"Vertex AI client proxy failed to start experiment execution workflow.\")\n",
        "        except ConfigurationError as conf_e:\n",
        "             logger.error(f\"Configuration error launching experiment {experiment_id}: {conf_e}\")\n",
        "             return {\"status\": \"error\", \"error\": str(conf_e)}\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Error launching experiment {experiment_id} workflow via proxy: {e}\", exc_info=True)\n",
        "             return {\"status\": \"error\", \"error\": str(e)}\n",
        "\n",
        "    async def analyze_experiment_results(self, experiment_id: str): # Kept for conceptual completeness\n",
        "        \"\"\" Triggers the Vertex AI Workflow for experiment analysis via REAL client proxy. \"\"\"\n",
        "        # --- TODO: Load self.experiments state ---\n",
        "        if experiment_id not in self.experiments: return None\n",
        "        if not self.workflow_client: return None\n",
        "\n",
        "        trace_id = self.experiments[experiment_id].get(\"trace_id\", f\"exp_analyze_{experiment_id}\")\n",
        "        self.logger.info(f\"Triggering Vertex AI Workflow to analyze experiment {experiment_id} (Trace: {trace_id})...\")\n",
        "        try:\n",
        "            workflow_id = self.config.vertex_ai.experiment_analysis_workflow_id\n",
        "            if not workflow_id: raise ConfigurationError(\"Experiment analysis workflow ID not configured.\")\n",
        "\n",
        "            workflow_input_payload = {\"experiment_id\": experiment_id, \"execution_id\": self.experiments[experiment_id].get(\"execution_id\")}\n",
        "            miz_oki_input = {\n",
        "                \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "                \"request_id\": f\"req_analyze_{experiment_id}\", \"trace_id\": trace_id,\n",
        "                \"source_component\": self.agent_id, \"target_component\": workflow_id,\n",
        "                \"payload\": workflow_input_payload\n",
        "            }\n",
        "            analysis_execution_name = await self.workflow_client.start_workflow(\n",
        "                project=self.project, location=self.location, workflow_id=workflow_id, miz_oki_input=miz_oki_input\n",
        "            )\n",
        "            if analysis_execution_name:\n",
        "                 self.experiments[experiment_id][\"status\"] = \"analysis_pending\"\n",
        "                 self.experiments[experiment_id][\"analysis_execution_id\"] = analysis_execution_name\n",
        "                 # --- TODO: Persist self.experiments state ---\n",
        "                 await self.save_state() # Conceptual call\n",
        "                 # --- End TODO ---\n",
        "                 self.logger.info(f\"Experiment analysis workflow started for {experiment_id}. Analysis Exec Name: {analysis_execution_name}\")\n",
        "                 return analysis_execution_name\n",
        "            else:\n",
        "                raise RuntimeError(\"Vertex AI client proxy failed to start experiment analysis workflow.\")\n",
        "        except ConfigurationError as conf_e:\n",
        "             logger.error(f\"Configuration error launching analysis for {experiment_id}: {conf_e}\")\n",
        "             return None\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Error launching experiment analysis workflow for {experiment_id} via proxy: {e}\", exc_info=True)\n",
        "             return None\n",
        "\n",
        "# --- Initialization (Conceptual for ADK Agent/Tool) ---\n",
        "# This would happen in the service's main entry point or factory\n",
        "\n",
        "# moe_registry_manager: Optional[MixtureOfExpertsRegistryManager] = None\n",
        "# aka_agent_logic: Optional[AutonomousKnowledgeAgentLogic] = None\n",
        "\n",
        "# async def initialize_foundational_layer():\n",
        "#     global moe_registry_manager, aka_agent_logic\n",
        "#     if not _config_obj:\n",
        "#         logger.critical(\"Cannot initialize Foundational Layer: CONFIG_OBJ not loaded.\")\n",
        "#         return\n",
        "#     if not _real_dependencies:\n",
        "#          logger.warning(\"Initializing Foundational Layer with MOCKED dependencies.\")\n",
        "\n",
        "#     try:\n",
        "#         moe_registry_manager = MixtureOfExpertsRegistryManager(_config_obj)\n",
        "#         await moe_registry_manager.initialize()\n",
        "\n",
        "#         aka_agent_logic = AutonomousKnowledgeAgentLogic(\n",
        "#             agent_id=\"AKA_001_Reworked\",\n",
        "#             config=_config_obj,\n",
        "#             kg_tool_proxy=_kg_tool_proxy, # Injected proxy\n",
        "#             fm_client_proxy=_fm_client_proxy, # Injected proxy\n",
        "#             kd_tool_proxy=_kd_tool_proxy, # Injected proxy\n",
        "#             moe_registry=moe_registry_manager,\n",
        "#             workflow_client_proxy=_workflow_client_proxy, # Injected client/proxy\n",
        "#             pubsub_client_proxy=_pubsub_client_proxy # Injected client/proxy\n",
        "#         )\n",
        "#         # In a real deployment, state would be loaded here or by the ADK framework\n",
        "#         await aka_agent_logic.setup(initial_state={})\n",
        "\n",
        "#         logger.info(\"Foundational Layer (AKA Logic) initialized.\")\n",
        "#     except Exception as e:\n",
        "#          logger.critical(f\"Foundational Layer initialization failed: {e}\", exc_info=True)\n",
        "#          moe_registry_manager = None; aka_agent_logic = None\n",
        "\n",
        "# async def cleanup_foundational_layer():\n",
        "#      if moe_registry_manager: await moe_registry_manager.cleanup()\n",
        "#      # Agent cleanup might be handled by ADK framework\n",
        "\n",
        "# --- Example Usage (Conceptual - How a workflow step might call this Tool) ---\n",
        "# async def workflow_step_call_aka(workflow_input: Dict):\n",
        "#      # Assuming workflow_input contains the MIZ OKI payload for the AKA tool\n",
        "#      if aka_agent_logic:\n",
        "#          aka_response = await aka_agent_logic.invoke(input_data=workflow_input)\n",
        "#          return aka_response # Return MIZ OKI response to workflow\n",
        "#      else:\n",
        "#          # Handle case where tool isn't initialized\n",
        "#          return {\"status\": \"error\", \"error_details\": [{\"message\": \"AKA Tool not available\"}]}\n",
        "\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 Foundational Layer Logic (Cell 4 - Reworked) ---\")\n",
        "print(\"AKA logic uses real dependencies/proxies and triggers real clients.\")\n",
        "print(\"Handles MIZ OKI payloads for API interaction.\")\n",
        "print(\"Requires implementation of KG queries, external fetch tool, MoE API client.\")\n",
        "print(\"State persistence needs explicit implementation or ADK framework support.\")\n",
        "print(\"-----------------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "-G7xRWwHUUn-",
        "outputId": "ced33096-cf35-4846-94ff-9d6faa3b254d"
      },
      "id": "-G7xRWwHUUn-",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-5-a6d709cfab81>, line 67)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a6d709cfab81>\"\u001b[0;36m, line \u001b[0;32m67\u001b[0m\n\u001b[0;31m    @dataclass class MockGcpConfig: project_id:Optional[str]=\"mock-proj\"; region:str=\"mock-region\"; gcs_bucket_name:Optional[str]=\"mock-bucket\"\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Core Processes Layer Implementation (Reworked)\n",
        "# Status: Tool logic uses real dependencies/proxies via MIZ OKI APIs/Events.\n",
        "#         Interactions via MIZ OKI payloads. Async implementation.\n",
        "#         Placeholders remain for Causal/Sim logic, config/rule loading, persistence.\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import uuid\n",
        "from typing import Dict, Any, Optional, List, Union, Callable, Tuple\n",
        "from collections import deque, defaultdict, Counter # Added Counter\n",
        "import asyncio\n",
        "\n",
        "# --- Assume Real Tool/Client Dependencies are Injected/Available ---\n",
        "# These proxies represent API clients for other deployed MIZ OKI services or GCP clients.\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    # Proxies for other MIZ OKI Tool APIs\n",
        "    if 'kg_tool_service_instance' not in globals(): raise NameError(\"kg_tool_service_instance proxy not found\") # Cell 3 Proxy\n",
        "    if 'moe_registry_manager' not in globals(): raise NameError(\"moe_registry_manager proxy not found\") # Cell 4 Instance/Proxy\n",
        "    if 'foundation_model_client' not in globals(): raise NameError(\"foundation_model_client proxy not found\") # Cell 18 Proxy\n",
        "    if 'expert_invoker' not in globals(): raise NameError(\"expert_invoker proxy not found\") # Needs definition/mock\n",
        "\n",
        "    # Real/Mock Clients for GCP Services\n",
        "    if '_workflow_executions_client' not in globals(): raise NameError(\"_workflow_executions_client not found\") # Cell 16 Client Proxy\n",
        "    if '_pubsub_client' not in globals(): raise NameError(\"_pubsub_client not found\") # Cell 8 Client Proxy\n",
        "\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _kg_tool_proxy = kg_tool_service_instance\n",
        "    _moe_registry_proxy = moe_registry_manager\n",
        "    _expert_invoker_proxy = expert_invoker\n",
        "    _fm_client_proxy = foundation_model_client\n",
        "    _workflow_client_proxy = _workflow_executions_client # Use client proxy from Cell 16\n",
        "    _pubsub_client_proxy = _pubsub_client # Use client proxy from Cell 8\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real/conceptual dependencies in Cell 5 (Reworked).\")\n",
        "\n",
        "except NameError as e:\n",
        "    logger.warning(f\"Dependency Error in Cell 5 ({e}). Using Mocks/Placeholders.\")\n",
        "    _real_dependencies = False\n",
        "    # --- Mock/Placeholder Setup ---\n",
        "    class MockKGTool: async def get_entity(self, request): return {\"status\": \"success\", \"payload\": {\"entity_data\": {\"entity_type\": \"mock\"}}}; async def execute_query(self, request): return {\"status\": \"success\", \"payload\": {\"results\": []}}; async def save_decision_record(self, request): return {\"status\": \"success\"}\n",
        "    class MockMoERegistryManager: async def find_expert_for_task(self, *args, **kwargs): return \"mock_expert_id\"; async def get_expert_details(self, *args, **kwargs): return {\"endpoint\": \"http://mock\"}\n",
        "    class MockExpertInvoker: async def invoke(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"prediction\": [0.5]}}\n",
        "    class MockFMClientTool: async def generate_text(self, input_data): return {\"status\": \"success\", \"payload\": {\"generated_text\": json.dumps({\"action_type\": \"mock_action\"})}}\n",
        "    class MockVertexWorkflowClient: async def start_workflow(self, project, location, workflow_id, miz_oki_input): return f\"projects/{project}/locations/{location}/workflows/{workflow_id}/executions/exec_{uuid.uuid4().hex[:8]}\" # Return full name\n",
        "    class MockPubSubClient: async def publish(self, topic, data_bytes): return f\"msg_{uuid.uuid4().hex[:8]}\"\n",
        "    # Define minimal config if needed\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ:\n",
        "        from dataclasses import dataclass, field\n",
        "        @dataclass class MockGcpConfig: project_id:Optional[str]=\"mock-proj\"; region:str=\"mock-region\"\n",
        "        @dataclass class MockSysThresholds: decision_confidence_threshold: float = 0.8; optimization_threshold: float = 0.7; goal_generation_threshold: float = 0.6\n",
        "        @dataclass class MockFmDefaults: llama4_maverick: str = \"mock-llama\"; llama4_scout: str = \"mock-scout\"\n",
        "        @dataclass class MockFmConfig: defaults: MockFmDefaults = field(default_factory=MockFmDefaults)\n",
        "        @dataclass class MockVertexAIConfig: planning_workflow_id: str = \"mock-planning-wf\"\n",
        "        @dataclass class MockBusinessImpact: kpis: Dict = field(default_factory=dict)\n",
        "        @dataclass class MockConfig: gcp: MockGcpConfig = field(default_factory=MockGcpConfig); system_thresholds: MockSysThresholds = field(default_factory=MockSysThresholds); mlops_trigger_topic:str=\"mock-topic\"; foundation_models: MockFmConfig = field(default_factory=MockFmConfig); vertex_ai: MockVertexAIConfig = field(default_factory=MockVertexAIConfig); business_impact: MockBusinessImpact = field(default_factory=MockBusinessImpact); miz_oki_schema_version: str = \"3.0\"; def get(self, key, default=None): parts=key.split('.'); val=self; try: [val := getattr(val, p) for p in parts]; return val; except: return default\n",
        "        _config_obj = MockConfig()\n",
        "\n",
        "    _kg_tool_proxy = MockKGTool(); _moe_registry_proxy = MockMoERegistryManager(); _expert_invoker_proxy = MockExpertInvoker(); _fm_client_proxy = MockFMClientTool(); _workflow_client_proxy = MockVertexWorkflowClient(); _pubsub_client_proxy = MockPubSubClient()\n",
        "    # Define dependent tools used here, using mocks if necessary\n",
        "    class MockOptimizerTool: async def get_current_objective_priorities(self, input_data): return {\"status\": \"success\", \"payload\": {'priorities': {'ROAS': 0.6}}}; objectives = {'ROAS': {'metrics': [{'name': 'roas'}]}}; metric_history = defaultdict(lambda: deque(maxlen=10)); baselines={'roas': 3}; targets={'roas': 8}; forecasting_models={'roas': 'mock_forecaster'}; def _evaluate_objectives(self, state): return {'ROAS': 0.5} # Simplified mock\n",
        "    _optimizer_tool_proxy = MockOptimizerTool()\n",
        "    class MockHdeTool: async def make_decision(self, input_data): return {\"status\": \"success\", \"payload\": {\"decision_id\": \"mock_dec\", \"action_recommended\": True, \"final_decision\": {\"action_type\": \"test_action\"}}}; async def get_history(self, request): return {\"status\": \"success\", \"payload\": {\"history\": []}}; async def update_decision_log(self, request): return {\"status\": \"success\"}\n",
        "    _hde_tool_proxy = MockHdeTool()\n",
        "    class MockLITool: async def integrate_learning(self, input_data): return {\"status\": \"success\", \"payload\": {\"integration_id\": \"mock_li\"}}\n",
        "    _li_tool_proxy = MockLITool()\n",
        "    # --- End Mock/Placeholder Setup ---\n",
        "\n",
        "logger = logging.getLogger('MIZ-OKI.CoreProcesses')\n",
        "\n",
        "# --- Causal/Simulation Tool Placeholders (Refined Async - Deployed as Services) ---\n",
        "class CausalReasoningTool:\n",
        "    \"\"\"Placeholder for the Causal Reasoning Tool Service.\"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any):\n",
        "        self.config = config; self.kg_tool = kg_tool_proxy\n",
        "        self.logger = logging.getLogger('MIZ-OKI.CausalTool')\n",
        "        self.logger.info(\"Causal Reasoning Tool logic initialized (Placeholder).\")\n",
        "\n",
        "    async def estimate_effect(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Estimates causal effect. Expects/Returns MIZ OKI.\"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        payload = input_data.get(\"payload\", {}); effect_query = payload.get(\"effect_query\", {}); context = payload.get(\"context\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "        status = \"pending\"; response_payload = None\n",
        "\n",
        "        target = effect_query.get('target_variable'); treatment = effect_query.get('treatment_variable')\n",
        "        if not target or not treatment: errors.append({\"code\": \"MISSING_PARAMS\", \"message\": \"target_variable and treatment_variable required in effect_query.\"})\n",
        "        if errors: status = \"bad_request\"\n",
        "        else:\n",
        "            self.logger.info(f\"[CAUSAL TOOL] Simulating async causal effect estimation via Tool API: {treatment} -> {target}\")\n",
        "            # --- TODO: Implement real causal inference ---\n",
        "            # 1. Fetch relevant data via KG Tool API proxy based on query and context.\n",
        "            # 2. Apply causal discovery/estimation algorithm (e.g., DoWhy, EconML).\n",
        "            # 3. Handle confounding variables.\n",
        "            await asyncio.sleep(random.uniform(0.1, 0.3)) # Simulate computation\n",
        "            # --- End TODO ---\n",
        "            status = \"success\"\n",
        "            response_payload = {\"effect_size\": random.uniform(-0.1, 0.2), \"confidence\": random.uniform(0.6, 0.95), \"query\": effect_query, \"_info\": \"Simulated Async Causal Result\"}\n",
        "\n",
        "        response = { # Manual MIZ OKI response construction\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version, \"request_id\": request_id, \"trace_id\": trace_id,\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(), \"source_component\": \"CausalReasoningTool\",\n",
        "            \"target_component\": input_data.get(\"source_component\"), \"status\": status, \"payload\": response_payload,\n",
        "            \"error_details\": errors if errors else None, \"metadata\": {\"processing_duration_ms\": (time.monotonic() - start_time) * 1000}\n",
        "        }\n",
        "        return response\n",
        "\n",
        "class SimulationTool:\n",
        "    \"\"\"Placeholder for the Business Simulation Tool Service.\"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any, fm_client_proxy: Optional[Any] = None):\n",
        "        self.config = config; self.kg_tool = kg_tool_proxy; self.fm_client = fm_client_proxy\n",
        "        self.logger = logging.getLogger('MIZ-OKI.SimulationTool')\n",
        "        self.logger.info(\"Simulation Tool logic initialized (Placeholder).\")\n",
        "\n",
        "    async def run_scenario(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Runs a simulation scenario. Expects/Returns MIZ OKI.\"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        payload = input_data.get(\"payload\", {}); scenario_config = payload.get(\"scenario_config\", {}); context = payload.get(\"context\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "        status = \"pending\"; response_payload = None\n",
        "\n",
        "        scenario_name = scenario_config.get('name', f\"sim_{uuid.uuid4().hex[:6]}\")\n",
        "        if not scenario_config: errors.append({\"code\": \"MISSING_CONFIG\", \"message\": \"scenario_config is required.\"})\n",
        "        if errors: status = \"bad_request\"\n",
        "        else:\n",
        "            self.logger.info(f\"[SIM TOOL] Running async simulation scenario via Tool API: {scenario_name}\")\n",
        "            # --- TODO: Implement real simulation logic ---\n",
        "            # 1. Fetch current state/parameters via KG Tool API proxy based on context.\n",
        "            # 2. Apply changes defined in scenario_config.\n",
        "            # 3. Use MoE/Expert Invoker or FM Client API proxies to predict outcomes.\n",
        "            # 4. Aggregate predicted metrics.\n",
        "            await asyncio.sleep(random.uniform(0.2, 0.5)) # Simulate computation\n",
        "            base_roas = context.get(\"current_metrics\", {}).get(\"roas\", 4.0) # Example context usage\n",
        "            sim_roas = base_roas * scenario_config.get(\"sim_cost_multiplier\", random.uniform(0.9, 1.2)) # Example effect\n",
        "            # --- End TODO ---\n",
        "            status = \"success\"\n",
        "            response_payload = {\"predicted_roas\": sim_roas, \"scenario_name\": scenario_name, \"_info\": f\"Simulated async outcome\"}\n",
        "\n",
        "        response = { # Manual MIZ OKI response construction\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version, \"request_id\": request_id, \"trace_id\": trace_id,\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(), \"source_component\": \"SimulationTool\",\n",
        "            \"target_component\": input_data.get(\"source_component\"), \"status\": status, \"payload\": response_payload,\n",
        "            \"error_details\": errors if errors else None, \"metadata\": {\"processing_duration_ms\": (time.monotonic() - start_time) * 1000}\n",
        "        }\n",
        "        return response\n",
        "\n",
        "# --- Ethical Guardrails Tool (Remains Relatively Stable - Internal Use, Sync OK) ---\n",
        "class EthicalGuardrailsTool:\n",
        "    \"\"\"Applies ethical checks to decisions. Internal component, sync logic acceptable.\"\"\"\n",
        "    def __init__(self, config: EnhancedConfig):\n",
        "        self.config = config\n",
        "        self.checks: Dict[str, List[Callable]] = defaultdict(list) # Store check functions\n",
        "        self.logger = logging.getLogger('MIZ-OKI.EthicalGuardrailsTool')\n",
        "        self._load_checks() # Load checks synchronously during init\n",
        "        self.logger.info(\"Ethical Guardrails Tool logic initialized.\")\n",
        "\n",
        "    def _load_checks(self):\n",
        "        \"\"\"Loads ethical check functions (e.g., based on config or dynamically).\"\"\"\n",
        "        # --- TODO: Load or define specific check functions ---\n",
        "        # Example check function (sync)\n",
        "        def check_fairness_basic(decision_type: str, context: Dict, decision: Dict) -> Tuple[bool, str]:\n",
        "            # Placeholder: Check if decision targets sensitive demographics unfairly\n",
        "            if decision.get(\"target_segment\") in [\"segment_A_sensitive\"]:\n",
        "                return False, \"Decision targets sensitive segment potentially unfairly.\"\n",
        "            return True, \"Fairness check passed (basic).\"\n",
        "\n",
        "        self.checks[\"all\"].append(check_fairness_basic) # Apply to all decisions\n",
        "        self.checks[\"ad_targeting\"].append(check_fairness_basic) # Apply specifically\n",
        "        # --- End TODO ---\n",
        "        self.logger.info(f\"Loaded {sum(len(v) for v in self.checks.values())} ethical checks.\")\n",
        "\n",
        "    def review_decision(self, decision_type: str, context: Dict, decision: Dict) -> Dict:\n",
        "        \"\"\"Reviews a decision against applicable ethical checks synchronously.\"\"\"\n",
        "        results = {\"approved\": True, \"checks_passed\": [], \"checks_failed\": [], \"reason\": \"All checks passed.\"}\n",
        "        checks_to_run = self.checks.get(\"all\", []) + self.checks.get(decision_type, [])\n",
        "        if not checks_to_run: return results # No checks defined\n",
        "\n",
        "        self.logger.debug(f\"Running {len(checks_to_run)} ethical checks for decision type '{decision_type}'.\")\n",
        "        for check_func in checks_to_run:\n",
        "            try:\n",
        "                passed, reason = check_func(decision_type, context, decision)\n",
        "                if passed:\n",
        "                    results[\"checks_passed\"].append(reason)\n",
        "                else:\n",
        "                    results[\"approved\"] = False\n",
        "                    results[\"checks_failed\"].append(reason)\n",
        "                    results[\"reason\"] = f\"Failed check: {reason}\"\n",
        "                    self.logger.warning(f\"Ethical check failed for decision type '{decision_type}': {reason}\")\n",
        "                    break # Stop on first failure\n",
        "            except Exception as e:\n",
        "                results[\"approved\"] = False\n",
        "                fail_reason = f\"Exception during check '{check_func.__name__}': {e}\"\n",
        "                results[\"checks_failed\"].append(fail_reason)\n",
        "                results[\"reason\"] = fail_reason\n",
        "                self.logger.error(f\"Exception during ethical check: {e}\", exc_info=True)\n",
        "                break\n",
        "        return results\n",
        "\n",
        "# --- Hybrid Decision Engine Tool (Reworked Async) ---\n",
        "class HybridDecisionEngineTool:\n",
        "    \"\"\" Makes decisions asynchronously using multiple reasoning modules. Deployed as a service. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any, moe_registry_proxy: Any, expert_invoker_proxy: Any,\n",
        "                 ethical_guardrails_tool: EthicalGuardrailsTool, fm_client_proxy: Optional[Any] = None):\n",
        "        if not all([config, kg_tool_proxy, moe_registry_proxy, expert_invoker_proxy, ethical_guardrails_tool]):\n",
        "            raise InitializationError(\"HDE Tool requires config, ethical guardrails, and proxies for KG, MoE Registry, Expert Invoker.\")\n",
        "        self.config = config\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.moe_registry = moe_registry_proxy\n",
        "        self.expert_invoker = expert_invoker_proxy\n",
        "        self.ethical_guardrails = ethical_guardrails_tool\n",
        "        self.fm_client = fm_client_proxy # Optional FM client proxy\n",
        "        self.decision_blueprints: Dict[str, Dict] = {}\n",
        "        # TODO: Replace deque with persistent storage (e.g., Firestore, BQ via KG Tool API) for production\n",
        "        self.decision_history = deque(maxlen=5000)\n",
        "        # Instantiate internal tools\n",
        "        self.causal_tool = CausalReasoningTool(config, kg_tool_proxy) # Internal instance or could be separate proxy\n",
        "        self.simulation_tool = SimulationTool(config, kg_tool_proxy, fm_client_proxy) # Internal instance or proxy\n",
        "        self.logger = logging.getLogger('MIZ-OKI.HybridDecisionEngineTool')\n",
        "        self._load_blueprints() # Load sync ok\n",
        "        self.logger.info(\"Hybrid Decision Engine Tool logic initialized (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"HybridDecisionEngineTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    def _load_blueprints(self):\n",
        "        \"\"\" Loads decision blueprints (e.g., from GCS, DB, or embedded). \"\"\"\n",
        "        # --- TODO: Load blueprints from a persistent/configurable source ---\n",
        "        # Example blueprints:\n",
        "        self.decision_blueprints = {\n",
        "            \"campaign_budget_adjustment\": {\n",
        "                \"description\": \"Adjusts campaign budget based on ROAS prediction and equity score.\",\n",
        "                \"reasoning_modules\": [\n",
        "                    {\"id\": \"roas_pred\", \"type\": \"model\", \"task_type\": \"forecasting\", \"domain\": \"roas\", \"inputs\": [\"campaign_features\"], \"output_variable\": \"predicted_roas\"},\n",
        "                    {\"id\": \"equity_score\", \"type\": \"kg_query\", \"query_template\": \"GET_EQUITY_QUERY for campaign {campaign_id}\", \"output_variable\": \"brand_equity\"},\n",
        "                    {\"id\": \"adjustment_logic\", \"type\": \"rule\", \"rule_func\": self._calculate_budget_adjustment_rule, \"inputs\": [\"predicted_roas\", \"brand_equity\", \"current_budget\"], \"output_variable\": \"budget_adjustment\"}\n",
        "                ],\n",
        "                \"aggregation_logic\": \"prioritized\", # Use output from 'adjustment_logic'\n",
        "                \"output_action_variable\": \"budget_adjustment\"\n",
        "            },\n",
        "            \"system_optimization\": {\n",
        "                 \"description\": \"Suggests system optimization actions based on current state.\",\n",
        "                 \"reasoning_modules\": [\n",
        "                     {\"id\": \"llm_suggest_action\", \"type\": \"llama4_reasoning\", \"model_alias\": \"llama4_scout\", \"output_variable\": \"llm_suggestion\", \"prompt_template\": \"Context: {current_metrics}, Failing Objectives: {failing_objectives}. Suggest one optimization action as JSON: {{'action_type': '...', 'params': {{...}}}}\"}\n",
        "                 ],\n",
        "                 \"aggregation_logic\": \"first_valid\", # Take the first valid suggestion\n",
        "                 \"output_action_variable\": \"llm_suggestion\"\n",
        "            }\n",
        "            # Add more blueprints...\n",
        "        }\n",
        "        self.logger.info(f\"Loaded {len(self.decision_blueprints)} decision blueprints (Example).\")\n",
        "\n",
        "    def _calculate_budget_adjustment_rule(self, context: Dict) -> Dict:\n",
        "        \"\"\"Example synchronous rule function.\"\"\"\n",
        "        pred_roas = context.get(\"predicted_roas\", {}).get(\"prediction\", [3.0])[0]\n",
        "        equity = context.get(\"brand_equity\", {}).get(\"results\", [{\"value\": 0.7}])[0].get(\"value\", 0.7)\n",
        "        current_budget = context.get(\"current_budget\", 100.0)\n",
        "        roas_target = self.config.business_impact.roas_target\n",
        "\n",
        "        # Simple logic: increase budget if ROAS > target, decrease if significantly below\n",
        "        adjustment_factor = 1.0\n",
        "        if pred_roas > roas_target * 1.1: adjustment_factor = 1.1 # Increase 10%\n",
        "        elif pred_roas < roas_target * 0.8: adjustment_factor = 0.9 # Decrease 10%\n",
        "\n",
        "        # Modify based on equity (simple example)\n",
        "        equity_factor = 1.0 + (equity - 0.7) * 0.2 # +/- up to 20% based on equity deviation from 0.7\n",
        "\n",
        "        new_budget = max(10.0, current_budget * adjustment_factor * equity_factor) # Min budget 10\n",
        "\n",
        "        return {\"action_type\": \"set_budget\", \"params\": {\"new_budget\": round(new_budget, 2)}, \"_confidence\": 0.85} # Add confidence\n",
        "\n",
        "    async def _execute_reasoning_module(self, module_config: Dict, context: Dict, all_module_outputs: Dict, trace_id: Optional[str]) -> Dict:\n",
        "        \"\"\" Executes a single reasoning module async, calling deployed Tool APIs proxies. \"\"\"\n",
        "        module_type = module_config.get(\"type\")\n",
        "        module_id = module_config.get(\"id\", f\"{module_type}_{uuid.uuid4().hex[:4]}\")\n",
        "        output = {\"id\": module_id, \"type\": module_type, \"status\": \"pending\"}\n",
        "        start_time = time.monotonic()\n",
        "        logger.debug(f\"HDE: Executing module: {module_id} ({module_type})\")\n",
        "\n",
        "        # --- Dependency Resolution Helper ---\n",
        "        def resolve_value(val_str: Any) -> Any:\n",
        "            if isinstance(val_str, str) and val_str.startswith('{') and val_str.endswith('}'):\n",
        "                key_path = val_str[1:-1]\n",
        "                parts = key_path.split('.')\n",
        "                source_id = parts[0]\n",
        "                if source_id in all_module_outputs:\n",
        "                    data = all_module_outputs[source_id]\n",
        "                    try:\n",
        "                        for part in parts[1:]:\n",
        "                            if isinstance(data, dict): data = data.get(part)\n",
        "                            elif isinstance(data, list) and part.isdigit(): data = data[int(part)]\n",
        "                            else: data = None; break\n",
        "                        return data # Return resolved value (could be None)\n",
        "                    except (KeyError, IndexError, TypeError, AttributeError):\n",
        "                        return None # Indicate resolution failure\n",
        "                else: return None # Source module output not found\n",
        "            return val_str # Return original value if not a placeholder\n",
        "\n",
        "        # Resolve inputs for the current module\n",
        "        module_inputs = {k: resolve_value(v) for k, v in module_config.get(\"inputs\", {}).items()}\n",
        "        module_context = {**context, **module_inputs} # Combine global context and resolved inputs\n",
        "\n",
        "        try:\n",
        "            output[\"status\"] = \"running\"\n",
        "            if module_type == \"model\":\n",
        "                task_type = module_config.get(\"task_type\", \"prediction\")\n",
        "                domain = module_config.get(\"domain\")\n",
        "                expert_id = await self.moe_registry.find_expert_for_task(task_type=task_type, domain=domain, context=module_context) # MoE API Call via proxy\n",
        "                if not expert_id: raise ValueError(f\"No suitable expert found via MoE for task '{task_type}/{domain}'.\")\n",
        "                expert_details = await self.moe_registry.get_expert_details(expert_id) # MoE API Call via proxy\n",
        "                expert_endpoint = expert_details.get(\"endpoint\") if expert_details else None\n",
        "                if not expert_endpoint or not self.expert_invoker: raise ValueError(f\"Expert endpoint for '{expert_id}' not found or invoker proxy missing.\")\n",
        "\n",
        "                # Prepare input data based on module config (using resolved values)\n",
        "                model_input_data = {k: module_context.get(k) for k in module_config.get(\"model_input_keys\", [])}\n",
        "                invoker_request = {\"payload\": {\"endpoint\": expert_endpoint, \"data\": model_input_data}, \"trace_id\": trace_id}\n",
        "                invoker_response = await self.expert_invoker.invoke(request=invoker_request) # Expert Invoker API Call via proxy\n",
        "\n",
        "                if invoker_response.get(\"status\") == \"success\":\n",
        "                    output[\"result\"] = invoker_response.get(\"payload\", {}) # Store entire payload from expert\n",
        "                    output[\"expert_id\"] = expert_id\n",
        "                else: raise RuntimeError(f\"Expert Invoker API call failed: {invoker_response.get('error_details')}\")\n",
        "\n",
        "            elif module_type == \"rule\":\n",
        "                 rule_func = module_config.get(\"rule_func\")\n",
        "                 if callable(rule_func):\n",
        "                     # Run sync rule in thread pool\n",
        "                     output[\"result\"] = await asyncio.to_thread(rule_func, module_context)\n",
        "                 else: raise TypeError(\"Rule function not callable.\")\n",
        "\n",
        "            elif module_type == \"causal\":\n",
        "                 causal_query = {k: module_context.get(k) for k in module_config.get(\"query_keys\", [])}\n",
        "                 causal_request = {\"payload\": {\"effect_query\": causal_query, \"context\": module_context}, \"trace_id\": trace_id}\n",
        "                 causal_response = await self.causal_tool.estimate_effect(input_data=causal_request) # Call Causal Tool API proxy\n",
        "                 if causal_response.get(\"status\") == \"success\": output[\"result\"] = causal_response.get(\"payload\", {})\n",
        "                 else: raise RuntimeError(f\"Causal Tool API call failed: {causal_response.get('error_details')}\")\n",
        "\n",
        "            elif module_type == \"simulation\":\n",
        "                 sim_config = {k: module_context.get(k) for k in module_config.get(\"config_keys\", [])}\n",
        "                 sim_request = {\"payload\": {\"scenario_config\": sim_config, \"context\": module_context}, \"trace_id\": trace_id}\n",
        "                 sim_response = await self.simulation_tool.run_scenario(input_data=sim_request) # Call Sim Tool API proxy\n",
        "                 if sim_response.get(\"status\") == \"success\": output[\"result\"] = sim_response.get(\"payload\", {})\n",
        "                 else: raise RuntimeError(f\"Simulation Tool API call failed: {sim_response.get('error_details')}\")\n",
        "\n",
        "            elif module_type == \"llama4_reasoning\" or module_type == \"fm_generate\": # Unified type\n",
        "                if not self.fm_client: raise RuntimeError(\"FM Client Tool proxy unavailable.\")\n",
        "                prompt_template = module_config.get(\"prompt_template\")\n",
        "                model_alias = module_config.get(\"model_alias\", self.config.foundation_models.defaults.llama4_maverick)\n",
        "                output_var = module_config.get(\"output_variable\", \"llm_output\") # Variable name to store result in\n",
        "\n",
        "                # Format prompt safely using combined context and previous outputs\n",
        "                template_context = {**module_context, **all_module_outputs}\n",
        "                prompt = prompt_template.format_map(defaultdict(lambda: 'N/A', template_context))\n",
        "\n",
        "                fm_request = {\"payload\": {\"prompt\": prompt, \"model_alias\": model_alias, \"max_tokens\": module_config.get(\"max_tokens\", 512)}, \"trace_id\": trace_id}\n",
        "                fm_response = await self.fm_client.generate_text(input_data=fm_request) # FM Client API Call via proxy\n",
        "\n",
        "                if fm_response.get(\"status\") == \"success\":\n",
        "                    raw_output = fm_response.get(\"payload\", {}).get(\"generated_text\")\n",
        "                    # Attempt to parse if JSON output is expected\n",
        "                    try: output[\"result\"] = json.loads(raw_output); output[\"parsed_json\"] = True\n",
        "                    except (json.JSONDecodeError, TypeError): output[\"result\"] = {\"raw_output\": raw_output}; output[\"parsed_json\"] = False\n",
        "                    output[\"model_alias\"] = model_alias\n",
        "                else: raise RuntimeError(f\"FM Client API call failed: {fm_response.get('error_details')}\")\n",
        "\n",
        "            elif module_type == \"kg_query\": # Added KG Query module\n",
        "                 if not self.kg_tool: raise RuntimeError(\"KG Tool proxy unavailable.\")\n",
        "                 query_template = module_config.get(\"query_template\")\n",
        "                 output_var = module_config.get(\"output_variable\", \"kg_results\")\n",
        "                 query_params = {k: module_context.get(k) for k in module_config.get(\"param_keys\", [])}\n",
        "                 query = query_template.format_map(defaultdict(lambda: 'N/A', module_context)) # Format query string\n",
        "\n",
        "                 kg_request = {\"payload\": {\"query\": query, \"parameters\": query_params}, \"trace_id\": trace_id}\n",
        "                 kg_response = await self.kg_tool.execute_query(request=kg_request) # KG Tool API Call via proxy\n",
        "                 if kg_response.get(\"status\") == \"success\":\n",
        "                     output[\"result\"] = kg_response.get(\"payload\", {}).get(\"results\", [])\n",
        "                 else: raise RuntimeError(f\"KG Tool API call failed: {kg_response.get('error_details')}\")\n",
        "\n",
        "            else:\n",
        "                output[\"error\"] = f\"Unsupported module type: {module_type}\"; output[\"status\"] = \"failed\"\n",
        "\n",
        "            if \"error\" not in output: output[\"status\"] = \"success\"\n",
        "\n",
        "        except Exception as mod_e:\n",
        "            output[\"error\"] = str(mod_e); output[\"status\"] = \"failed\"\n",
        "            self.logger.error(f\"HDE Module {module_id} error async: {mod_e}\", exc_info=False) # Log error but don't crash HDE\n",
        "\n",
        "        output[\"duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return output\n",
        "\n",
        "    def _aggregate_outputs(self, module_outputs: Dict[str, Dict], logic: str) -> Tuple[Optional[Dict], float, Optional[str], Optional[str]]:\n",
        "        \"\"\" Aggregates outputs from multiple reasoning modules based on specified logic (sync). \"\"\"\n",
        "        if not module_outputs: return None, 0.0, None, None\n",
        "\n",
        "        successful_outputs = {mod_id: output for mod_id, output in module_outputs.items() if output.get(\"status\") == \"success\" and output.get(\"result\") is not None}\n",
        "        if not successful_outputs: return None, 0.0, None, None # No successful modules\n",
        "\n",
        "        if logic == \"prioritized\":\n",
        "            # Assumes blueprints define priority or uses order\n",
        "            # Or looks for a specific output variable name defined in the blueprint\n",
        "            output_var_name = self.decision_blueprints.get(self._current_decision_type, {}).get(\"output_action_variable\") # Need to track current type\n",
        "            for mod_id, output in successful_outputs.items():\n",
        "                 if output_var_name and output_var_name in output.get(\"result\", {}):\n",
        "                      decision = output[\"result\"][output_var_name]\n",
        "                      confidence = output.get(\"result\", {}).get(\"_confidence\", output.get(\"confidence\", 0.7)) # Get confidence if provided\n",
        "                      return decision, confidence, mod_id, output.get(\"type\")\n",
        "            # Fallback: return first successful result if specific variable not found\n",
        "            first_id = list(successful_outputs.keys())[0]\n",
        "            first_output = successful_outputs[first_id]\n",
        "            return first_output.get(\"result\"), first_output.get(\"confidence\", 0.7), first_id, first_output.get(\"type\")\n",
        "\n",
        "        elif logic == \"average\": # Example for numerical outputs\n",
        "            values = [output[\"result\"] for output in successful_outputs.values() if isinstance(output.get(\"result\"), (int, float))]\n",
        "            confidences = [output.get(\"confidence\", 0.7) for output in successful_outputs.values() if isinstance(output.get(\"result\"), (int, float))]\n",
        "            if not values: return None, 0.0, None, None\n",
        "            avg_value = np.average(values, weights=confidences if len(confidences) == len(values) else None)\n",
        "            avg_confidence = np.mean(confidences) if confidences else 0.7\n",
        "            return {\"aggregated_value\": avg_value}, avg_confidence, \"aggregation\", \"average\"\n",
        "\n",
        "        elif logic == \"majority_vote\": # Example for classification outputs\n",
        "            votes = [output[\"result\"].get(\"class\") for output in successful_outputs.values() if isinstance(output.get(\"result\"), dict) and \"class\" in output[\"result\"]]\n",
        "            if not votes: return None, 0.0, None, None\n",
        "            majority_class = Counter(votes).most_common(1)[0][0]\n",
        "            # Confidence could be proportion of votes\n",
        "            confidence = votes.count(majority_class) / len(votes)\n",
        "            return {\"class\": majority_class}, confidence, \"aggregation\", \"majority_vote\"\n",
        "\n",
        "        elif logic == \"first_valid\": # Return the first successful module's output\n",
        "            first_id = list(successful_outputs.keys())[0]\n",
        "            first_output = successful_outputs[first_id]\n",
        "            # Extract confidence if available in the result dict itself\n",
        "            confidence = first_output.get(\"result\", {}).get(\"_confidence\", first_output.get(\"confidence\", 0.7))\n",
        "            return first_output.get(\"result\"), confidence, first_id, first_output.get(\"type\")\n",
        "\n",
        "        else: # Default to first valid\n",
        "            self.logger.warning(f\"Unsupported aggregation logic '{logic}', defaulting to 'first_valid'.\")\n",
        "            first_id = list(successful_outputs.keys())[0]; first_output = successful_outputs[first_id]\n",
        "            confidence = first_output.get(\"result\", {}).get(\"_confidence\", first_output.get(\"confidence\", 0.7))\n",
        "            return first_output.get(\"result\"), confidence, first_id, first_output.get(\"type\")\n",
        "\n",
        "    async def make_decision(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Makes a decision asynchronously. Expects MIZ OKI input, returns MIZ OKI response. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); decision_type = payload.get(\"decision_type\"); context = payload.get(\"context\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # Prepare MIZ OKI response structure\n",
        "        response = self._create_miz_oki_response(input_data, \"pending\")\n",
        "        decision_id = f\"dec_{decision_type or 'unknown'}_{uuid.uuid4().hex[:8]}\"\n",
        "        log_entry = { # Detailed log for history and XAI\n",
        "            \"decision_id\": decision_id, \"decision_type\": decision_type, \"trace_id\": trace_id,\n",
        "            \"timestamp_start\": response[\"timestamp\"], \"context_preview\": str(context)[:250], # Limit preview size\n",
        "            \"status\": \"pending\", \"module_outputs\": {}, \"final_decision\": None,\n",
        "            \"final_confidence\": 0.0, \"chain_of_thought\": [], \"ethical_review\": None,\n",
        "            \"action_recommended\": False\n",
        "        }\n",
        "        cot = log_entry[\"chain_of_thought\"]; all_module_outputs = {}\n",
        "        self._current_decision_type = decision_type # Track for aggregation logic\n",
        "\n",
        "        try:\n",
        "            if not decision_type or decision_type not in self.decision_blueprints:\n",
        "                raise ValueError(f\"Decision blueprint '{decision_type}' not found or not specified.\")\n",
        "            blueprint = self.decision_blueprints[decision_type]\n",
        "            cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] Start HDE decision '{decision_type}'. Blueprint: {blueprint.get('description', 'N/A')}\")\n",
        "\n",
        "            # Execute Modules async\n",
        "            cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] Executing {len(blueprint.get('reasoning_modules', []))} reasoning modules async...\")\n",
        "            module_tasks = [\n",
        "                self._execute_reasoning_module(mod_cfg, context, all_module_outputs, trace_id)\n",
        "                for mod_cfg in blueprint.get(\"reasoning_modules\", [])\n",
        "            ]\n",
        "            module_results_list = await asyncio.gather(*module_tasks, return_exceptions=True)\n",
        "\n",
        "            # Process module results\n",
        "            errors_found_in_modules = False\n",
        "            for result in module_results_list:\n",
        "                 if isinstance(result, Exception):\n",
        "                      errors_found_in_modules = True; cot.append(f\"  - ERROR during module execution: {result}\")\n",
        "                      # Log error but don't necessarily stop the whole decision process\n",
        "                      logger.error(f\"HDE {decision_id}: Exception in reasoning module: {result}\", exc_info=True)\n",
        "                      continue\n",
        "                 if isinstance(result, dict) and (mod_id := result.get(\"id\")):\n",
        "                      all_module_outputs[mod_id] = result # Store full module output\n",
        "                      status_indicator = \"OK\" if result.get(\"status\") == \"success\" else f\"FAILED ({result.get('error')})\"\n",
        "                      cot.append(f\"  - Module {mod_id} ({result.get('type')}) finished. Status: {status_indicator}. Duration: {result.get('duration_ms'):.2f} ms\")\n",
        "                      if result.get(\"status\") != \"success\": errors_found_in_modules = True\n",
        "                 else:\n",
        "                      cot.append(f\"  - WARNING: Received invalid result type from a module: {type(result)}\")\n",
        "                      errors_found_in_modules = True\n",
        "            log_entry[\"module_outputs\"] = all_module_outputs\n",
        "            cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] Reasoning modules execution attempt complete. Errors found: {errors_found_in_modules}\")\n",
        "\n",
        "            # Aggregation (Sync)\n",
        "            agg_logic = blueprint.get(\"aggregation_logic\", \"prioritized\")\n",
        "            cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] Aggregating module outputs using logic: '{agg_logic}'...\")\n",
        "            agg_decision, agg_conf, src_mod_id, src_mod_type = self._aggregate_outputs(all_module_outputs, agg_logic)\n",
        "\n",
        "            if agg_decision is None:\n",
        "                raise ValueError(f\"Aggregation failed ('{agg_logic}'). No valid decision could be determined from module outputs.\")\n",
        "            cot.append(f\"  - Aggregated decision from module '{src_mod_id}' ({src_mod_type}). Confidence: {agg_conf:.4f}\")\n",
        "            log_entry[\"aggregated_decision\"] = agg_decision; log_entry[\"aggregated_confidence\"] = agg_conf\n",
        "\n",
        "            # Ethical Guardrails (Sync - as it's internal logic)\n",
        "            cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] Performing ethical review...\")\n",
        "            ethical_review = self.ethical_guardrails.review_decision(decision_type, context, agg_decision)\n",
        "            log_entry[\"ethical_review\"] = ethical_review\n",
        "            cot.append(f\"  - Ethical review complete. Approved: {ethical_review.get('approved')}. Reason: {ethical_review.get('reason')}\")\n",
        "\n",
        "            # Final Decision & Actionability (Sync)\n",
        "            final_decision = agg_decision; final_confidence = agg_conf\n",
        "            if not ethical_review.get(\"approved\", False):\n",
        "                # Modify decision or flag it if ethics check fails\n",
        "                final_decision[\"ethics_flag\"] = {\"failed_checks\": ethical_review.get(\"checks_failed\"), \"reason\": ethical_review.get(\"reason\")}\n",
        "                final_confidence *= 0.8 # Reduce confidence if flagged\n",
        "                log_entry[\"status\"] = \"ethics_review_required\"\n",
        "                cot.append(f\"  - Decision flagged due to ethical concerns.\")\n",
        "            else:\n",
        "                log_entry[\"status\"] = \"approved_by_engine\"\n",
        "                cot.append(f\"  - Ethical review passed.\")\n",
        "\n",
        "            log_entry[\"final_decision\"] = final_decision; log_entry[\"final_confidence\"] = final_confidence\n",
        "            cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] Final decision compiled.\")\n",
        "\n",
        "            # Determine if action should be recommended based on confidence threshold\n",
        "            min_confidence = self.config.system_thresholds.decision_confidence_threshold\n",
        "            # Check if final_decision represents a concrete action (not 'no_action' or empty)\n",
        "            is_actionable = isinstance(final_decision, dict) and final_decision.get('action_type') not in ['no_action', None, '']\n",
        "            should_recommend = is_actionable and final_confidence >= min_confidence and log_entry[\"status\"] != \"ethics_review_required\"\n",
        "            log_entry[\"action_recommended\"] = should_recommend\n",
        "            cot.append(f\"  - Action Recommended: {should_recommend} (Confidence: {final_confidence:.4f} vs Threshold: {min_confidence:.4f})\")\n",
        "\n",
        "            response[\"status\"] = \"success\" # HDE process completed successfully (even if action not recommended or ethics flagged)\n",
        "            response[\"payload\"] = log_entry # Return the detailed log entry\n",
        "\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Async decision making FAILED for '{decision_type}' (ID: {decision_id}): {e}\", exc_info=True)\n",
        "             response[\"status\"] = \"internal_error\"; errors.append({\"code\": \"HDE_ERROR\", \"message\": str(e)})\n",
        "             log_entry[\"status\"] = \"failed\"; log_entry[\"error\"] = str(e); cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] PROCESS FAILED: {e}\")\n",
        "             response[\"payload\"] = log_entry # Return log even on failure\n",
        "\n",
        "        log_entry[\"total_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        self.decision_history.append(log_entry) # Add to in-memory history\n",
        "\n",
        "        # Persist decision log async via KG Tool API proxy (fire-and-forget)\n",
        "        if self.kg_tool and hasattr(self.kg_tool, 'save_decision_record'):\n",
        "             # Prepare MIZ OKI request for KG Tool\n",
        "             kg_request = {\"payload\": {\"record\": log_entry}, \"trace_id\": trace_id, \"request_id\": f\"kg_save_dec_{decision_id}\"}\n",
        "             asyncio.create_task(self.kg_tool.save_decision_record(request=kg_request), name=f\"save_xai_{decision_id}\")\n",
        "        else:\n",
        "             logger.warning(f\"KG Tool proxy unavailable. Cannot persist decision log {decision_id} to KG.\")\n",
        "\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = log_entry[\"total_duration_ms\"]\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        return response\n",
        "\n",
        "    # --- Add get_history and update_decision_log methods (conceptual, need persistence) ---\n",
        "    async def get_history(self, request: Dict) -> Dict: # Expects MIZ OKI\n",
        "        \"\"\" Retrieves recent decision history. Needs persistent store integration. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        payload = request.get(\"payload\", {}); limit = payload.get(\"limit\", 100)\n",
        "        status = \"pending\"; response_payload = None\n",
        "        try:\n",
        "            # --- TODO: Implement retrieval from persistent store (KG/DB) ---\n",
        "            # Example using in-memory deque:\n",
        "            history_list = list(self.decision_history)[-limit:]\n",
        "            # --- End TODO ---\n",
        "            status = \"success\"; response_payload = {\"history\": history_list}\n",
        "        except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"HISTORY_ERROR\", \"message\": str(e)})\n",
        "            logger.error(f\"Error retrieving HDE history: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    async def update_decision_log(self, request: Dict) -> Dict: # Expects MIZ OKI\n",
        "        \"\"\" Updates a decision log, e.g., with human review status. Needs persistent store integration. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        payload = request.get(\"payload\", {}); decision_id = payload.get(\"decision_id\"); update_data = payload.get(\"update_data\")\n",
        "        status = \"pending\"; response_payload = None\n",
        "\n",
        "        if not decision_id or not update_data: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'decision_id' and 'update_data' required.\"})\n",
        "        if errors: status = \"bad_request\"\n",
        "        else:\n",
        "            try:\n",
        "                # --- TODO: Implement update logic for persistent store (KG/DB) ---\n",
        "                # Example: Find log in DB/KG and update fields.\n",
        "                # kg_update_request = {\"payload\": {\"decision_id\": decision_id, \"updates\": update_data}, ...}\n",
        "                # kg_response = await self.kg_tool.update_decision_log_endpoint(request=kg_update_request) # Needs KG endpoint\n",
        "                # success = kg_response.get(\"status\") == \"success\"\n",
        "\n",
        "                # Placeholder using in-memory deque:\n",
        "                log_entry = next((item for item in self.decision_history if item.get('decision_id') == decision_id), None)\n",
        "                success = False\n",
        "                if log_entry:\n",
        "                    log_entry.update(update_data)\n",
        "                    log_entry[\"last_updated_by_api\"] = datetime.now(datetime.timezone.utc).isoformat()\n",
        "                    success = True\n",
        "                    logger.info(f\"Updated decision log {decision_id} in memory.\")\n",
        "                else:\n",
        "                    logger.warning(f\"Decision log {decision_id} not found in memory for update.\")\n",
        "                # --- End TODO ---\n",
        "\n",
        "                if success: status = \"success\"; response_payload = {\"updated\": True, \"decision_id\": decision_id}\n",
        "                else: status = \"not_found\"; errors.append({\"code\": \"LOG_NOT_FOUND\", \"message\": f\"Decision log {decision_id} not found for update.\"})\n",
        "\n",
        "            except Exception as e:\n",
        "                status = \"internal_error\"; errors.append({\"code\": \"UPDATE_ERROR\", \"message\": str(e)})\n",
        "                logger.error(f\"Error updating decision log {decision_id}: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "\n",
        "# --- Learning Integration Tool (Reworked Async) ---\n",
        "class LearningIntegrationTool:\n",
        "    \"\"\" Manages async integration of learning insights, triggers MLOps via Pub/Sub client proxy. Deployed as a service. \"\"\"\n",
        "    def __init__(self, kg_tool_proxy: Any, moe_registry_proxy: Any, expert_invoker_proxy: Any, pubsub_client_proxy: Any, config: EnhancedConfig):\n",
        "        # Inject dependencies\n",
        "        if not all([config, kg_tool_proxy, moe_registry_proxy, expert_invoker_proxy, pubsub_client_proxy]):\n",
        "            raise InitializationError(\"LearningIntegrationTool requires config and proxies for KG, MoE, Expert Invoker, and PubSub.\")\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.moe_registry = moe_registry_proxy\n",
        "        self.expert_invoker = expert_invoker_proxy\n",
        "        self.pubsub_client = pubsub_client_proxy\n",
        "        self.config = config\n",
        "        # TODO: Replace deque with persistent storage for production\n",
        "        self.learning_history = deque(maxlen=5000)\n",
        "        self.integration_rules = defaultdict(dict) # Rules for how to handle different knowledge types\n",
        "        self.bias_detectors = [] # Placeholder for bias detection functions/modules\n",
        "        self.mlops_trigger_topic_name = config.mlops_trigger_topic\n",
        "        self.project = config.gcp.project_id\n",
        "        self.location = config.gcp.region\n",
        "        self.logger = logging.getLogger('MIZ-OKI.LearningIntegrationTool')\n",
        "        self._load_rules() # Load rules synchronously\n",
        "        self.logger.info(\"Learning Integration Tool logic initialized (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"LearningIntegrationTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    def _load_rules(self):\n",
        "        \"\"\"Loads integration rules (e.g., from config file, DB).\"\"\"\n",
        "        # --- TODO: Load rules from a persistent/configurable source ---\n",
        "        # Example rules:\n",
        "        self.integration_rules['feedback_batch'] = [\n",
        "            {\"action\": \"update_kg\", \"params\": {\"label\": \"Feedback\"}},\n",
        "            {\"action\": \"retrain_expert\", \"params\": {\"expert_domain\": \"sentiment_analysis\"}, \"condition\": \"sentiment_negative_high\"}\n",
        "        ]\n",
        "        self.integration_rules['validation_alert'] = [\n",
        "            {\"action\": \"update_kg\", \"params\": {\"label\": \"SystemAlert\"}},\n",
        "            {\"action\": \"retrain_expert\", \"params\": {\"expert_domain\": \"drift_detection_model\"}, \"condition\": \"drift_detected\"}\n",
        "        ]\n",
        "        # --- End TODO ---\n",
        "        self.logger.info(f\"Loaded {sum(len(v) for v in self.integration_rules.values())} integration rules.\")\n",
        "\n",
        "    async def _run_bias_checks(self, knowledge_data: Any, source: str) -> Tuple[List[Dict], bool]:\n",
        "        \"\"\"Placeholder for running bias detection checks.\"\"\"\n",
        "        # --- TODO: Implement bias detection logic ---\n",
        "        # - Requires access to sensitive attributes and model predictions.\n",
        "        # - Calculate fairness metrics.\n",
        "        # --- End TODO ---\n",
        "        logger.debug(f\"LI: Running bias checks for source '{source}' (Placeholder).\")\n",
        "        await asyncio.sleep(random.uniform(0.02, 0.08)) # Simulate check time\n",
        "        bias_found = random.random() < 0.03 # Simulate 3% chance\n",
        "        checks_log = [{\"check\": \"demographic_parity\", \"result\": \"passed\" if not bias_found else \"failed\"}]\n",
        "        return checks_log, bias_found\n",
        "\n",
        "    async def integrate_learning(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Integrates learning insights/feedback asynchronously. Expects/Returns MIZ OKI payload. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); knowledge_type = payload.get(\"knowledge_type\"); knowledge_data = payload.get(\"knowledge_data\"); source = payload.get(\"source\"); importance = payload.get(\"importance\", 0.5)\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # Prepare MIZ OKI response structure\n",
        "        response = self._create_miz_oki_response(input_data, \"pending\")\n",
        "        integration_id = f\"li_{knowledge_type or 'unknown'}_{uuid.uuid4().hex[:8]}\"\n",
        "        log_entry = { # Detailed log\n",
        "            \"integration_id\": integration_id, \"trace_id\": trace_id,\n",
        "            \"timestamp\": response[\"timestamp\"], \"knowledge_type\": knowledge_type,\n",
        "            \"source\": source, \"importance\": importance,\n",
        "            \"input_preview\": str(knowledge_data)[:250], # Limit preview size\n",
        "            \"status\": \"pending\", \"bias_checks\": [], \"actions_taken\": [], \"triggered_messages\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            if not knowledge_type or knowledge_data is None:\n",
        "                raise ValueError(\"Missing 'knowledge_type' or 'knowledge_data' in payload.\")\n",
        "\n",
        "            # 1. Bias Checks (Optional)\n",
        "            bias_checks_log, bias_found = await self._run_bias_checks(knowledge_data, source)\n",
        "            log_entry[\"bias_checks\"] = bias_checks_log\n",
        "            if bias_found:\n",
        "                log_entry[\"bias_mitigation\"] = \"Flagged\" # Or trigger specific mitigation action\n",
        "                logger.warning(f\"LI {integration_id}: Potential bias detected in knowledge from '{source}'.\")\n",
        "                # Decide whether to proceed or halt based on policy\n",
        "\n",
        "            # 2. Determine Actions based on Rules\n",
        "            actions_to_take = []\n",
        "            rules_for_type = self.integration_rules.get(knowledge_type, [])\n",
        "            if rules_for_type:\n",
        "                logger.debug(f\"LI {integration_id}: Applying {len(rules_for_type)} rules for type '{knowledge_type}'.\")\n",
        "                for rule in rules_for_type:\n",
        "                    condition = rule.get(\"condition\")\n",
        "                    # --- TODO: Implement condition evaluation logic ---\n",
        "                    # Example: Check if 'sentiment_negative_high' flag is set in knowledge_data\n",
        "                    condition_met = True # Placeholder\n",
        "                    if condition:\n",
        "                         # condition_met = evaluate_condition(condition, knowledge_data, log_entry)\n",
        "                         pass\n",
        "                    # --- End TODO ---\n",
        "                    if condition_met:\n",
        "                        actions_to_take.append(rule) # Add rule (which contains action type and params)\n",
        "            else:\n",
        "                logger.info(f\"LI {integration_id}: No specific integration rules found for type '{knowledge_type}'. Default actions might apply.\")\n",
        "                # Optionally define default actions, e.g., always log to KG\n",
        "                actions_to_take.append({\"action\": \"update_kg\", \"params\": {\"label\": knowledge_type or \"GenericKnowledge\"}})\n",
        "\n",
        "            # 3. Execute Actions Asynchronously\n",
        "            if actions_to_take:\n",
        "                action_tasks = [self._execute_action(action, integration_id, log_entry) for action in actions_to_take]\n",
        "                action_results = await asyncio.gather(*action_tasks, return_exceptions=True)\n",
        "                # Process results, update log_entry\n",
        "                processed_action_results = []\n",
        "                for i, res in enumerate(action_results):\n",
        "                    action_info = actions_to_take[i]\n",
        "                    if isinstance(res, Exception):\n",
        "                        action_info[\"result\"] = {\"status\": \"error\", \"error\": str(res)}\n",
        "                        logger.error(f\"LI Action '{action_info.get('action')}' failed: {res}\")\n",
        "                    elif isinstance(res, dict):\n",
        "                        action_info[\"result\"] = res\n",
        "                    processed_action_results.append(action_info)\n",
        "                log_entry[\"actions_taken\"] = processed_action_results\n",
        "            else:\n",
        "                logger.info(f\"LI {integration_id}: No LI actions determined for '{knowledge_type}'.\")\n",
        "\n",
        "            response[\"status\"] = \"success\"\n",
        "            log_entry[\"status\"] = \"success\"\n",
        "            response[\"payload\"] = log_entry # Return the detailed log as payload\n",
        "\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Async LI FAILED for '{knowledge_type}' ({integration_id}): {e}\", exc_info=True)\n",
        "             response[\"status\"] = \"internal_error\"; errors.append({\"code\": \"LI_ERROR\", \"message\": str(e)})\n",
        "             log_entry[\"status\"] = \"failed\"; log_entry[\"error\"] = str(e)\n",
        "             response[\"payload\"] = log_entry # Return log even on failure\n",
        "\n",
        "        log_entry[\"total_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        self.learning_history.append(log_entry) # Add to in-memory history\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = log_entry[\"total_duration_ms\"]\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        # --- TODO: Persist log_entry async via KG Tool API or logging service ---\n",
        "        return response\n",
        "\n",
        "    async def _execute_action(self, action_rule: Dict, integration_id: str, log_entry_ref: Dict) -> Dict:\n",
        "        \"\"\" Executes LI action, potentially triggering MLOps via Pub/Sub client proxy using MIZ OKI format. \"\"\"\n",
        "        action_type = action_rule.get(\"action\")\n",
        "        action_params = action_rule.get(\"params\", {})\n",
        "        action_log = {\"action_type\": action_type, \"params\": action_params, \"status\": \"pending\"}\n",
        "        start_time = time.monotonic()\n",
        "        self.logger.info(f\"LI Action ({integration_id}): Executing '{action_type}'...\")\n",
        "\n",
        "        try:\n",
        "            if action_type == \"update_kg\":\n",
        "                # --- TODO: Implement KG update via KG Tool API Proxy ---\n",
        "                # kg_request = {\"payload\": {\"entities\": [...], \"relationships\": [...]}, \"trace_id\": log_entry_ref.get(\"trace_id\")}\n",
        "                # kg_response = await self.kg_tool.add_entities_bulk_endpoint(request=kg_request)\n",
        "                # if kg_response.get(\"status\") == \"success\": action_log[\"status\"] = \"success\"\n",
        "                # else: action_log[\"status\"] = \"failed\"; action_log[\"error\"] = kg_response.get(\"error_details\")\n",
        "                action_log[\"status\"] = \"success\"; action_log[\"details\"] = \"KG update executed (Simulated).\"\n",
        "                await asyncio.sleep(0.05) # Simulate KG call\n",
        "                # --- End TODO ---\n",
        "            elif action_type == \"retrain_expert\" or action_type == \"finetune_llm\":\n",
        "                if not self.pubsub_client: raise RuntimeError(\"PubSub client proxy unavailable.\")\n",
        "                if not self.project: raise ConfigurationError(\"GCP Project ID not configured.\")\n",
        "\n",
        "                pipeline_name = action_params.get(\"pipeline_name\", \"miz3_expert_training_pipeline_v1deploy_apireg\" if action_type==\"retrain_expert\" else \"miz3_llm_finetuning_pipeline\")\n",
        "                # Prepare pipeline parameters, merging rule params with defaults/context\n",
        "                mlops_pipeline_params = {\n",
        "                    \"project\": self.project,\n",
        "                    \"location\": self.location,\n",
        "                    \"trigger_source\": f\"li:{integration_id}\",\n",
        "                    \"timestamp_trigger\": log_entry_ref[\"timestamp\"],\n",
        "                    **action_params # Include params from the rule (e.g., model_id, dataset_uri)\n",
        "                }\n",
        "                # Ensure required params for the specific pipeline are present\n",
        "                if \"model_display_name_prefix\" not in mlops_pipeline_params and \"model_id_to_retrain\" not in mlops_pipeline_params:\n",
        "                     mlops_pipeline_params[\"model_display_name_prefix\"] = f\"li_retrained_{action_params.get('expert_domain', 'model')}\"\n",
        "\n",
        "                # Prepare MIZ OKI Pub/Sub Message\n",
        "                message_data = {\n",
        "                    \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "                    \"event_type\": \"trigger_mlops_pipeline\",\n",
        "                    \"payload\": {\"pipeline_name\": pipeline_name, \"parameters\": mlops_pipeline_params},\n",
        "                    \"metadata\": {\"trace_id\": log_entry_ref.get(\"trace_id\"), \"source_component\": \"LearningIntegrationTool\", \"li_integration_id\": integration_id}\n",
        "                }\n",
        "                message_bytes = json.dumps(message_data).encode('utf-8')\n",
        "                mlops_topic_full_path = f\"projects/{self.project}/topics/{self.mlops_trigger_topic_name}\"\n",
        "\n",
        "                # Call Pub/Sub Client Proxy method\n",
        "                message_id = await self.pubsub_client.publish(mlops_topic_full_path, message_bytes)\n",
        "\n",
        "                action_log[\"status\"] = \"training_triggered\"; action_log[\"pipeline_name\"] = pipeline_name; action_log[\"message_id\"] = message_id\n",
        "                log_entry_ref[\"triggered_messages\"].append(message_id) # Log triggered message ID\n",
        "            else:\n",
        "                action_log[\"status\"] = \"skipped\"; action_log[\"reason\"] = f\"Unsupported LI action type: {action_type}\"\n",
        "                logger.warning(f\"LI Action ({integration_id}): Unsupported action type '{action_type}'.\")\n",
        "\n",
        "        except Exception as exec_e:\n",
        "            logger.error(f\"Error executing async LI action '{action_type}': {exec_e}\", exc_info=True)\n",
        "            action_log[\"status\"] = \"error\"; action_log[\"error\"] = str(exec_e)\n",
        "\n",
        "        action_log[\"duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        self.logger.info(f\"LI Action ({integration_id}): Finished '{action_type}' with status '{action_log['status']}'.\")\n",
        "        return action_log\n",
        "\n",
        "# --- Holistic Optimizer Tool (Reworked Async) ---\n",
        "class HolisticOptimizerTool:\n",
        "    \"\"\" Optimizes overall system performance asynchronously based on objectives. Deployed as a service. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any, hde_tool_proxy: Any, moe_registry_proxy: Any, expert_invoker_proxy: Any):\n",
        "        # Inject dependencies\n",
        "        if not all([config, kg_tool_proxy, hde_tool_proxy, moe_registry_proxy, expert_invoker_proxy]):\n",
        "            raise InitializationError(\"HolisticOptimizerTool requires config and proxies for KG, HDE, MoE Registry, and Expert Invoker.\")\n",
        "        self.config = config\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.hde_tool = hde_tool_proxy\n",
        "        self.moe_registry = moe_registry_proxy\n",
        "        self.expert_invoker = expert_invoker_proxy\n",
        "        self.objectives: Dict[str, Dict] = {}\n",
        "        self.targets: Dict[str, float] = {}\n",
        "        self.baselines: Dict[str, float] = {}\n",
        "        # TODO: Replace deque with persistent storage (e.g., Timeseries DB, BQ) for production\n",
        "        self.metric_history = defaultdict(lambda: deque(maxlen=1000))\n",
        "        self.forecasting_models: Dict[str, str] = {}\n",
        "        # TODO: Replace deque with persistent storage\n",
        "        self.optimization_history = deque(maxlen=500)\n",
        "        self.logger = logging.getLogger('MIZ-OKI.HolisticOptimizerTool')\n",
        "        self._load_objectives_from_config() # Sync load ok\n",
        "        self.logger.info(\"Holistic Optimizer Tool logic initialized (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"HolisticOptimizerTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    def _load_objectives_from_config(self):\n",
        "        \"\"\"Loads objectives, targets, baselines, and forecasters from the main config object.\"\"\"\n",
        "        try:\n",
        "            self.objectives = self.config.get(\"optimizer_objectives\", {})\n",
        "            kpis_config = self.config.get(\"business_impact.kpis\", {})\n",
        "            self.targets = {kpi: data.get('target') for kpi, data in kpis_config.items() if 'target' in data and isinstance(data.get('target'), (int, float))}\n",
        "            self.baselines = {kpi: data.get('baseline', 0) for kpi, data in kpis_config.items() if isinstance(data.get('baseline'), (int, float))}\n",
        "            self.forecasting_models = self.config.get(\"optimizer_forecasting_models\", {})\n",
        "            self.logger.info(f\"PO: Loaded {len(self.objectives)} objectives, {len(self.targets)} targets, {len(self.forecasting_models)} forecasters.\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"PO: Failed to load objectives from config: {e}\", exc_info=True)\n",
        "            # Initialize with empty dicts to prevent errors later\n",
        "            self.objectives = {}; self.targets = {}; self.baselines = {}; self.forecasting_models = {}\n",
        "\n",
        "    def update_metric(self, metric_name: str, value: float, timestamp_iso: Optional[str] = None):\n",
        "        \"\"\"Updates the history for a given metric. Needs persistent storage.\"\"\"\n",
        "        # --- TODO: Implement persistent storage update ---\n",
        "        # Example: Write to BigQuery or Timeseries DB\n",
        "        # For now, using in-memory deque placeholder\n",
        "        try:\n",
        "            ts = timestamp_iso or datetime.now(datetime.timezone.utc).isoformat()\n",
        "            metric_value = float(value)\n",
        "            if not np.isnan(metric_value) and not np.isinf(metric_value):\n",
        "                self.metric_history[metric_name].append({\"timestamp\": ts, \"value\": metric_value})\n",
        "                # logger.debug(f\"PO: Updated metric '{metric_name}' with value {metric_value}. History size: {len(self.metric_history[metric_name])}\")\n",
        "            else:\n",
        "                 logger.warning(f\"PO: Received invalid value (NaN/Inf) for metric '{metric_name}'. Ignoring.\")\n",
        "        except (ValueError, TypeError) as e:\n",
        "            logger.error(f\"PO: Failed to update metric '{metric_name}' with value '{value}': {e}\")\n",
        "        # --- End TODO ---\n",
        "\n",
        "    async def _predict_metric_value(self, metric_name: str, trace_id: Optional[str], horizon_steps: int = 1) -> Optional[float]:\n",
        "        \"\"\" Predicts metric value using MoE Invoker API proxy. \"\"\"\n",
        "        expert_alias = self.forecasting_models.get(metric_name)\n",
        "        if not expert_alias:\n",
        "            logger.warning(f\"PO: No forecasting model configured for metric '{metric_name}'.\")\n",
        "            return None\n",
        "        if not self.moe_registry or not self.expert_invoker:\n",
        "            logger.error(\"PO: MoE Registry or Expert Invoker proxy unavailable for prediction.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Find expert via MoE Registry API proxy\n",
        "            expert_id = await self.moe_registry.find_expert_for_task(task_type=\"forecasting\", domain=metric_name)\n",
        "            if not expert_id:\n",
        "                logger.warning(f\"PO: Could not find forecasting expert via MoE for '{metric_name}'.\")\n",
        "                return None\n",
        "            expert_details = await self.moe_registry.get_expert_details(expert_id)\n",
        "            expert_endpoint = expert_details.get(\"endpoint\") if expert_details else None\n",
        "            if not expert_endpoint:\n",
        "                logger.warning(f\"PO: Endpoint not found for forecasting expert '{expert_id}'.\")\n",
        "                return None\n",
        "\n",
        "            # --- TODO: Fetch sufficient history from persistent store ---\n",
        "            # Example using in-memory deque:\n",
        "            recent_history = [h['value'] for h in list(self.metric_history.get(metric_name, []))[-20:]] # Get last 20 points\n",
        "            # --- End TODO ---\n",
        "            if len(recent_history) < 5: # Need minimum history for forecast\n",
        "                logger.warning(f\"PO: Insufficient history ({len(recent_history)} points) for forecasting '{metric_name}'.\")\n",
        "                return None\n",
        "\n",
        "            # Prepare input for the forecasting model\n",
        "            input_data_payload = {\"historical_values\": recent_history, \"steps_to_predict\": horizon_steps}\n",
        "\n",
        "            # Call Expert Invoker API proxy\n",
        "            invoker_request = {\n",
        "                \"payload\": {\"endpoint\": expert_endpoint, \"data\": input_data_payload},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"invoker_predict_{metric_name}_{trace_id or uuid.uuid4().hex[:6]}\"\n",
        "            }\n",
        "            invoker_response = await self.expert_invoker.invoke(request=invoker_request) # Call API proxy\n",
        "\n",
        "            if invoker_response.get(\"status\") == \"success\":\n",
        "                result_payload = invoker_response.get(\"payload\", {})\n",
        "                # --- TODO: Adapt parsing based on actual forecaster output format ---\n",
        "                prediction = result_payload.get(\"prediction\")\n",
        "                if isinstance(prediction, list) and len(prediction) >= horizon_steps:\n",
        "                    return float(prediction[horizon_steps - 1])\n",
        "                # --- End TODO ---\n",
        "                else: logger.warning(f\"PO: Forecast expert '{expert_id}' returned invalid format or insufficient steps: {prediction}\")\n",
        "            else:\n",
        "                logger.warning(f\"PO: Expert Invoker API call failed for forecast expert '{expert_id}': {invoker_response.get('error_details')}\")\n",
        "\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PO: Async forecast via invoker proxy failed for expert '{expert_alias}': {e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    def _evaluate_objectives(self, metric_state: Dict[str, float]) -> Dict[str, float]:\n",
        "        \"\"\"Evaluates objectives based on current/predicted metrics (sync logic ok).\"\"\"\n",
        "        objective_scores = {}\n",
        "        if not self.objectives: return {}\n",
        "        for obj_id, obj_data in self.objectives.items():\n",
        "            target = obj_data.get('target')\n",
        "            baseline = obj_data.get('baseline', 0)\n",
        "            metric_name = obj_data.get('metric') # Assuming simple 1 metric per objective for now\n",
        "            lower_is_better = obj_data.get('lower_is_better', False)\n",
        "\n",
        "            if metric_name and metric_name in metric_state and target is not None:\n",
        "                current_value = metric_state[metric_name]\n",
        "                try:\n",
        "                    target_f = float(target); baseline_f = float(baseline)\n",
        "                    # Normalize score between 0 (at baseline) and 1 (at or beyond target)\n",
        "                    if target_f == baseline_f: score = 1.0 if (lower_is_better and current_value <= target_f) or (not lower_is_better and current_value >= target_f) else 0.0\n",
        "                    elif lower_is_better: score = (baseline_f - current_value) / (baseline_f - target_f)\n",
        "                    else: score = (current_value - baseline_f) / (target_f - baseline_f)\n",
        "                    objective_scores[obj_id] = max(0.0, min(1.0, score)) # Clamp score between 0 and 1\n",
        "                except (ValueError, TypeError):\n",
        "                    logger.warning(f\"PO: Invalid target/baseline for objective '{obj_id}'. Skipping score calculation.\")\n",
        "                    objective_scores[obj_id] = 0.0 # Or None?\n",
        "            else:\n",
        "                logger.debug(f\"PO: Cannot evaluate objective '{obj_id}'. Missing metric '{metric_name}' in state or target not set.\")\n",
        "                objective_scores[obj_id] = 0.0 # Or None?\n",
        "        return objective_scores\n",
        "\n",
        "    async def check_and_optimize(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Evaluates objectives async and triggers HDE Tool API proxy if needed. Expects/Returns MIZ OKI payload. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); predictive = payload.get(\"predictive\", False); context = payload.get(\"context\", {}) # Optional context\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # Prepare MIZ OKI response\n",
        "        response = self._create_miz_oki_response(input_data, \"pending\")\n",
        "        opt_id = f\"po_check_{uuid.uuid4().hex[:8]}\"\n",
        "        opt_log = { # Detailed log\n",
        "            \"optimization_id\": opt_id, \"trace_id\": trace_id,\n",
        "            \"timestamp\": response[\"timestamp\"], \"predictive_check\": predictive,\n",
        "            \"status\": \"started\", \"state_evaluated\": None, \"eval_state_type\": None,\n",
        "            \"objective_scores\": {}, \"triggered_hde\": False, \"hde_decision_log_ref\": None\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # 1. Get current/predicted state\n",
        "            # --- TODO: Fetch current_state from persistent metric store ---\n",
        "            current_state = {m: hist[-1][\"value\"] for m, hist in self.metric_history.items() if hist} # Placeholder using in-memory\n",
        "            # --- End TODO ---\n",
        "            state_to_evaluate = current_state; eval_state_type = \"current\"\n",
        "\n",
        "            if predictive:\n",
        "                predicted_state = {}\n",
        "                predict_tasks = [self._predict_metric_value(metric, trace_id) for metric in self.targets.keys()]\n",
        "                predictions = await asyncio.gather(*predict_tasks)\n",
        "                for i, metric in enumerate(self.targets.keys()):\n",
        "                    if predictions[i] is not None: predicted_state[metric] = predictions[i]\n",
        "                if predicted_state:\n",
        "                    state_to_evaluate = predicted_state; eval_state_type = \"predicted\"\n",
        "                    logger.info(f\"PO {opt_id}: Using predicted state for evaluation: {predicted_state}\")\n",
        "                else: logger.warning(f\"PO {opt_id}: Predictive check requested but failed to get predictions. Using current state.\")\n",
        "\n",
        "            opt_log[\"state_evaluated\"] = state_to_evaluate; opt_log[\"eval_state_type\"] = eval_state_type\n",
        "            if not state_to_evaluate: raise ValueError(\"Insufficient metric data for evaluation.\")\n",
        "\n",
        "            # 2. Evaluate objectives (Sync logic is fine here)\n",
        "            objective_scores = self._evaluate_objectives(state_to_evaluate)\n",
        "            opt_log[\"objective_scores\"] = objective_scores\n",
        "            logger.info(f\"PO {opt_id}: Evaluated objective scores ({eval_state_type} state): {objective_scores}\")\n",
        "\n",
        "            # 3. Check Thresholds and Trigger HDE via API Proxy\n",
        "            threshold = self.config.system_thresholds.optimization_threshold\n",
        "            failing_objectives = {obj_id: score for obj_id, score in objective_scores.items() if score < threshold}\n",
        "\n",
        "            if failing_objectives:\n",
        "                # Prioritize the objective with the lowest score\n",
        "                primary_failing_id, lowest_score = sorted(failing_objectives.items(), key=lambda item: item[1])[0]\n",
        "                trigger_reason = f\"Objective '{primary_failing_id}' score ({lowest_score:.3f}) below threshold ({threshold:.3f}).\"\n",
        "                self.logger.warning(f\"PO {opt_id}: {trigger_reason} Triggering HDE Tool API proxy for 'system_optimization'.\")\n",
        "                opt_log[\"triggered_hde\"] = True; opt_log[\"trigger_reason\"] = trigger_reason\n",
        "\n",
        "                if not self.hde_tool or not hasattr(self.hde_tool, 'make_decision'):\n",
        "                    raise RuntimeError(\"HDE Tool proxy unavailable or method missing.\")\n",
        "\n",
        "                # Prepare MIZ OKI payload for HDE Tool API proxy\n",
        "                hde_context = {\"current_metrics\": current_state, \"predicted_metrics\": predicted_state if predictive else None, \"objective_scores\": objective_scores, \"failing_objectives\": failing_objectives, **context} # Pass context from input\n",
        "                hde_request = {\n",
        "                    \"payload\": {\"decision_type\": \"system_optimization\", \"context\": hde_context},\n",
        "                    \"trace_id\": trace_id, \"request_id\": f\"hde_opt_{opt_id}\"\n",
        "                }\n",
        "                # Call HDE Tool API via proxy\n",
        "                decision_response = await self.hde_tool.make_decision(input_data=hde_request) # Pass MIZ OKI structure\n",
        "\n",
        "                decision_log = decision_response.get(\"payload\", {}) # HDE returns its log in the payload\n",
        "                opt_log[\"hde_decision_log_ref\"] = decision_log.get(\"decision_id\")\n",
        "                opt_log[\"hde_decision_status\"] = decision_log.get(\"status\")\n",
        "\n",
        "                # Determine PO status based on HDE outcome\n",
        "                if decision_log.get(\"status\") in [\"success\", \"approved_by_engine\"] and decision_log.get(\"action_recommended\"):\n",
        "                    opt_log[\"status\"] = \"optimization_action_recommended\"\n",
        "                elif decision_log.get(\"status\") == \"ethics_review_required\":\n",
        "                     opt_log[\"status\"] = \"optimization_needs_review\"\n",
        "                else: # HDE failed, or succeeded but recommended no action\n",
        "                    opt_log[\"status\"] = \"optimization_check_complete_no_action\"\n",
        "            else:\n",
        "                opt_log[\"status\"] = \"objectives_met\"\n",
        "                logger.info(f\"PO {opt_id}: All objectives met or above threshold.\")\n",
        "\n",
        "            response[\"status\"] = \"success\" # PO check itself succeeded\n",
        "            response[\"payload\"] = opt_log # Return PO log\n",
        "\n",
        "        except Exception as e:\n",
        "             logger.error(f\"PO check_and_optimize FAILED (ID: {opt_id}): {e}\", exc_info=True)\n",
        "             response[\"status\"] = \"internal_error\"; errors.append({\"code\": \"PO_ERROR\", \"message\": str(e)})\n",
        "             opt_log[\"status\"] = \"failed\"; opt_log[\"error\"] = str(e)\n",
        "             response[\"payload\"] = opt_log # Return log even on failure\n",
        "\n",
        "        opt_log[\"total_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        self.optimization_history.append(opt_log) # Add to in-memory history\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = opt_log[\"total_duration_ms\"]\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        # --- TODO: Persist opt_log async via KG Tool API or logging service ---\n",
        "        return response\n",
        "\n",
        "    async def get_current_objective_priorities(self, input_data: Dict = None) -> Dict:\n",
        "         \"\"\" Returns current objective priorities (lower score = higher priority). Expects/Returns MIZ OKI. \"\"\"\n",
        "         start_time = time.monotonic(); errors = []\n",
        "         request_data = input_data or {}\n",
        "         trace_id = request_data.get(\"trace_id\")\n",
        "         status = \"pending\"; response_payload = None\n",
        "\n",
        "         try:\n",
        "             # --- TODO: Fetch current_state from persistent metric store ---\n",
        "             current_state = {m: hist[-1][\"value\"] for m, hist in self.metric_history.items() if hist} # Placeholder\n",
        "             # --- End TODO ---\n",
        "             if not current_state: raise ValueError(\"Insufficient metric data to calculate priorities.\")\n",
        "\n",
        "             objective_scores = self._evaluate_objectives(current_state)\n",
        "             # Simple priority: 1.0 - score (lower score means higher priority)\n",
        "             priorities = {obj_id: max(0.0, min(1.0, 1.0 - score)) for obj_id, score in objective_scores.items()}\n",
        "\n",
        "             status = \"success\"; response_payload = {\"priorities\": priorities, \"scores_evaluated\": objective_scores}\n",
        "             logger.debug(f\"PO: Calculated objective priorities: {priorities}\")\n",
        "\n",
        "         except Exception as e:\n",
        "              status = \"internal_error\"; errors.append({\"code\": \"PRIORITY_ERROR\", \"message\": str(e)})\n",
        "              logger.error(f\"PO: Failed to get objective priorities: {e}\", exc_info=True)\n",
        "\n",
        "         response = self._create_miz_oki_response(request_data, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "# --- Autonomous Goal Generator Tool (Reworked Async) ---\n",
        "class AutonomousGoalGeneratorTool:\n",
        "    \"\"\" Generates goals based on optimization status, triggers planning workflow via REAL Vertex client proxy. Deployed as a service. \"\"\"\n",
        "    def __init__(self, optimizer_tool_proxy: Any, workflow_client_proxy: Any, config: EnhancedConfig):\n",
        "        # Inject dependencies\n",
        "        if not config or not optimizer_tool_proxy or not workflow_client_proxy:\n",
        "            raise InitializationError(\"AutonomousGoalGeneratorTool requires config and proxies for Optimizer Tool and Workflow Client.\")\n",
        "        self.optimizer_tool = optimizer_tool_proxy\n",
        "        self.workflow_client = workflow_client_proxy # Use REAL client proxy\n",
        "        self.config = config\n",
        "        self.project = config.gcp.project_id\n",
        "        self.location = config.gcp.region\n",
        "        # --- TODO: Replace dict/deque with persistent storage (e.g., Firestore, BQ) ---\n",
        "        self.goals: Dict[str, Dict] = {}\n",
        "        self.goal_history = deque(maxlen=1000)\n",
        "        # --- End TODO ---\n",
        "        self.logger = logging.getLogger('MIZ-OKI.AutonomousGoalGeneratorTool')\n",
        "        self.logger.info(\"Autonomous Goal Generator Tool logic initialized (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"AutonomousGoalGeneratorTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    async def identify_and_generate_goals(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Analyzes objectives via PO Tool API proxy, triggers planning workflows via Vertex client proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # Prepare MIZ OKI response\n",
        "        response = self._create_miz_oki_response(input_data, \"pending\")\n",
        "        agg_id = f\"agg_run_{uuid.uuid4().hex[:8]}\"\n",
        "        run_log = { # Detailed log\n",
        "            \"agg_run_id\": agg_id, \"trace_id\": trace_id,\n",
        "            \"timestamp\": response[\"timestamp\"], \"status\": \"started\",\n",
        "            \"goals_generated\": 0, \"triggered_workflows\": [], \"objective_priorities_evaluated\": None\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            if not self.optimizer_tool or not hasattr(self.optimizer_tool, 'get_current_objective_priorities'):\n",
        "                 raise InitializationError(\"Optimizer Tool proxy unavailable or method missing.\")\n",
        "\n",
        "            # 1. Call PO Tool API proxy to get objective priorities\n",
        "            po_request = {\"miz_oki_version\": \"3.0\", \"trace_id\": trace_id, \"request_id\": f\"po_get_prio_for_agg_{agg_id}\"}\n",
        "            po_response = await self.optimizer_tool.get_current_objective_priorities(input_data=po_request) # Call proxy method\n",
        "\n",
        "            if po_response.get(\"status\") != \"success\":\n",
        "                raise RuntimeError(f\"Failed to get objective priorities from PO Tool API proxy: {po_response.get('error_details')}\")\n",
        "\n",
        "            objective_priorities = po_response.get(\"payload\", {}).get(\"priorities\", {}) # Lower score = higher priority\n",
        "            run_log[\"objective_priorities_evaluated\"] = objective_priorities\n",
        "            logger.info(f\"AGG {agg_id}: Evaluated objective priorities: {objective_priorities}\")\n",
        "\n",
        "            # 2. Identify objectives needing goals\n",
        "            goal_gen_threshold = self.config.system_thresholds.goal_generation_threshold # Lower score = higher priority\n",
        "            generation_tasks = []\n",
        "            # --- TODO: Load active goals state from persistent store ---\n",
        "            active_goal_objectives = {g.get(\"related_objective_id\") for g in self.goals.values() if g.get(\"status\") == \"active\"} # Using in-memory placeholder\n",
        "            # --- End TODO ---\n",
        "\n",
        "            for obj_id, priority_score in objective_priorities.items():\n",
        "                 # Trigger goal if priority is high (score is low) and no active goal exists for this objective\n",
        "                 if priority_score > (1.0 - goal_gen_threshold) and obj_id not in active_goal_objectives:\n",
        "                      logger.info(f\"AGG {agg_id}: Objective '{obj_id}' priority ({priority_score:.3f}) exceeds threshold ({1.0 - goal_gen_threshold:.3f}). Queueing goal generation.\")\n",
        "                      # Pass objective ID and score to generation function\n",
        "                      generation_tasks.append(self._generate_and_trigger_goal(obj_id, priority_score, trace_id))\n",
        "                 else:\n",
        "                      logger.debug(f\"AGG {agg_id}: Skipping goal generation for objective '{obj_id}' (Priority: {priority_score:.3f}, Active: {obj_id in active_goal_objectives}).\")\n",
        "\n",
        "            # 3. Trigger Goal Generation & Planning Workflows Concurrently\n",
        "            if generation_tasks:\n",
        "                 results = await asyncio.gather(*generation_tasks, return_exceptions=True)\n",
        "                 for i, res in enumerate(results):\n",
        "                      if isinstance(res, dict) and res.get(\"status\") == \"planning_triggered\":\n",
        "                           run_log[\"goals_generated\"] += 1\n",
        "                           run_log[\"triggered_workflows\"].append(res) # Contains goal_id, execution_id\n",
        "                      elif isinstance(res, Exception):\n",
        "                           logger.error(f\"AGG {agg_id}: Goal generation/trigger failed: {res}\", exc_info=True)\n",
        "                           errors.append({\"code\": \"GOAL_GEN_ERROR\", \"message\": str(res)})\n",
        "                      else: # Handle unexpected non-exception, non-dict results\n",
        "                           logger.error(f\"AGG {agg_id}: Received unexpected result from goal generation: {res}\")\n",
        "                           errors.append({\"code\": \"UNEXPECTED_RESULT\", \"message\": \"Unexpected result during goal generation.\"})\n",
        "\n",
        "            run_log[\"status\"] = \"success\" if not errors else \"partial_failure\"\n",
        "            response[\"status\"] = run_log[\"status\"]\n",
        "            response[\"payload\"] = run_log\n",
        "\n",
        "        except Exception as e:\n",
        "             logger.error(f\"AGG identify_and_generate_goals FAILED (ID: {agg_id}): {e}\", exc_info=True)\n",
        "             response[\"status\"] = \"internal_error\"; errors.append({\"code\": \"AGG_ERROR\", \"message\": str(e)})\n",
        "             run_log[\"status\"] = \"failed\"; run_log[\"error\"] = str(e)\n",
        "             response[\"payload\"] = run_log # Return log even on failure\n",
        "\n",
        "        run_log[\"total_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = run_log[\"total_duration_ms\"]\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        self.logger.info(f\"AGG run {agg_id} finished. Status: {response['status']}. Goals Generated: {run_log['goals_generated']}\")\n",
        "        return response\n",
        "\n",
        "    async def _generate_and_trigger_goal(self, obj_id: str, priority_score: float, trace_id: Optional[str]) -> Dict:\n",
        "        \"\"\" Generates goal data, triggers planning workflow via REAL Vertex client proxy. \"\"\"\n",
        "        goal_id = f\"goal_{obj_id}_{uuid.uuid4().hex[:6]}\"\n",
        "        try:\n",
        "            # --- Goal Data Generation (Sync logic ok) ---\n",
        "            obj_config = self.config.get(f\"optimizer_objectives.{obj_id}\", {})\n",
        "            kpis = [m['name'] for m in obj_config.get(\"metrics\", []) if 'name' in m]\n",
        "            # Priority mapping (e.g., higher score -> higher priority value 0-1)\n",
        "            priority_value = max(0.1, min(1.0, priority_score * 1.2)) # Example mapping\n",
        "\n",
        "            # Fetch targets/baselines from config\n",
        "            target_values = {kpi: self.config.get(f\"business_impact.kpis.{kpi}.target\") for kpi in kpis if self.config.get(f\"business_impact.kpis.{kpi}.target\") is not None}\n",
        "            baseline_values = {kpi: self.config.get(f\"business_impact.kpis.{kpi}.baseline\") for kpi in kpis if self.config.get(f\"business_impact.kpis.{kpi}.baseline\") is not None}\n",
        "\n",
        "            goal_data = {\n",
        "                \"id\": goal_id,\n",
        "                \"description\": f\"Autonomously generated goal to improve objective '{obj_config.get('name', obj_id)}' (Current Priority Score: {priority_score:.3f})\",\n",
        "                \"kpis\": kpis,\n",
        "                \"target_values\": target_values,\n",
        "                \"baseline_values\": baseline_values,\n",
        "                \"related_objective_id\": obj_id,\n",
        "                \"priority\": priority_value,\n",
        "                \"status\": \"planning\", # Initial status\n",
        "                \"source\": \"AutonomousGoalGeneratorTool\",\n",
        "                \"created_at\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "                \"trace_id\": trace_id\n",
        "            }\n",
        "            self.goals[goal_id] = goal_data # Add to in-memory store\n",
        "            # --- TODO: Persist goal_data async (e.g., Firestore, BQ) ---\n",
        "            # await self._persist_goal(goal_data)\n",
        "            # --- End TODO ---\n",
        "            # --- End Goal Data Generation ---\n",
        "\n",
        "            # Trigger Planning Workflow via Vertex Client Proxy\n",
        "            if not self.workflow_client: raise RuntimeError(\"Workflow client proxy unavailable.\")\n",
        "            if not self.project: raise ConfigurationError(\"GCP Project ID not configured.\")\n",
        "\n",
        "            planning_workflow_id = self.config.vertex_ai.planning_workflow_id\n",
        "            if not planning_workflow_id: raise ConfigurationError(\"Planning workflow ID not configured.\")\n",
        "\n",
        "            # Prepare MIZ OKI input for the planning workflow\n",
        "            workflow_input_payload = {\"goal_id\": goal_id, \"goal_details\": goal_data}\n",
        "            miz_oki_input = {\n",
        "                \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "                \"request_id\": f\"req_plan_{goal_id}\", \"trace_id\": trace_id,\n",
        "                \"source_component\": \"AutonomousGoalGeneratorTool\", \"target_component\": planning_workflow_id,\n",
        "                \"payload\": workflow_input_payload\n",
        "            }\n",
        "            # Use real Vertex client proxy method\n",
        "            execution_name = await self.workflow_client.start_workflow(\n",
        "                project=self.project, location=self.location, workflow_id=planning_workflow_id, miz_oki_input=miz_oki_input\n",
        "            )\n",
        "\n",
        "            if execution_name:\n",
        "                 self.goals[goal_id][\"planning_execution_id\"] = execution_name\n",
        "                 self.goals[goal_id][\"status\"] = \"planning\" # Update status\n",
        "                 # --- TODO: Persist goal update async ---\n",
        "                 # await self._persist_goal(self.goals[goal_id])\n",
        "                 # --- End TODO ---\n",
        "                 self.logger.info(f\"Triggered planning workflow {planning_workflow_id} (Exec: {execution_name}) for goal {goal_id}.\")\n",
        "                 return {\"status\": \"planning_triggered\", \"goal_id\": goal_id, \"execution_id\": execution_name}\n",
        "            else:\n",
        "                # If start_workflow returns None or empty string on failure\n",
        "                raise RuntimeError(\"Vertex client proxy failed to start planning workflow (returned no execution name).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate/trigger goal for objective {obj_id}: {e}\", exc_info=True)\n",
        "            self.goals.pop(goal_id, None) # Remove partial goal on failure\n",
        "            # --- TODO: Remove from persistent store if creation failed mid-way ---\n",
        "            raise # Re-raise exception to be caught by the caller\n",
        "\n",
        "    # --- Add get_active_goals and add_goal methods (conceptual, need persistence) ---\n",
        "    async def get_active_goals(self, request: Dict) -> Dict: # Expects MIZ OKI\n",
        "        \"\"\" Retrieves active goals. Needs persistent store integration. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        status = \"pending\"; response_payload = None\n",
        "        try:\n",
        "            # --- TODO: Load goals from persistent store ---\n",
        "            active_goals_list = [g for g in self.goals.values() if g.get(\"status\") == \"active\"] # Using in-memory placeholder\n",
        "            # --- End TODO ---\n",
        "            status = \"success\"; response_payload = {\"active_goals\": active_goals_list}\n",
        "        except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"GOAL_FETCH_ERROR\", \"message\": str(e)})\n",
        "            logger.error(f\"Error fetching active goals: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    async def add_goal(self, request: Dict) -> Dict: # Expects MIZ OKI\n",
        "        \"\"\" Adds a manual goal and triggers planning. Needs persistence. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        payload = request.get(\"payload\", {})\n",
        "        # Basic validation\n",
        "        if not all(k in payload for k in [\"description\", \"kpis\"]):\n",
        "             errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'description' and 'kpis' required in payload.\"})\n",
        "             status = \"bad_request\"\n",
        "        else:\n",
        "            status = \"pending\"; response_payload = None\n",
        "            try:\n",
        "                # --- TODO: Implement persistence and trigger planning workflow ---\n",
        "                # 1. Generate goal_id\n",
        "                # 2. Construct goal_data dict similar to _generate_and_trigger_goal\n",
        "                # 3. Persist goal_data\n",
        "                # 4. Trigger planning workflow via self.workflow_client.start_workflow\n",
        "                # 5. Update persisted goal with execution_id\n",
        "                # --- Placeholder ---\n",
        "                goal_id = f\"goal_manual_{uuid.uuid4().hex[:6]}\"\n",
        "                logger.info(f\"Placeholder: Adding manual goal {goal_id}. Needs persistence and workflow trigger.\")\n",
        "                # Simulate triggering workflow\n",
        "                await asyncio.sleep(0.1)\n",
        "                execution_id = f\"projects/p/locations/l/workflows/w/executions/exec_manual_{goal_id}\"\n",
        "                # --- End Placeholder ---\n",
        "                status = \"success\"; response_payload = {\"goal_id\": goal_id, \"planning_execution_id\": execution_id}\n",
        "\n",
        "            except Exception as e:\n",
        "                status = \"internal_error\"; errors.append({\"code\": \"ADD_GOAL_ERROR\", \"message\": str(e)})\n",
        "                logger.error(f\"Error adding manual goal: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(request, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "# --- Self-Correcting Feedback Tool (Reworked Async) ---\n",
        "class SelfCorrectingFeedbackTool:\n",
        "    \"\"\" Processes feedback async, triggers LI Tool API proxy. Deployed as a service. \"\"\"\n",
        "    def __init__(self, kg_tool_proxy: Any, learning_integrator_tool_proxy: Any, config: EnhancedConfig):\n",
        "        # Inject dependencies\n",
        "        if not config or not kg_tool_proxy or not learning_integrator_tool_proxy:\n",
        "            raise InitializationError(\"SelfCorrectingFeedbackTool requires config and proxies for KG and LI tools.\")\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.learning_integrator_tool = learning_integrator_tool_proxy\n",
        "        self.config = config\n",
        "        # TODO: Replace deque with persistent storage for production\n",
        "        self.feedback_history = deque(maxlen=5000)\n",
        "        self.correction_rules = defaultdict(dict) # Rules based on entity type and feedback content\n",
        "        self.logger = logging.getLogger('MIZ-OKI.SelfCorrectingFeedbackTool')\n",
        "        self._load_rules() # Sync load ok\n",
        "        self.logger.info(\"Self-Correcting Feedback Tool logic initialized (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"SelfCorrectingFeedbackTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    def _load_rules(self):\n",
        "        \"\"\"Loads correction rules (e.g., from config file, DB).\"\"\"\n",
        "        # --- TODO: Load rules from a persistent/configurable source ---\n",
        "        # Example rules:\n",
        "        self.correction_rules['ModelPrediction'] = [\n",
        "            {\"condition\": \"feedback_sentiment == 'negative' and confidence < 0.5\", \"action\": \"trigger_learning\", \"params\": {\"knowledge_type\": \"model_correction_low_conf\", \"importance\": 0.6}},\n",
        "            {\"condition\": \"feedback_contains('incorrect_entity')\", \"action\": \"update_kg\", \"params\": {\"label\": \"IncorrectPrediction\"}},\n",
        "            {\"condition\": \"feedback_rating <= 2\", \"action\": \"trigger_learning\", \"params\": {\"knowledge_type\": \"model_correction_low_rating\", \"importance\": 0.8}}\n",
        "        ]\n",
        "        self.correction_rules['KnowledgeGraphEntity'] = [\n",
        "             {\"condition\": \"feedback_type == 'data_error'\", \"action\": \"update_kg\", \"params\": {\"label\": \"DataQualityIssue\"}},\n",
        "        ]\n",
        "        # --- End TODO ---\n",
        "        self.logger.info(f\"Loaded {sum(len(v) for v in self.correction_rules.values())} correction rules.\")\n",
        "\n",
        "    async def process_feedback(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Processes feedback async. Expects/Returns MIZ OKI payload. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); entity_id = payload.get(\"entity_id\"); feedback_data = payload.get(\"feedback_data\"); source = payload.get(\"source\")\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # Prepare MIZ OKI response\n",
        "        response = self._create_miz_oki_response(input_data, \"pending\")\n",
        "        feedback_id = f\"fb_{entity_id or 'general'}_{uuid.uuid4().hex[:8]}\"\n",
        "        log_entry = { # Detailed log\n",
        "            \"feedback_id\": feedback_id, \"trace_id\": trace_id,\n",
        "            \"timestamp\": response[\"timestamp\"], \"entity_id\": entity_id, \"source\": source,\n",
        "            \"input_preview\": str(feedback_data)[:250], # Limit preview size\n",
        "            \"status\": \"pending\", \"entity_type\": None, \"corrections_identified\": [], \"triggered_li_integrations\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Basic validation\n",
        "            if not feedback_data: raise ValueError(\"Missing 'feedback_data' in payload.\")\n",
        "            if not self.learning_integrator_tool: raise InitializationError(\"Learning Integrator Tool proxy unavailable.\")\n",
        "\n",
        "            entity_type = \"unknown\"\n",
        "            # 1. Get Entity Context (Optional, if entity_id provided)\n",
        "            if entity_id and self.kg_tool:\n",
        "                try:\n",
        "                    kg_request = {\"payload\": {\"mizId\": entity_id}, \"trace_id\": trace_id, \"request_id\": f\"kg_get_fb_entity_{feedback_id}\"}\n",
        "                    kg_response = await self.kg_tool.get_entity_endpoint(request=kg_request) # Call KG API proxy\n",
        "                    if kg_response.get(\"status\") == \"success\":\n",
        "                        entity = kg_response.get(\"payload\", {}).get(\"entity_data\", {})\n",
        "                        entity_type = entity.get(\"entity_type\", \"unknown\")\n",
        "                        log_entry[\"entity_type\"] = entity_type\n",
        "                        logger.debug(f\"SCF {feedback_id}: Fetched context for entity {entity_id} (Type: {entity_type}).\")\n",
        "                    else:\n",
        "                        logger.warning(f\"SCF {feedback_id}: Could not fetch entity {entity_id} from KG: {kg_response.get('error_details')}\")\n",
        "                except Exception as kg_e:\n",
        "                     logger.error(f\"SCF {feedback_id}: Error fetching entity context from KG: {kg_e}\")\n",
        "                     # Continue without entity context if KG fails\n",
        "\n",
        "            # 2. Determine Correction Actions based on Rules\n",
        "            correction_actions = []\n",
        "            rules_for_type = self.correction_rules.get(entity_type, []) + self.correction_rules.get(\"default\", []) # Include default rules\n",
        "            if rules_for_type:\n",
        "                logger.debug(f\"SCF {feedback_id}: Applying {len(rules_for_type)} rules for type '{entity_type}'.\")\n",
        "                for rule in rules_for_type:\n",
        "                    condition = rule.get(\"condition\")\n",
        "                    # --- TODO: Implement condition evaluation logic ---\n",
        "                    # This needs a safe way to evaluate conditions against feedback_data and entity context\n",
        "                    # Example: evaluate_condition(condition, {\"feedback\": feedback_data, \"entity\": entity})\n",
        "                    condition_met = True # Placeholder\n",
        "                    # --- End TODO ---\n",
        "                    if condition_met:\n",
        "                        correction_actions.append(rule) # Add rule (contains action type and params)\n",
        "                        log_entry[\"corrections_identified\"].append({\"rule_condition\": condition, \"action\": rule.get(\"action\")})\n",
        "            else:\n",
        "                logger.info(f\"SCF {feedback_id}: No specific correction rules found for type '{entity_type}'. Default actions might apply.\")\n",
        "                # Optionally define default actions, e.g., always trigger learning\n",
        "                correction_actions.append({\"action\": \"trigger_learning\", \"params\": {\"knowledge_type\": \"general_feedback\", \"importance\": 0.5}})\n",
        "                log_entry[\"corrections_identified\"].append({\"rule_condition\": \"default\", \"action\": \"trigger_learning\"})\n",
        "\n",
        "\n",
        "            # 3. Trigger Learning Integrator Tool API Proxy for determined actions\n",
        "            if correction_actions:\n",
        "                li_tasks = []\n",
        "                for action_rule in correction_actions:\n",
        "                    if action_rule.get(\"action\") == \"trigger_learning\":\n",
        "                        li_payload = {\n",
        "                            \"knowledge_type\": action_rule.get(\"params\", {}).get(\"knowledge_type\", \"correction\"),\n",
        "                            \"knowledge_data\": {\"feedback\": feedback_data, \"entity_id\": entity_id, \"entity_type\": entity_type}, # Pass relevant data\n",
        "                            \"source\": f\"scf:{feedback_id}\",\n",
        "                            \"importance\": action_rule.get(\"params\", {}).get(\"importance\", 0.8)\n",
        "                        }\n",
        "                        li_request = {\n",
        "                            \"payload\": li_payload,\n",
        "                            \"trace_id\": trace_id, \"request_id\": f\"li_trigger_{feedback_id}_{uuid.uuid4().hex[:4]}\"\n",
        "                        }\n",
        "                        li_tasks.append(self.learning_integrator_tool.integrate_learning(input_data=li_request)) # Call LI API proxy\n",
        "                    elif action_rule.get(\"action\") == \"update_kg\":\n",
        "                        # --- TODO: Implement KG Update via KG Tool API Proxy ---\n",
        "                        logger.warning(f\"SCF {feedback_id}: KG Update action not yet implemented.\")\n",
        "                        # --- End TODO ---\n",
        "                    # Add other action types if needed\n",
        "\n",
        "                if li_tasks:\n",
        "                      li_results = await asyncio.gather(*li_tasks, return_exceptions=True)\n",
        "                      # Parse MIZ OKI responses from LI Tool\n",
        "                      log_entry[\"triggered_li_integrations\"] = []\n",
        "                      for res in li_results:\n",
        "                          if isinstance(res, dict) and res.get(\"status\") == \"success\":\n",
        "                              log_entry[\"triggered_li_integrations\"].append(res.get(\"payload\", {}))\n",
        "                          elif isinstance(res, dict): # LI Tool reported an error\n",
        "                              log_entry[\"triggered_li_integrations\"].append({\"error\": res.get(\"error_details\")})\n",
        "                              logger.error(f\"SCF {feedback_id}: LI Tool API proxy call failed: {res.get('error_details')}\")\n",
        "                          else: # Exception during call\n",
        "                              log_entry[\"triggered_li_integrations\"].append({\"error\": str(res)})\n",
        "                              logger.error(f\"SCF {feedback_id}: Exception calling LI Tool API proxy: {res}\")\n",
        "\n",
        "            response[\"status\"] = \"success\"\n",
        "            log_entry[\"status\"] = \"processed\"\n",
        "            response[\"payload\"] = log_entry # Return the detailed log\n",
        "\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Error processing feedback async via SCF Tool for {entity_id}: {e}\", exc_info=True)\n",
        "             response[\"status\"] = \"internal_error\"; errors.append({\"code\": \"SCF_ERROR\", \"message\": str(e)})\n",
        "             log_entry[\"status\"] = \"failed\"; log_entry[\"error\"] = str(e)\n",
        "             response[\"payload\"] = log_entry # Return log even on failure\n",
        "\n",
        "        log_entry[\"total_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        self.feedback_history.append(log_entry) # Add to in-memory history\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = log_entry[\"total_duration_ms\"]\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        # --- TODO: Persist log_entry async via KG Tool API or logging service ---\n",
        "        return response\n",
        "\n",
        "# --- Initialization (Conceptual - Tools instantiated by orchestrator/framework) ---\n",
        "# _ethical_guardrails_tool: Optional[EthicalGuardrailsTool] = None\n",
        "# _hde_tool: Optional[HybridDecisionEngineTool] = None\n",
        "# _optimizer_tool: Optional[HolisticOptimizerTool] = None\n",
        "# _li_tool: Optional[LearningIntegrationTool] = None # This is the one being called by SCF\n",
        "# _agg_tool: Optional[AutonomousGoalGeneratorTool] = None\n",
        "# _scf_tool: Optional[SelfCorrectingFeedbackTool] = None\n",
        "\n",
        "# async def initialize_core_processes():\n",
        "#      global _ethical_guardrails_tool, _hde_tool, _optimizer_tool, _li_tool, _agg_tool, _scf_tool\n",
        "#      if not _config_obj or not _real_dependencies:\n",
        "#          logger.critical(\"Cannot initialize Core Processes: Config or dependencies missing.\")\n",
        "#          return\n",
        "#      try:\n",
        "#          _ethical_guardrails_tool = EthicalGuardrailsTool(_config_obj) # Sync init ok\n",
        "#          _hde_tool = HybridDecisionEngineTool(_config_obj, _kg_tool_proxy, _moe_registry_proxy, _expert_invoker_proxy, _ethical_guardrails_tool, _fm_client_proxy)\n",
        "#          _optimizer_tool = HolisticOptimizerTool(_config_obj, _kg_tool_proxy, _hde_tool, _moe_registry_proxy, _expert_invoker_proxy) # Pass HDE proxy\n",
        "#          _li_tool = LearningIntegrationTool(_kg_tool_proxy, _moe_registry_proxy, _expert_invoker_proxy, _pubsub_client_proxy, _config_obj)\n",
        "#          _agg_tool = AutonomousGoalGeneratorTool(_optimizer_tool, _workflow_client_proxy, _config_obj) # Pass Optimizer proxy, REAL workflow client proxy\n",
        "#          _scf_tool = SelfCorrectingFeedbackTool(_kg_tool_proxy, _li_tool, _config_obj) # Pass KG and LI proxies\n",
        "#          logger.info(\"Core Process Tools initialized.\")\n",
        "#      except Exception as e:\n",
        "#           logger.critical(f\"Core Process Tools initialization failed: {e}\", exc_info=True)\n",
        "#           # Set all to None on failure\n",
        "#           _ethical_guardrails_tool = _hde_tool = _optimizer_tool = _li_tool = _agg_tool = _scf_tool = None\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 Core Processes Layer Logic (Cell 5 - Reworked) ---\")\n",
        "print(\"Tool logic uses real dependencies/proxies via MIZ OKI APIs/Events.\")\n",
        "print(\"Handles MIZ OKI payloads for API interaction. Async implementation.\")\n",
        "print(\"Requires implementation of Causal/Sim tools, config/rule loading, persistence.\")\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "rgcXdoJNeN3P",
        "outputId": "1c76f045-5423-45ec-8d3d-6d0bcd0942e0"
      },
      "id": "rgcXdoJNeN3P",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-9-005ee98b627a>, line 49)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-005ee98b627a>\"\u001b[0;36m, line \u001b[0;32m49\u001b[0m\n\u001b[0;31m    class MockKGTool: async def get_entity(self, request): return {\"status\": \"success\", \"payload\": {\"entity_data\": {\"entity_type\": \"mock\"}}}; async def execute_query(self, request): return {\"status\": \"success\", \"payload\": {\"results\": []}}; async def save_decision_record(self, request): return {\"status\": \"success\"}\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Technical Flow Components (Reworked)\n",
        "# Status: RAG, R2, NN Tools refactored as async services handling MIZ OKI payloads.\n",
        "#         Uses real dependencies/proxies via MIZ OKI APIs.\n",
        "#         RL/MoE base classes unchanged (logic used externally).\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple, Callable\n",
        "from collections import deque, defaultdict # Added defaultdict\n",
        "import asyncio\n",
        "import re # For R2 template parsing\n",
        "\n",
        "# --- Assume Real Tool/Client Dependencies are Injected/Available ---\n",
        "# These proxies represent API clients for other deployed MIZ OKI services.\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    # Proxies for other MIZ OKI Tool APIs\n",
        "    if 'kg_tool_service_instance' not in globals(): raise NameError(\"kg_tool_service_instance proxy not found\") # Cell 3 Proxy\n",
        "    if 'foundation_model_client' not in globals(): raise NameError(\"foundation_model_client proxy not found\") # Cell 18 Proxy\n",
        "    if 'hde_tool' not in globals(): raise NameError(\"hde_tool proxy not found\") # Cell 5 Proxy\n",
        "\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _kg_tool_proxy = kg_tool_service_instance\n",
        "    _fm_client_proxy = foundation_model_client\n",
        "    _hde_tool_proxy = hde_tool # Inject HDE tool proxy for R2\n",
        "\n",
        "    # NN Tool delegates to FM Client\n",
        "    class NeuralProcessingToolDelegate:\n",
        "        \"\"\"Simple delegate class for NN embedding tasks using FM Client.\"\"\"\n",
        "        def __init__(self, fm_client_proxy: Any):\n",
        "            self.fm_client = fm_client_proxy\n",
        "            self.logger = logging.getLogger('MIZ-OKI.NNToolDelegate')\n",
        "            if not self.fm_client: self.logger.error(\"NNToolDelegate initialized without FM Client proxy!\")\n",
        "\n",
        "        async def get_embedding(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            \"\"\"Forward embedding request to FM Client Tool.\"\"\"\n",
        "            if not self.fm_client: return {\"status\": \"error\", \"error_details\": [{\"message\": \"FM Client unavailable\"}]}\n",
        "            # Assume input_data is already a valid MIZ OKI request for the FM Client\n",
        "            return await self.fm_client.generate_embedding(input_data=input_data)\n",
        "\n",
        "        async def batch_embed(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            \"\"\"Forward batch embedding request to FM Client Tool.\"\"\"\n",
        "            if not self.fm_client: return {\"status\": \"error\", \"error_details\": [{\"message\": \"FM Client unavailable\"}]}\n",
        "            # FM client's generate_embedding handles list input for batching\n",
        "            return await self.fm_client.generate_embedding(input_data=input_data)\n",
        "\n",
        "    _nn_tool_proxy = NeuralProcessingToolDelegate(_fm_client_proxy)\n",
        "\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real/conceptual dependencies in Cell 6 (Reworked).\")\n",
        "\n",
        "except NameError as e:\n",
        "    logger.warning(f\"Dependency Error in Cell 6 ({e}). Using Mocks/Placeholders.\")\n",
        "    _real_dependencies = False\n",
        "    # --- Mock/Placeholder Setup ---\n",
        "    class MockKGTool: async def get_entity(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"entity_data\": {\"name\": \"Mock Entity\"}}}; async def get_neighbors(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"neighbors\": []}}; async def search_vector_index(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"results\": [(\"mock_id\", 0.9, {})]}}; async def execute_query(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"results\": []}}\n",
        "    class MockFMClientTool: async def generate_text(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"generated_text\": \"Mock LLM\"}}; async def generate_embedding(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"embedding\": [[0.1]*10] if isinstance(input_data['payload']['data'], list) else [0.1]*10}}\n",
        "    class MockHdeTool: async def make_decision(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"decision_id\": \"mock_dec\"}}\n",
        "    class MockNNTool: async def get_embedding(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"embedding\": [0.1]*10}}; async def batch_embed(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"embeddings\": [[0.1]*10]*len(input_data['payload']['data'])}}\n",
        "    # Define minimal config if needed\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ:\n",
        "        from dataclasses import dataclass, field\n",
        "        @dataclass class MockGcpConfig: project_id:Optional[str]=\"mock-proj\"; region:str=\"mock-region\"\n",
        "        @dataclass class MockKgConfig: vector_index_name: str = \"mock_index\"\n",
        "        @dataclass class MockFmDefaults: llama4_embedding_model: str = \"mock-emb\"; llama4_scout: str = \"mock-scout\"; llama4_maverick: str = \"mock-mav\"\n",
        "        @dataclass class MockFmConfig: defaults: MockFmDefaults = field(default_factory=MockFmDefaults)\n",
        "        @dataclass class MockConfig: gcp: MockGcpConfig = field(default_factory=MockGcpConfig); kg: MockKgConfig = field(default_factory=MockKgConfig); foundation_models: MockFmConfig = field(default_factory=MockFmConfig); miz_oki_schema_version: str = \"3.0\"; def get_model_info(self, alias): return {\"provider\": \"mock\", \"model_id\": alias, \"pricing\": {\"prompt\": 0.1, \"completion\": 0.2}}; def get(self, key, default=None): parts=key.split('.'); val=self; try: [val := getattr(val, p) for p in parts]; return val; except: return default\n",
        "        _config_obj = MockConfig()\n",
        "    _kg_tool_proxy = MockKGTool(); _fm_client_proxy = MockFMClientTool(); _hde_tool_proxy = MockHdeTool(); _nn_tool_proxy = MockNNTool()\n",
        "    # --- End Mock/Placeholder Setup ---\n",
        "\n",
        "logger = logging.getLogger('MIZ-OKI.TechnicalFlows')\n",
        "\n",
        "# --- Semantic Graph RAG (Reworked Async Tool Logic) ---\n",
        "class SemanticGraphRAGTool:\n",
        "    \"\"\" Implements Graph-enhanced RAG logic. Deployed as a service callable via MIZ OKI API. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any, fm_client_proxy: Any, nn_tool_proxy: Any):\n",
        "        if not all([config, kg_tool_proxy, fm_client_proxy, nn_tool_proxy]):\n",
        "            raise InitializationError(\"SemanticGraphRAGTool requires config, KG, FM, and NN tool proxies.\")\n",
        "        self.config = config\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.fm_client = fm_client_proxy\n",
        "        self.nn_tool = nn_tool_proxy # Used for embeddings via FM Client delegate\n",
        "        self.default_vector_index = config.kg.vector_index_name\n",
        "        self.default_embedding_alias = config.foundation_models.defaults.llama4_embedding_model\n",
        "        self.logger = logging.getLogger('MIZ-OKI.SemanticGraphRAGTool')\n",
        "        self.logger.info(\"Semantic Graph RAG Tool logic initialized (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"SemanticGraphRAGTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    async def _get_embedding(self, text: str, model_alias: Optional[str] = None, trace_id: Optional[str] = None) -> Optional[List[float]]:\n",
        "        \"\"\" Helper to get embedding via NN/FM Client Tool API proxy. \"\"\"\n",
        "        if not self.nn_tool:\n",
        "            self.logger.error(\"NN/FM Tool proxy unavailable for embedding generation.\")\n",
        "            return None\n",
        "        target_model_alias = model_alias or self.default_embedding_alias\n",
        "        try:\n",
        "            # Prepare MIZ OKI request for NN/FM Tool\n",
        "            nn_request = {\n",
        "                \"payload\": {\"data\": text, \"model_alias\": target_model_alias},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"rag_embed_{uuid.uuid4().hex[:6]}\"\n",
        "            }\n",
        "            nn_response = await self.nn_tool.get_embedding(input_data=nn_request) # Call API proxy\n",
        "\n",
        "            if nn_response.get(\"status\") == \"success\":\n",
        "                embedding = nn_response.get(\"payload\", {}).get(\"embedding\")\n",
        "                # Handle both single and potential batch returns defensively\n",
        "                if isinstance(embedding, list) and len(embedding) > 0 and isinstance(embedding[0], float):\n",
        "                    return embedding # Single embedding returned as list of floats\n",
        "                elif isinstance(embedding, list) and len(embedding) > 0 and isinstance(embedding[0], list) and isinstance(embedding[0][0], float):\n",
        "                    return embedding[0] # First embedding from a batch result\n",
        "                else:\n",
        "                    logger.error(f\"Unexpected embedding format from NN/FM proxy: {type(embedding)}\")\n",
        "                    return None\n",
        "            else:\n",
        "                logger.error(f\"NN/FM Tool API proxy failed for embedding: {nn_response.get('error_details')}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to get embedding via NN/FM client proxy: {e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    async def retrieve_nodes_semantic(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Retrieve relevant nodes via KG Tool API proxy. Expects/Returns MIZ OKI payload structure. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); query = payload.get(\"query\"); k = payload.get(\"k\", 5); namespace = payload.get(\"namespace\"); filter_dict = payload.get(\"filter_dict\"); vector_index_name = payload.get(\"vector_index_name\")\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not query: errors.append({\"code\": \"MISSING_QUERY\", \"message\": \"Query parameter is required.\"})\n",
        "        if not self.kg_tool: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"KG Tool proxy unavailable.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        target_index = vector_index_name or self.default_vector_index\n",
        "        logger.debug(f\"RAG: Retrieving nodes semantically. Query: '{query[:50]}...', Index: {target_index}, K={k}, Filter={filter_dict}\")\n",
        "        ids_scores_meta = []\n",
        "        status = \"pending\"\n",
        "        response_payload = None\n",
        "\n",
        "        try:\n",
        "            query_embedding = await self._get_embedding(query, trace_id=trace_id)\n",
        "            if query_embedding is None: raise ValueError(\"Failed to generate query embedding via NN/FM proxy.\")\n",
        "\n",
        "            # Call KG Tool API proxy (search_vector_index endpoint)\n",
        "            kg_request = {\n",
        "                \"payload\": {\"query_vector\": query_embedding, \"k\": k, \"namespace\": namespace, \"filter_dict\": filter_dict, \"index_name\": target_index},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"kg_vec_search_{request_id}\"\n",
        "            }\n",
        "            kg_response = await self.kg_tool.search_vector_endpoint(request=kg_request) # Call proxy method (ensure method name matches Cell 3 service)\n",
        "\n",
        "            if kg_response.get(\"status\") == \"success\":\n",
        "                vector_results = kg_response.get(\"payload\", {}).get(\"results\", [])\n",
        "                # Ensure results are in the expected format (id, score, metadata_dict)\n",
        "                ids_scores_meta = [(vid, score, meta) for vid, score, meta in vector_results if vid and isinstance(score, (float, int))]\n",
        "                status = \"success\"\n",
        "                response_payload = {\"retrieved_nodes\": ids_scores_meta[:k]} # Ensure K results\n",
        "                logger.info(f\"RAG: Retrieved {len(ids_scores_meta)} nodes via KG Tool API proxy.\")\n",
        "            else:\n",
        "                raise RuntimeError(f\"KG Tool API search failed: {kg_response.get('error_details')}\")\n",
        "\n",
        "        except (ValueError, RuntimeError) as vr_e:\n",
        "             status = \"failed\"; errors.append({\"code\": \"RETRIEVAL_FAILED\", \"message\": str(vr_e)}); logger.warning(f\"RAG: Semantic retrieval failed: {vr_e}\")\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": str(e)}); logger.error(f\"RAG: Semantic retrieval failed unexpectedly: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    async def retrieve_and_augment(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Retrieves and augments nodes via KG Tool API proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); query = payload.get(\"query\"); k = payload.get(\"k\", 5); namespace = payload.get(\"namespace\"); filter_dict = payload.get(\"filter_dict\"); include_neighbors = payload.get(\"include_neighbors\", True); neighbor_limit = payload.get(\"neighbor_limit\", 3); vector_index_name = payload.get(\"vector_index_name\")\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not query: errors.append({\"code\": \"MISSING_QUERY\", \"message\": \"Query parameter is required.\"})\n",
        "        if not self.kg_tool: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"KG Tool proxy unavailable.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        logger.info(f\"RAG: Retrieving and augmenting nodes for query: '{query[:50]}...'\")\n",
        "        final_results = []; status = \"pending\"; response_payload = None\n",
        "\n",
        "        try:\n",
        "            # 1. Retrieve initial nodes via internal call (which uses KG API proxy)\n",
        "            retrieval_request = {\n",
        "                \"payload\": {\"query\": query, \"k\": k, \"namespace\": namespace, \"filter_dict\": filter_dict, \"vector_index_name\": vector_index_name},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"rag_retrieve_{request_id}\" # Link requests\n",
        "            }\n",
        "            retrieval_response = await self.retrieve_nodes_semantic(retrieval_request)\n",
        "\n",
        "            if retrieval_response.get(\"status\") != \"success\":\n",
        "                raise RuntimeError(f\"Semantic retrieval step failed: {retrieval_response.get('error_details')}\")\n",
        "\n",
        "            top_nodes_with_meta = retrieval_response.get(\"payload\", {}).get(\"retrieved_nodes\", [])\n",
        "            if not top_nodes_with_meta:\n",
        "                status = \"success_no_results\"; response_payload = {\"augmented_context\": []}\n",
        "                response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "                response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "                return response\n",
        "\n",
        "            # 2. Augment concurrently using KG Tool API proxy's get_entity and get_neighbors\n",
        "            async def augment_single(node_id: str, score: float, node_meta: Dict) -> Optional[Dict]:\n",
        "                try:\n",
        "                     # Prepare MIZ OKI requests for KG Tool API proxy\n",
        "                     entity_req = {\"payload\": {\"mizId\": node_id}, \"trace_id\": trace_id, \"request_id\": f\"kg_get_entity_{node_id}\"}\n",
        "                     # Construct neighbor request payload based on KG Tool API spec\n",
        "                     neighbor_payload = {\"mizId\": node_id, \"limit\": neighbor_limit}\n",
        "                     if include_neighbors:\n",
        "                         neighbor_req = {\"payload\": neighbor_payload, \"trace_id\": trace_id, \"request_id\": f\"kg_get_neighbors_{node_id}\"}\n",
        "                         neighbors_resp_task = self.kg_tool.get_neighbors_endpoint(request=neighbor_req) # Call API proxy method\n",
        "                     else:\n",
        "                         neighbors_resp_task = asyncio.sleep(0, result={\"status\":\"success\", \"payload\": {\"neighbors\":[]}}) # No-op if neighbors not needed\n",
        "\n",
        "                     entity_resp_task = self.kg_tool.get_entity_endpoint(request=entity_req) # Call API proxy method\n",
        "\n",
        "                     entity_resp, neighbors_resp = await asyncio.gather(entity_resp_task, neighbors_resp_task, return_exceptions=True)\n",
        "\n",
        "                     node_data = None; neighbors_data = []\n",
        "                     # Process entity response\n",
        "                     if isinstance(entity_resp, dict) and entity_resp.get(\"status\") == \"success\":\n",
        "                         node_data = entity_resp.get(\"payload\",{}).get(\"entity_data\")\n",
        "                     elif isinstance(entity_resp, Exception):\n",
        "                          logger.warning(f\"RAG Augment: KG get_entity API call failed for {node_id}: {entity_resp}\")\n",
        "                     else: logger.warning(f\"RAG Augment: KG get_entity API proxy returned unexpected status for {node_id}: {entity_resp.get('status')}\")\n",
        "\n",
        "                     # Process neighbors response\n",
        "                     if isinstance(neighbors_resp, dict) and neighbors_resp.get(\"status\") == \"success\":\n",
        "                         neighbors_data = neighbors_resp.get(\"payload\",{}).get(\"neighbors\", [])\n",
        "                     elif isinstance(neighbors_resp, Exception):\n",
        "                          logger.warning(f\"RAG Augment: KG get_neighbors API call failed for {node_id}: {neighbors_resp}\")\n",
        "                     else: logger.warning(f\"RAG Augment: KG get_neighbors API proxy returned unexpected status for {node_id}: {neighbors_resp.get('status')}\")\n",
        "\n",
        "                     if node_data is None:\n",
        "                         logger.warning(f\"RAG Augment: Skipping node {node_id} due to failed entity data retrieval.\")\n",
        "                         return None # Skip if core node data failed to retrieve\n",
        "\n",
        "                     # Format the result item\n",
        "                     result_item = {\n",
        "                         \"node_id\": node_id,\n",
        "                         \"data\": node_data, # Contains full properties from get_entity\n",
        "                         \"score\": score, # Similarity score from vector search\n",
        "                         \"vector_metadata\": node_meta, # Metadata returned by vector search (if any)\n",
        "                         \"neighbors\": []\n",
        "                     }\n",
        "                     for neighbor in neighbors_data:\n",
        "                          props = neighbor.get(\"neighborProps\", {})\n",
        "                          result_item[\"neighbors\"].append({\n",
        "                              \"id\": neighbor.get(\"neighborId\"),\n",
        "                              \"type\": props.get(\"entity_type\"), # Assuming entity_type is stored\n",
        "                              \"relationship\": neighbor.get(\"relationshipType\"),\n",
        "                              \"rel_props\": neighbor.get(\"relationshipProps\", {}),\n",
        "                              \"preview\": props.get('name', props.get('title', neighbor.get(\"neighborId\"))) # Simple preview\n",
        "                          })\n",
        "                     return result_item\n",
        "                except Exception as augment_e:\n",
        "                    logger.error(f\"RAG Augment: Error processing node {node_id}: {augment_e}\", exc_info=False)\n",
        "                    return None\n",
        "\n",
        "            augment_tasks = [augment_single(nid, score, meta) for nid, score, meta in top_nodes_with_meta]\n",
        "            augmented_results = await asyncio.gather(*augment_tasks)\n",
        "            final_results = [res for res in augmented_results if res is not None]\n",
        "\n",
        "            status = \"success\"\n",
        "            response_payload = {\"augmented_context\": final_results}\n",
        "            logger.info(f\"RAG: Successfully augmented {len(final_results)} nodes via KG Tool API proxy.\")\n",
        "\n",
        "        except (ValueError, RuntimeError) as vr_e:\n",
        "             status = \"failed\"; errors.append({\"code\": \"AUGMENT_FAILED\", \"message\": str(vr_e)}); logger.warning(f\"RAG: Augmentation failed: {vr_e}\")\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": str(e)}); logger.error(f\"RAG: Augmentation failed unexpectedly: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    async def generate_response(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Generates response via FM Client API proxy, grounded in context. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); query = payload.get(\"query\"); retrieved_context = payload.get(\"retrieved_context\", []); model_alias = payload.get(\"model_alias\"); max_tokens = payload.get(\"max_tokens\", 512); temperature = payload.get(\"temperature\", 0.2)\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not query: errors.append({\"code\": \"MISSING_QUERY\", \"message\": \"Query parameter is required.\"})\n",
        "        if not self.fm_client: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"FM Client Tool proxy unavailable.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        target_model_alias = model_alias or self.config.foundation_models.defaults.llama4_scout # Use scout for generation if not specified\n",
        "        status = \"pending\"; response_payload = None; response_metadata = {}\n",
        "\n",
        "        # Format context string concisely\n",
        "        context_str = \"No relevant context found.\"\n",
        "        if retrieved_context and isinstance(retrieved_context, list):\n",
        "            context_parts = []\n",
        "            for item in retrieved_context[:5]: # Limit context length for prompt\n",
        "                if not isinstance(item, dict): continue\n",
        "                node_data = item.get('data', {})\n",
        "                preview = node_data.get('name', node_data.get('title', item.get('node_id', 'Unknown Node')))\n",
        "                # Simple text representation\n",
        "                context_parts.append(f\"- Node {item.get('node_id')}: {preview} (Score: {item.get('score', 0):.2f})\")\n",
        "                # Optionally add concise neighbor info\n",
        "                # neighbors_preview = [f\"{n.get('relationship')} -> {n.get('preview')}\" for n in item.get('neighbors', [])[:2]]\n",
        "                # if neighbors_preview: context_parts.append(f\"  Neighbors: {'; '.join(neighbors_preview)}\")\n",
        "            if context_parts:\n",
        "                context_str = \"Relevant Context:\\n\" + \"\\n\".join(context_parts)\n",
        "            if len(retrieved_context) > 5:\n",
        "                 context_str += f\"\\n... (and {len(retrieved_context) - 5} more context items)\"\n",
        "\n",
        "        # Construct prompt\n",
        "        prompt = f\"Based on the following context, answer the query.\\n\\nContext:\\n---\\n{context_str}\\n---\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
        "        logger.debug(f\"RAG Generate Prompt (first 300 chars): {prompt[:300]}...\")\n",
        "\n",
        "        try:\n",
        "            # Prepare MIZ OKI request for FM Client Tool API proxy\n",
        "            fm_request = {\n",
        "                \"payload\": {\n",
        "                    \"prompt\": prompt,\n",
        "                    \"model_alias\": target_model_alias,\n",
        "                    \"max_tokens\": max_tokens,\n",
        "                    \"temperature\": temperature\n",
        "                },\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"fm_rag_gen_{request_id}\"\n",
        "            }\n",
        "            fm_response = await self.fm_client.generate_text(input_data=fm_request) # Call API proxy\n",
        "\n",
        "            if fm_response.get(\"status\") == \"success\":\n",
        "                generated_text = fm_response.get(\"payload\", {}).get(\"generated_text\")\n",
        "                if generated_text:\n",
        "                    status = \"success\"; response_payload = {\"generated_response\": generated_text.strip()}\n",
        "                    # Include metadata from FM call if available\n",
        "                    response_metadata = fm_response.get(\"metadata\", {})\n",
        "                else:\n",
        "                    status = \"failed\"; errors.append({\"code\": \"GENERATION_EMPTY\", \"message\": \"FM Client API proxy returned empty response.\"})\n",
        "            else:\n",
        "                raise RuntimeError(f\"FM Client API proxy failed: {fm_response.get('error_details')}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"GENERATION_ERROR\", \"message\": str(e)}); logger.error(f\"RAG: LLM generation via FM Client API proxy failed: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        # Add processing time, potentially merging with FM metadata if it exists\n",
        "        proc_duration = (time.monotonic() - start_time) * 1000\n",
        "        response[\"metadata\"] = response_metadata # Start with FM metadata\n",
        "        response[\"metadata\"][\"rag_tool_processing_duration_ms\"] = proc_duration\n",
        "        return response\n",
        "\n",
        "# --- Context-Adaptive RL Base Class (Remains Sync Logic - Used by External ADK Agents) ---\n",
        "class ContextAdaptiveRLBase:\n",
        "    \"\"\" Base class for RL algorithms. Logic used by external ADK agents. \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.epsilon = epsilon # Exploration rate\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.model = self._build_network() # Requires implementation in subclass\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def _build_network(self):\n",
        "        \"\"\"Builds the neural network model (e.g., using TensorFlow/Keras).\"\"\"\n",
        "        # Placeholder - Subclasses must implement this based on the chosen RL algorithm (DQN, PPO, etc.)\n",
        "        # Example using TF/Keras (requires tensorflow import):\n",
        "        # try:\n",
        "        #     import tensorflow as tf\n",
        "        #     model = tf.keras.models.Sequential([\n",
        "        #         tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_dim,)),\n",
        "        #         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        #         tf.keras.layers.Dense(self.action_dim, activation='linear') # Output Q-values or policy probabilities\n",
        "        #     ])\n",
        "        #     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse') # Adjust loss based on algorithm\n",
        "        #     return model\n",
        "        # except ImportError:\n",
        "        #      self.logger.error(\"TensorFlow not installed. Cannot build RL network.\")\n",
        "        #      return None\n",
        "        raise NotImplementedError(\"Subclasses must implement _build_network.\")\n",
        "\n",
        "    def get_action(self, state, use_exploration=True):\n",
        "        \"\"\"Selects an action based on the current state (epsilon-greedy).\"\"\"\n",
        "        if self.model is None: raise RuntimeError(\"RL model not built.\")\n",
        "        if use_exploration and np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_dim) # Explore\n",
        "        else:\n",
        "            # Ensure state is correctly shaped for the model (e.g., batch dimension)\n",
        "            try:\n",
        "                state_input = np.reshape(state, [1, self.state_dim])\n",
        "                act_values = self.model.predict(state_input, verbose=0) # Exploit - Get Q-values or policy\n",
        "                return np.argmax(act_values[0]) # Choose best action\n",
        "            except Exception as e:\n",
        "                 self.logger.error(f\"Error during action prediction: {e}\")\n",
        "                 return random.randrange(self.action_dim) # Fallback to random action on error\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Stores experience in memory (usually managed by the agent/buffer).\"\"\"\n",
        "        # This base class doesn't manage memory; the agent using it does.\n",
        "        pass\n",
        "\n",
        "    def train_batch(self, batch):\n",
        "        \"\"\"Trains the model on a batch of experiences.\"\"\"\n",
        "        if self.model is None: raise RuntimeError(\"RL model not built.\")\n",
        "        # Placeholder - Subclasses implement specific training logic (e.g., DQN update rule)\n",
        "        # Example DQN logic:\n",
        "        # try:\n",
        "        #     states = np.array([experience[0] for experience in batch])\n",
        "        #     actions = np.array([experience[1] for experience in batch])\n",
        "        #     rewards = np.array([experience[2] for experience in batch])\n",
        "        #     next_states = np.array([experience[3] for experience in batch])\n",
        "        #     dones = np.array([experience[4] for experience in batch])\n",
        "        #\n",
        "        #     current_q_values = self.model.predict(states, verbose=0)\n",
        "        #     next_q_values = self.model.predict(next_states, verbose=0)\n",
        "        #\n",
        "        #     target_q_values = current_q_values.copy()\n",
        "        #\n",
        "        #     for i in range(len(batch)):\n",
        "        #         if dones[i]:\n",
        "        #             target_q_values[i, actions[i]] = rewards[i]\n",
        "        #         else:\n",
        "        #             target_q_values[i, actions[i]] = rewards[i] + self.gamma * np.amax(next_q_values[i])\n",
        "        #\n",
        "        #     self.model.fit(states, target_q_values, epochs=1, verbose=0)\n",
        "        #\n",
        "        #     # Decay epsilon\n",
        "        #     if self.epsilon > self.epsilon_min:\n",
        "        #         self.epsilon *= self.epsilon_decay\n",
        "        # except Exception as e:\n",
        "        #      self.logger.error(f\"Error during batch training: {e}\")\n",
        "        raise NotImplementedError(\"Subclasses must implement train_batch.\")\n",
        "\n",
        "    def load_weights(self, filepath):\n",
        "        \"\"\"Loads model weights from a file.\"\"\"\n",
        "        if self.model is None: raise RuntimeError(\"RL model not built.\")\n",
        "        try:\n",
        "            self.model.load_weights(filepath)\n",
        "            self.logger.info(f\"Loaded model weights from {filepath}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading weights from {filepath}: {e}\")\n",
        "\n",
        "    def save_weights(self, filepath):\n",
        "        \"\"\"Saves model weights to a file.\"\"\"\n",
        "        if self.model is None: raise RuntimeError(\"RL model not built.\")\n",
        "        try:\n",
        "            # Ensure directory exists\n",
        "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "            self.model.save_weights(filepath)\n",
        "            self.logger.info(f\"Saved model weights to {filepath}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving weights to {filepath}: {e}\")\n",
        "\n",
        "# --- Dynamic Expert Evolution (Remains Sync Logic - Used by MLOps/Monitoring) ---\n",
        "class DynamicExpertEvolution:\n",
        "    \"\"\" Logic for deciding MoE evolution actions. Triggered externally (e.g., by monitoring service). \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, moe_registry_proxy: MixtureOfExpertsRegistryManager):\n",
        "        if not config or not moe_registry_proxy:\n",
        "             raise InitializationError(\"DynamicExpertEvolution requires config and MoE Registry proxy.\")\n",
        "        self.config = config\n",
        "        self.moe_registry = moe_registry_proxy # Use the proxy\n",
        "        self.logger = logging.getLogger('MIZ-OKI.DynamicExpertEvolution')\n",
        "        # TODO: Load thresholds, strategies from config more robustly\n",
        "        self.performance_threshold = config.get(\"system_thresholds.expert_performance_threshold\", 0.7) # Example config path\n",
        "        self.improvement_threshold = config.get(\"system_thresholds.expert_improvement_threshold\", 0.05)\n",
        "        self.retirement_threshold = config.get(\"system_thresholds.expert_retirement_threshold\", 0.5)\n",
        "        self.min_evaluations = config.get(\"system_thresholds.expert_min_evaluations\", 10)\n",
        "\n",
        "    def _get_expert_performance_history(self, expert_id: str) -> List[Dict]:\n",
        "        \"\"\" Placeholder: Fetches performance history for an expert (e.g., from monitoring DB/logs or MoE Registry API). \"\"\"\n",
        "        # In reality, query a database or monitoring system, or potentially the MoE Registry API if it stores history.\n",
        "        self.logger.debug(f\"Fetching performance history for expert {expert_id} (Placeholder).\")\n",
        "        # Simulate some history\n",
        "        # This should ideally fetch actual metrics like accuracy, latency, cost per call etc.\n",
        "        return [{\"accuracy\": random.uniform(self.retirement_threshold - 0.1, self.performance_threshold + 0.1), \"timestamp\": datetime.now().isoformat()} for _ in range(random.randint(5, 15))]\n",
        "\n",
        "    def decide_evolution_actions(self, expert_id: str) -> List[Dict]:\n",
        "        \"\"\"Decides evolution actions based on expert performance history.\"\"\"\n",
        "        actions = []\n",
        "        try:\n",
        "            performance_history = self._get_expert_performance_history(expert_id)\n",
        "            if len(performance_history) < self.min_evaluations:\n",
        "                self.logger.info(f\"Expert {expert_id}: Insufficient evaluations ({len(performance_history)}/{self.min_evaluations}) for evolution decision.\")\n",
        "                return actions\n",
        "\n",
        "            # Analyze trends (e.g., average performance, improvement rate)\n",
        "            recent_performance = [p.get('accuracy', 0) for p in performance_history[-5:]] # Look at last 5 evaluations\n",
        "            avg_recent_perf = np.mean(recent_performance) if recent_performance else 0\n",
        "            # --- TODO: Implement more sophisticated trend analysis (e.g., slope of performance over time) ---\n",
        "\n",
        "            self.logger.info(f\"Expert {expert_id}: Avg recent performance (accuracy) = {avg_recent_perf:.3f}\")\n",
        "\n",
        "            # --- Decision Logic ---\n",
        "            if avg_recent_perf < self.retirement_threshold:\n",
        "                actions.append({\"action\": \"retire\", \"expert_id\": expert_id, \"reason\": f\"Performance ({avg_recent_perf:.3f}) below retirement threshold ({self.retirement_threshold:.3f})\"})\n",
        "            elif avg_recent_perf < self.performance_threshold:\n",
        "                actions.append({\"action\": \"retrain\", \"expert_id\": expert_id, \"reason\": f\"Performance ({avg_recent_perf:.3f}) below target threshold ({self.performance_threshold:.3f})\"})\n",
        "                # Optionally suggest distillation if performance is stagnant but acceptable\n",
        "                # if trend_is_stagnant: actions.append({\"action\": \"distill_new_student\", ...})\n",
        "            else:\n",
        "                self.logger.info(f\"Expert {expert_id} is performing adequately.\")\n",
        "                # Consider specialization or fine-tuning if performance is very high?\n",
        "\n",
        "        except Exception as e:\n",
        "             self.logger.error(f\"Error deciding evolution for expert {expert_id}: {e}\", exc_info=True)\n",
        "\n",
        "        return actions\n",
        "\n",
        "# --- Neural Processing Tool (Reworked Async - Delegates to FM Client API Proxy) ---\n",
        "class NeuralProcessingTool:\n",
        "    \"\"\" Handles embedding generation via FoundationModelClient API proxy. Deployed as a service. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, fm_client_proxy: Any):\n",
        "        if not config or not fm_client_proxy:\n",
        "            raise InitializationError(\"NeuralProcessingTool requires config and FM Client proxy.\")\n",
        "        self.config = config\n",
        "        self.fm_client = fm_client_proxy\n",
        "        self.default_embedding_alias = config.foundation_models.defaults.llama4_embedding_model\n",
        "        self.logger = logging.getLogger('MIZ-OKI.NeuralProcessingTool')\n",
        "        self.logger.info(\"Neural Processing Tool logic initialized (Reworked - delegates to FM Client).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"NeuralProcessingTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    async def get_embedding(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Gets embedding via FM Client API proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); data = payload.get(\"data\"); model_alias = payload.get(\"model_alias\")\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if data is None: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'payload.data' (str or List[str]) is required.\"})\n",
        "        if not self.fm_client: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"FM Client Tool proxy unavailable.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        target_model_alias = model_alias or self.default_embedding_alias\n",
        "        status = \"pending\"; response_payload = None; response_metadata = {}\n",
        "\n",
        "        try:\n",
        "            # Prepare MIZ OKI request for FM Client Tool API proxy\n",
        "            fm_request = {\n",
        "                \"payload\": {\"data\": data, \"model_alias\": target_model_alias},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"fm_embed_{request_id}\"\n",
        "            }\n",
        "            fm_response = await self.fm_client.generate_embedding(input_data=fm_request) # Call API proxy\n",
        "\n",
        "            if fm_response.get(\"status\") == \"success\":\n",
        "                embedding_result = fm_response.get(\"payload\", {}).get(\"embedding\")\n",
        "                if embedding_result is not None:\n",
        "                    status = \"success\"; response_payload = {\"embedding\": embedding_result}\n",
        "                    # Include metadata from FM call if available\n",
        "                    response_metadata = fm_response.get(\"metadata\", {})\n",
        "                else:\n",
        "                    status = \"failed\"; errors.append({\"code\": \"EMBEDDING_EMPTY\", \"message\": \"FM Client API proxy returned no embedding.\"})\n",
        "            else:\n",
        "                raise RuntimeError(f\"FM Client API proxy failed for embedding: {fm_response.get('error_details')}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"EMBEDDING_ERROR\", \"message\": str(e)}); logger.error(f\"Async embedding via FM Client proxy failed: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        # Add processing time, potentially merging with FM metadata if it exists\n",
        "        proc_duration = (time.monotonic() - start_time) * 1000\n",
        "        response[\"metadata\"] = response_metadata # Start with FM metadata\n",
        "        response[\"metadata\"][\"nn_tool_processing_duration_ms\"] = proc_duration\n",
        "        return response\n",
        "\n",
        "    async def batch_embed(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "         \"\"\" Batch embedding via FM Client API proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "         # The logic is identical to get_embedding because the FM Client's\n",
        "         # generate_embedding method is designed to handle both single string and list input.\n",
        "         payload = input_data.get(\"payload\", {})\n",
        "         if \"data\" in payload and not isinstance(payload[\"data\"], list):\n",
        "              # If single item passed to batch endpoint, wrap it in a list for FM client\n",
        "              payload[\"data\"] = [payload[\"data\"]]\n",
        "              input_data[\"payload\"] = payload # Update input_data\n",
        "              logger.debug(\"Wrapped single item in list for batch_embed call to FM Client.\")\n",
        "\n",
        "         # Call the same underlying method\n",
        "         return await self.get_embedding(input_data)\n",
        "\n",
        "# --- R2 Reasoning Tool (Reworked Async) ---\n",
        "class R2ReasoningTool:\n",
        "    \"\"\" Implements step-by-step reasoning async, calling deployed Tool APIs via proxies. Deployed as a service. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any, hde_tool_proxy: Any, fm_client_proxy: Any):\n",
        "        if not all([config, kg_tool_proxy, hde_tool_proxy, fm_client_proxy]):\n",
        "            raise InitializationError(\"R2ReasoningTool requires config and proxies for KG, HDE, and FM tools.\")\n",
        "        self.config = config\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.hde_tool = hde_tool_proxy\n",
        "        self.fm_client = fm_client_proxy\n",
        "        self.reasoning_templates: Dict[str, Dict] = {}\n",
        "        # TODO: Replace deque with persistent storage (e.g., Firestore, BQ) for production\n",
        "        self.reasoning_history = deque(maxlen=1000)\n",
        "        self.logger = logging.getLogger('MIZ-OKI.R2ReasoningTool')\n",
        "        self._load_templates() # Load templates synchronously during init\n",
        "        self.logger.info(\"R2 Reasoning Tool logic initialized (Reworked).\")\n",
        "\n",
        "    def _load_templates(self):\n",
        "        \"\"\" Loads reasoning templates (e.g., from GCS, DB, or embedded). \"\"\"\n",
        "        # --- TODO: Load templates from a persistent/configurable source ---\n",
        "        # Example template structure with async step calling KG Tool API proxy:\n",
        "        async def _get_campaign_details_step_api(variables: Dict, kg_tool: Any, **kwargs) -> Dict:\n",
        "             \"\"\"Async step function calling KG Tool API proxy.\"\"\"\n",
        "             campaign_id = variables.get(\"campaign_id\")\n",
        "             trace_id = kwargs.get(\"trace_id\")\n",
        "             if not campaign_id: return {\"_info\": \"Skipped: campaign_id missing\", \"campaign_details\": None}\n",
        "             try:\n",
        "                 # Prepare MIZ OKI request for KG Tool\n",
        "                 kg_request = {\"payload\": {\"mizId\": campaign_id}, \"trace_id\": trace_id, \"request_id\": f\"r2_kg_get_{campaign_id}\"}\n",
        "                 kg_response = await kg_tool.get_entity_endpoint(request=kg_request) # Call API proxy method\n",
        "                 if kg_response.get(\"status\") == \"success\":\n",
        "                     details = kg_response.get(\"payload\", {}).get(\"entity_data\")\n",
        "                     return {\"_info\": f\"Fetched details for {campaign_id} via KG API\", \"campaign_details\": details or {}}\n",
        "                 else:\n",
        "                     logger.warning(f\"KG API get_entity failed for {campaign_id}: {kg_response.get('error_details')}\")\n",
        "                     return {\"_info\": f\"Failed fetch for {campaign_id}\", \"campaign_details\": None, \"_error\": True}\n",
        "             except Exception as e:\n",
        "                  logger.error(f\"Error in _get_campaign_details_step_api for {campaign_id}: {e}\")\n",
        "                  return {\"_info\": f\"Exception fetch for {campaign_id}\", \"campaign_details\": None, \"_error\": True}\n",
        "\n",
        "        self.reasoning_templates = {\n",
        "            \"analyze_campaign_perf\": {\n",
        "                \"description\": \"Analyzes campaign performance using details and LLM reasoning.\",\n",
        "                \"variables\": [\"campaign_id\", \"budget\"], # Input variables expected\n",
        "                \"steps\": [\n",
        "                    {\n",
        "                        \"id\": \"fetch_details\",\n",
        "                        \"logic\": _get_campaign_details_step_api, # Use the async function calling the API proxy\n",
        "                        \"description\": \"Fetch campaign details from Knowledge Graph.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"id\": \"compare_spend\",\n",
        "                        # Simple sync logic is fine here, will be run in thread by default if needed\n",
        "                        \"logic\": lambda vars, **kwargs: {\"spend_vs_budget\": vars.get('campaign_details',{}).get('spend',0) - vars.get('budget',0)},\n",
        "                        \"description\": \"Compare actual spend against budget.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"id\": \"llm_reason\",\n",
        "                        # Define structured logic for calling FM Client Tool API proxy\n",
        "                        \"logic\": {\n",
        "                            \"type\": \"fm_generate\", # Custom type identifier\n",
        "                            \"model_alias\": self.config.foundation_models.defaults.llama4_scout, # Fetch from config\n",
        "                            \"output_variable\": \"llm_analysis\",\n",
        "                            \"prompt_template\": \"Analyze campaign performance based on the following data:\\nCampaign ID: {campaign_id}\\nBudget: {budget}\\nSpend Difference: {spend_vs_budget}\\nDetails:\\n{campaign_details}\\n\\nProvide a concise analysis (2-3 sentences):\"\n",
        "                        },\n",
        "                        \"description\": \"Use LLM to provide a qualitative analysis.\"\n",
        "                    }\n",
        "                    # Add more steps (e.g., call HDE for recommendation)\n",
        "                ],\n",
        "                \"conclusion_template\": \"Analysis for Campaign {campaign_id}: {llm_analysis}\" # Template for final output\n",
        "            }\n",
        "            # Add more templates...\n",
        "        }\n",
        "        self.logger.info(f\"Loaded {len(self.reasoning_templates)} R2 reasoning templates (Example).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"R2ReasoningTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    async def reason(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Perform step-by-step reasoning async. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); template_id = payload.get(\"template_id\"); task_input_data = payload.get(\"input_data\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not template_id or template_id not in self.reasoning_templates:\n",
        "            errors.append({\"code\": \"TEMPLATE_NOT_FOUND\", \"message\": f\"Reasoning template '{template_id}' not found.\"})\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        template = self.reasoning_templates[template_id]\n",
        "        reasoning_id = f\"r2_{template_id}_{uuid.uuid4().hex[:8]}\"\n",
        "        status = \"pending\"; response_payload = None\n",
        "        log_entry = {\n",
        "            \"reasoning_id\": reasoning_id, \"template_id\": template_id, \"trace_id\": trace_id,\n",
        "            \"input_preview\": str(task_input_data)[:200], \"status\": \"running\",\n",
        "            \"steps_executed\": [], \"variables_state\": {}, \"conclusion\": None,\n",
        "            \"chain_of_thought\": [] # Detailed log of execution\n",
        "        }\n",
        "        # Initialize variables from input data based on template definition\n",
        "        variables = {var: task_input_data.get(var) for var in template.get(\"variables\", []) if var in task_input_data}\n",
        "        log_entry[\"variables_state\"][\"initial\"] = variables.copy()\n",
        "        cot = log_entry[\"chain_of_thought\"]\n",
        "        cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] START Reasoning: {template_id}. Initial Vars: {variables}\")\n",
        "\n",
        "        try:\n",
        "            for i, step_config in enumerate(template[\"steps\"]):\n",
        "                step_start = time.monotonic()\n",
        "                step_id = step_config.get(\"id\", f\"step_{i+1}\")\n",
        "                step_desc = step_config.get(\"description\", \"No description\")\n",
        "                step_log = {\"step_id\": step_id, \"description\": step_desc, \"status\": \"pending\"}\n",
        "                cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] Executing Step: {step_id} ({step_desc})...\")\n",
        "\n",
        "                try:\n",
        "                    logic = step_config.get(\"logic\")\n",
        "                    step_output_vars = {}\n",
        "\n",
        "                    # --- Execute Logic based on type ---\n",
        "                    if isinstance(logic, dict) and 'type' in logic:\n",
        "                        logic_type = logic['type']\n",
        "                        cot.append(f\"  - Logic Type: Structured ({logic_type})\")\n",
        "                        # --- Call Tool APIs via Proxies ---\n",
        "                        if logic_type == 'fm_generate' and self.fm_client:\n",
        "                            prompt_template = logic[\"prompt_template\"]\n",
        "                            model_alias = logic.get(\"model_alias\", self.config.foundation_models.defaults.llama4_scout)\n",
        "                            output_var = logic[\"output_variable\"]\n",
        "                            # Safely format prompt using available variables\n",
        "                            prompt = prompt_template.format_map(defaultdict(lambda: 'N/A', variables))\n",
        "                            cot.append(f\"  - Calling FM Client API (Model: {model_alias})\")\n",
        "                            fm_request = {\"payload\": {\"prompt\": prompt, \"model_alias\": model_alias, \"max_tokens\": logic.get(\"max_tokens\", 512)}, \"trace_id\": trace_id, \"request_id\": f\"r2_fm_{step_id}\"}\n",
        "                            fm_response = await self.fm_client.generate_text(input_data=fm_request) # Call API proxy\n",
        "                            if fm_response.get(\"status\") == \"success\":\n",
        "                                step_output_vars[output_var] = fm_response.get(\"payload\", {}).get(\"generated_text\")\n",
        "                                cot.append(f\"  - FM Response stored in '{output_var}'.\")\n",
        "                            else: raise RuntimeError(f\"FM Client API call failed: {fm_response.get('error_details')}\")\n",
        "\n",
        "                        elif logic_type == 'kg_query' and self.kg_tool:\n",
        "                            query_template = logic[\"query_template\"]\n",
        "                            output_var = logic[\"output_variable\"]\n",
        "                            query = query_template.format_map(defaultdict(lambda: 'N/A', variables))\n",
        "                            cot.append(f\"  - Calling KG Tool API (Query: {query[:50]}...)\")\n",
        "                            kg_request = {\"payload\": {\"query\": query, \"parameters\": logic.get(\"parameters\", {})}, \"trace_id\": trace_id, \"request_id\": f\"r2_kg_{step_id}\"} # Pass params if needed\n",
        "                            kg_response = await self.kg_tool.execute_query(request=kg_request) # Call API proxy\n",
        "                            if kg_response.get(\"status\") == \"success\":\n",
        "                                step_output_vars[output_var] = kg_response.get(\"payload\", {}).get(\"results\", [])\n",
        "                                cot.append(f\"  - KG Response stored in '{output_var}'.\")\n",
        "                            else: raise RuntimeError(f\"KG Tool API call failed: {kg_response.get('error_details')}\")\n",
        "\n",
        "                        elif logic_type == 'hde_call' and self.hde_tool:\n",
        "                            decision_type = logic[\"decision_type\"]\n",
        "                            context_vars = logic.get(\"context_variables\", list(variables.keys()))\n",
        "                            output_var = logic[\"output_variable\"]\n",
        "                            hde_context = {k: variables[k] for k in context_vars if k in variables}\n",
        "                            cot.append(f\"  - Calling HDE Tool API (Decision: {decision_type})\")\n",
        "                            hde_request = {\"payload\": {\"decision_type\": decision_type, \"context\": hde_context}, \"trace_id\": trace_id, \"request_id\": f\"r2_hde_{step_id}\"}\n",
        "                            hde_response = await self.hde_tool.make_decision(input_data=hde_request) # Call API proxy\n",
        "                            if hde_response.get(\"status\") in [\"success\", \"ethics_review_required\", \"partial_success\"]: # Handle various success states\n",
        "                                step_output_vars[output_var] = hde_response.get(\"payload\", {}) # Store full HDE log\n",
        "                                cot.append(f\"  - HDE Response stored in '{output_var}'. Status: {hde_response.get('status')}\")\n",
        "                            else: raise RuntimeError(f\"HDE Tool API call failed: {hde_response.get('error_details')}\")\n",
        "\n",
        "                        else:\n",
        "                            raise ValueError(f\"Unsupported structured logic type or missing tool proxy: {logic_type}\")\n",
        "\n",
        "                    elif callable(logic): # Execute custom async or sync function\n",
        "                        cot.append(f\"  - Logic Type: Callable Function ({logic.__name__})\")\n",
        "                        # Pass proxies to the function\n",
        "                        tool_proxies = {\"kg_tool\": self.kg_tool, \"hde_tool\": self.hde_tool, \"fm_client\": self.fm_client, \"trace_id\": trace_id}\n",
        "                        if asyncio.iscoroutinefunction(logic):\n",
        "                            step_result = await logic(variables, **tool_proxies)\n",
        "                        else:\n",
        "                            # Run sync function in thread pool\n",
        "                            step_result = await asyncio.to_thread(logic, variables, **tool_proxies)\n",
        "\n",
        "                        if step_result and isinstance(step_result, dict):\n",
        "                            step_output_vars = {k:v for k,v in step_result.items() if not k.startswith('_')}\n",
        "                            step_log[\"info\"] = step_result.get(\"_info\", \"Callable executed.\")\n",
        "                            if step_result.get(\"_error\"): raise RuntimeError(step_log[\"info\"])\n",
        "                        else:\n",
        "                            logger.warning(f\"R2 Step {step_id}: Callable logic returned unexpected type {type(step_result)}\")\n",
        "                            step_log[\"info\"] = \"Callable returned non-dict or None.\"\n",
        "                    else:\n",
        "                        raise TypeError(f\"Invalid logic type for step {step_id}: {type(logic)}\")\n",
        "\n",
        "                    # Update variables and log step success\n",
        "                    variables.update(step_output_vars)\n",
        "                    step_log[\"status\"] = \"success\"\n",
        "                    step_log[\"output_preview\"] = {k: str(v)[:100] + ('...' if len(str(v)) > 100 else '') for k, v in step_output_vars.items()} # Preview output\n",
        "                    cot.append(f\"  - Step Success. Output Keys: {list(step_output_vars.keys())}\")\n",
        "\n",
        "                except Exception as step_e:\n",
        "                    logger.error(f\"R2 Step {step_id} FAILED: {step_e}\", exc_info=True)\n",
        "                    step_log[\"status\"] = \"failed\"; step_log[\"error\"] = str(step_e)\n",
        "                    log_entry[\"steps_executed\"].append(step_log)\n",
        "                    cot.append(f\"  - Step FAILED: {step_e}\")\n",
        "                    raise step_e # Propagate error to stop reasoning chain\n",
        "\n",
        "                step_log[\"duration_ms\"] = (time.monotonic() - step_start) * 1000\n",
        "                log_entry[\"steps_executed\"].append(step_log)\n",
        "                log_entry[\"variables_state\"][f\"after_{step_id}\"] = variables.copy() # Log state after step\n",
        "\n",
        "            # --- Generate Conclusion ---\n",
        "            conclusion_template = template.get(\"conclusion_template\")\n",
        "            conclusion = None\n",
        "            if conclusion_template:\n",
        "                try:\n",
        "                    conclusion = conclusion_template.format_map(defaultdict(lambda: 'N/A', variables))\n",
        "                    cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] Generated Conclusion: {str(conclusion)[:100]}...\")\n",
        "                except KeyError as fmt_e:\n",
        "                    conclusion = f\"Conclusion generation failed: Missing key {fmt_e}\"\n",
        "                    cot.append(f\"  - ERROR generating conclusion: {fmt_e}\")\n",
        "                    errors.append({\"code\": \"CONCLUSION_FORMAT_ERROR\", \"message\": str(fmt_e)})\n",
        "                    status = \"partial_success\" # Mark as partial if conclusion fails\n",
        "            else:\n",
        "                cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] No conclusion template defined.\")\n",
        "                conclusion = variables # Return final variables if no template\n",
        "\n",
        "            log_entry[\"conclusion\"] = conclusion\n",
        "            log_entry[\"status\"] = \"success\" if status == \"pending\" else status # Update status if not already set to partial\n",
        "            status = log_entry[\"status\"]\n",
        "            response_payload = log_entry # Return the full log entry as payload\n",
        "\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"R2_ERROR\", \"message\": str(e)}); log_entry[\"status\"] = \"failed\"; log_entry[\"error\"] = str(e); cot.append(f\"[{datetime.now(datetime.timezone.utc).isoformat()}] PROCESS FAILED: {e}\")\n",
        "             logger.error(f\"R2 Reasoning failed for template '{template_id}' (ID: {reasoning_id}): {e}\", exc_info=True)\n",
        "             response_payload = log_entry # Return log even on failure\n",
        "\n",
        "        log_entry[\"total_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        self.reasoning_history.append(log_entry) # Add to in-memory history\n",
        "        # --- TODO: Persist log_entry async via KG Tool API or other logging service ---\n",
        "        # if self.kg_tool:\n",
        "        #     asyncio.create_task(self.kg_tool.save_reasoning_log(log_entry)) # Conceptual method\n",
        "        # --- End TODO ---\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = log_entry[\"total_duration_ms\"]\n",
        "        return response\n",
        "\n",
        "# --- Initialization (Conceptual - Tools instantiated by framework/orchestrator) ---\n",
        "# semantic_rag_tool: Optional[SemanticGraphRAGTool] = None\n",
        "# r2_reasoning_tool: Optional[R2ReasoningTool] = None\n",
        "# nn_tool_proxy: Optional[NeuralProcessingToolDelegate] = None # Use the delegate\n",
        "\n",
        "# async def initialize_technical_flows():\n",
        "#     global semantic_rag_tool, r2_reasoning_tool, nn_tool_proxy\n",
        "#     if not _config_obj or not _real_dependencies:\n",
        "#         logger.critical(\"Cannot initialize Technical Flows: Config or dependencies missing.\")\n",
        "#         return\n",
        "#     try:\n",
        "#         # NN Tool is just a delegate around FM Client proxy\n",
        "#         nn_tool_proxy = NeuralProcessingToolDelegate(_fm_client_proxy)\n",
        "#         semantic_rag_tool = SemanticGraphRAGTool(_config_obj, _kg_tool_proxy, _fm_client_proxy, nn_tool_proxy)\n",
        "#         r2_reasoning_tool = R2ReasoningTool(_config_obj, _kg_tool_proxy, _hde_tool_proxy, _fm_client_proxy)\n",
        "#         logger.info(\"Technical Flow Tools initialized.\")\n",
        "#     except Exception as e:\n",
        "#         logger.critical(f\"Technical Flow Tools initialization failed: {e}\", exc_info=True)\n",
        "#         semantic_rag_tool = None; r2_reasoning_tool = None; nn_tool_proxy = None\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 Technical Flows Layer Logic (Cell 6 - Reworked) ---\")\n",
        "print(\"RAG, R2, NN Tools refactored as async services handling MIZ OKI payloads.\")\n",
        "print(\"Uses real dependencies/proxies via MIZ OKI APIs.\")\n",
        "print(\"RL/MoE Base classes unchanged; logic used by external ADK Agents/MLOps.\")\n",
        "print(\"----------------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "nCuOeJdtp9QU",
        "outputId": "b592a80e-7cce-434a-9775-c000ebe662f2"
      },
      "id": "nCuOeJdtp9QU",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-10-1f907cc6ac45>, line 63)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-1f907cc6ac45>\"\u001b[0;36m, line \u001b[0;32m63\u001b[0m\n\u001b[0;31m    class MockKGTool: async def get_entity(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"entity_data\": {\"name\": \"Mock Entity\"}}}; async def get_neighbors(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"neighbors\": []}}; async def search_vector_index(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"results\": [(\"mock_id\", 0.9, {})]}}; async def execute_query(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"results\": []}}\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Business Applications Layer (Reworked)\n",
        "# Status: AWE Service uses real Vertex AI Client proxies (if available).\n",
        "#         App Tools (BEAB, HP, RTB, AdOpt) use real dependencies/proxies via MIZ OKI APIs.\n",
        "#         External API tools remain critical placeholders. Logic within App tools needs full implementation.\n",
        "\n",
        "import logging\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import json\n",
        "import uuid\n",
        "import asyncio\n",
        "import re # Added for AWE\n",
        "import hashlib # Added for Pseudonymizer\n",
        "from typing import Dict, Any, Optional, List, Union, Callable, Tuple # Added Tuple\n",
        "import aiohttp # For API proxies\n",
        "\n",
        "# --- GCP Client Libraries & Dependencies ---\n",
        "# Needed for AWE Service interacting with Vertex AI Workflows\n",
        "try:\n",
        "   from google.cloud import workflows_v1\n",
        "   from google.cloud.workflows import executions_v1\n",
        "   from google.cloud.workflows.executions_v1.types import Execution\n",
        "   from google.protobuf import json_format, field_mask_pb2\n",
        "   from google.api_core import exceptions as google_api_exceptions\n",
        "   VERTEX_WORKFLOWS_SDK_AVAILABLE = True\n",
        "   # Assume clients are initialized elsewhere and passed in\n",
        "   logger.debug(\"Successfully imported google-cloud-workflows libraries for Cell 7.\")\n",
        "except ImportError:\n",
        "   logger.warning(\"google-cloud-workflows library not found. AWE service implementation is limited.\")\n",
        "   VERTEX_WORKFLOWS_SDK_AVAILABLE = False\n",
        "   # Dummy classes from Cell 16 reformation (or Cell 9)\n",
        "   class ExecutionState: ACTIVE=\"ACTIVE\"; SUCCEEDED=\"SUCCEEDED\"; FAILED=\"FAILED\"; CANCELLED=\"CANCELLED\"; SUSPENDED=\"SUSPENDED\"\n",
        "   class DummyProto: pass\n",
        "   class workflows_v1: class WorkflowsAsyncClient: pass; class Workflow: pass; class GetWorkflowRequest: pass; class UpdateWorkflowRequest: pass; class CreateWorkflowRequest: pass\n",
        "   class executions_v1: ExecutionState = ExecutionState; class ExecutionsAsyncClient: pass; class Execution(DummyProto): pass; class CreateExecutionRequest: pass; class GetExecutionRequest: pass; class CancelExecutionRequest: pass; class ListExecutionsRequest: pass; class ListExecutionsResponse: pass; ExecutionView = type('Enum', (), {'BASIC': 1, 'FULL': 2})()\n",
        "   class json_format: @staticmethod def ParseDict(*args, **kwargs): pass; @staticmethod def MessageToDict(msg, **kwargs): return getattr(msg, '_fields', {})\n",
        "   class google_api_exceptions: class NotFound(Exception): pass; class InvalidArgument(Exception): pass; class PermissionDenied(Exception): pass; class FailedPrecondition(Exception):pass; class GoogleAPIError(Exception): pass\n",
        "   class field_mask_pb2: class FieldMask: pass\n",
        "\n",
        "# --- Assume Real Tool/Client Dependencies are Injected/Available ---\n",
        "# These proxies represent API clients for other deployed MIZ OKI services or GCP clients.\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    # Proxies for other MIZ OKI Tool APIs\n",
        "    if 'kg_tool_service_instance' not in globals(): raise NameError(\"kg_tool_service_instance proxy not found\") # Cell 3 Proxy\n",
        "    if 'moe_registry_manager' not in globals(): raise NameError(\"moe_registry_manager not found\") # Cell 4 Instance/Proxy\n",
        "    if 'foundation_model_client' not in globals(): raise NameError(\"foundation_model_client proxy not found\") # Cell 18 Proxy\n",
        "    if 'expert_invoker' not in globals(): raise NameError(\"expert_invoker proxy not found\") # Needs definition/mock\n",
        "    if 'xai' not in globals(): raise NameError(\"xai (ExplainableAI instance/proxy) not found\") # Cell 11 Proxy\n",
        "\n",
        "    # Real/Mock Clients for GCP Services\n",
        "    if '_workflow_executions_client' not in globals(): raise NameError(\"_workflow_executions_client not found\") # Cell 16 Client Proxy\n",
        "    if '_workflows_client' not in globals(): raise NameError(\"_workflows_client not found\") # Cell 7 needs this\n",
        "\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _kg_tool_proxy = kg_tool_service_instance\n",
        "    _moe_registry_proxy = moe_registry_manager\n",
        "    _expert_invoker_proxy = expert_invoker\n",
        "    _fm_client_proxy = foundation_model_client\n",
        "    _xai_proxy = xai\n",
        "    _workflow_executions_client_proxy = _workflow_executions_client # Use client proxy from Cell 16\n",
        "    _workflows_client_proxy = _workflows_client # Use client proxy\n",
        "\n",
        "    # NN Tool delegates to FM Client (from Cell 6)\n",
        "    class NeuralProcessingToolDelegate:\n",
        "        def __init__(self, fm_client_proxy: Any): self.fm_client = fm_client_proxy; self.logger = logging.getLogger('MIZ-OKI.NNToolDelegate');\n",
        "        async def get_embedding(self, input_data: Dict): return await self.fm_client.generate_embedding(input_data=input_data)\n",
        "    _nn_tool_proxy = NeuralProcessingToolDelegate(_fm_client_proxy)\n",
        "\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real/conceptual dependencies in Cell 7 (Reworked).\")\n",
        "\n",
        "except NameError as e:\n",
        "    logger.warning(f\"Dependency Error in Cell 7 ({e}). Using Mocks/Placeholders.\")\n",
        "    _real_dependencies = False\n",
        "    # --- Mock/Placeholder Setup ---\n",
        "    class MockKGTool: async def execute_query(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"results\": [{'value': 0.7}]}}; async def get_entity(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"entity_data\": {}}}\n",
        "    class MockMoERegistryManager: async def find_expert_for_task(self, *args, **kwargs): await asyncio.sleep(0.01); return \"mock_id\"; async def get_expert_details(self, *args,**kwargs): await asyncio.sleep(0.01); return {\"endpoint\":\"http://mock\"}\n",
        "    class MockFMClientTool: async def generate_text(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"generated_text\": \"```yaml\\nsteps:\\n- mock_step: ...\\n```\"}}; async def generate_embedding(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"embedding\": [0.1]}}\n",
        "    class MockExpertInvoker: async def invoke(self, request): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"prediction\": [5.0], \"recommendations\": [\"ITEM_1\"]}}\n",
        "    class MockNNTool: async def get_embedding(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"embedding\": [0.1]}}\n",
        "    class MockXAI: async def record_decision(self, request): await asyncio.sleep(0.01); pass\n",
        "    class MockVertexClient: # Simplified mock for both workflow/execution\n",
        "        async def create_execution(self, request): await asyncio.sleep(0.01); return MagicMock(name=f\"{request.parent}/executions/{uuid.uuid4().hex[:8]}\")\n",
        "        async def get_execution(self, request): await asyncio.sleep(0.01); return MagicMock(state=ExecutionState.SUCCEEDED, result='{\"mock\": \"result\"}', _pb={})\n",
        "        async def create_workflow(self, request): await asyncio.sleep(0.01); return MagicMock(result=lambda timeout: True) # Simulate LRO\n",
        "        async def update_workflow(self, request): await asyncio.sleep(0.01); return MagicMock(result=lambda timeout: True) # Simulate LRO\n",
        "        async def get_workflow(self, request): await asyncio.sleep(0.01); return MagicMock(source_contents='current_source_code')\n",
        "    # Define minimal config if needed\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ:\n",
        "        from dataclasses import dataclass, field\n",
        "        @dataclass class MockGcpConfig: project_id:Optional[str]=\"mock-proj\"; region:str=\"mock-region\"; miz_salt:str=\"mock_salt\"\n",
        "        @dataclass class MockKgConfig: vector_db_type:str=\"none\"\n",
        "        @dataclass class MockFmDefaults: llama4_scout: str = \"mock-scout\"; llama4_maverick: str = \"mock-mav\"\n",
        "        @dataclass class MockFmConfig: defaults: MockFmDefaults = field(default_factory=MockFmDefaults)\n",
        "        @dataclass class MockBusinessImpact: roas_target: float = 8.0\n",
        "        @dataclass class ServiceEndpointsConfig: ads_platform_api_endpoint: Optional[str]=None; crm_api_endpoint: Optional[str]=None; moe_registry_api_endpoint: Optional[str]=None; expert_invoker_api_endpoint: Optional[str]=None; kg_tool_api_endpoint: Optional[str]=None\n",
        "        @dataclass class MockConfig: gcp: MockGcpConfig=field(default_factory=MockGcpConfig); kg:MockKgConfig=field(default_factory=MockKgConfig); foundation_models: MockFmConfig = field(default_factory=MockFmConfig); business_impact: MockBusinessImpact = field(default_factory=MockBusinessImpact); miz_salt: str = \"mock_salt\"; miz_oki_schema_version: str = \"3.0\"; service_endpoints: ServiceEndpointsConfig = field(default_factory=ServiceEndpointsConfig); def get(self, key, default=None): parts=key.split('.'); val=self; try: [val := getattr(val, p) for p in parts]; return val; except: return default\n",
        "        _config_obj = MockConfig()\n",
        "\n",
        "    _kg_tool_proxy = MockKGTool(); _moe_registry_proxy = MockMoERegistryManager(); _expert_invoker_proxy = MockExpertInvoker(); _fm_client_proxy = MockFMClientTool(); _nn_tool_proxy = MockNNTool(); _xai_proxy = MockXAI(); _workflow_executions_client_proxy = MockVertexClient(); _workflows_client_proxy = MockVertexClient()\n",
        "    VERTEX_WORKFLOWS_SDK_AVAILABLE = False # Ensure mock status reflected\n",
        "    # --- End Mock/Placeholder Setup ---\n",
        "\n",
        "logger = logging.getLogger('MIZ-OKI.BusinessApplications')\n",
        "\n",
        "# --- Placeholder External API Tools (Need Implementation as Services) ---\n",
        "# These represent separate microservices deployed (e.g., Cloud Run) that handle\n",
        "# the specifics of interacting with external platforms like Google Ads, Meta Ads, Shopify, Klaviyo.\n",
        "# They accept and return MIZ OKI payloads.\n",
        "\n",
        "class AdsPlatformApiToolProxy: # Renamed to Proxy\n",
        "    \"\"\"Proxy for the deployed Ads Platform Interaction Tool.\"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, service_endpoint: Optional[str]):\n",
        "        self.config = config\n",
        "        self.endpoint = service_endpoint # Fetched from config or passed directly\n",
        "        self.logger = logging.getLogger('MIZ-OKI.AdsPlatformApiToolProxy')\n",
        "        self.session: Optional[aiohttp.ClientSession] = None\n",
        "        if not self.endpoint: self.logger.error(\"Ads Platform API endpoint not configured!\")\n",
        "\n",
        "    async def initialize(self): self.session = aiohttp.ClientSession()\n",
        "    async def cleanup(self):\n",
        "        if self.session: await self.session.close()\n",
        "\n",
        "    async def _make_request(self, action: str, payload: Dict, trace_id: Optional[str]) -> Dict:\n",
        "        \"\"\"Makes an async HTTP request to the deployed Ads Tool.\"\"\"\n",
        "        if not self.endpoint or not self.session: return {\"status\": \"error\", \"error_details\": [{\"message\": \"Ads Tool endpoint/session not available\"}]}\n",
        "        url = f\"{self.endpoint.rstrip('/')}/{action}\" # e.g., /adjust_bid, /update_budget\n",
        "        miz_oki_request = {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": f\"ads_{action}_{uuid.uuid4().hex[:8]}\",\n",
        "            \"trace_id\": trace_id,\n",
        "            \"source_component\": \"BusinessAppLayerProxy\", # Identify caller\n",
        "            \"payload\": payload\n",
        "        }\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "        # --- TODO: Add authentication header ---\n",
        "        # Example using OIDC token for Cloud Run invoker:\n",
        "        # token = await self._get_oidc_token(self.endpoint)\n",
        "        # if token: headers[\"Authorization\"] = f\"Bearer {token}\"\n",
        "        # else: return {\"status\": \"error\", \"error_details\": [{\"message\": \"Failed to get auth token for Ads Tool\"}]}\n",
        "        # --- End TODO ---\n",
        "        try:\n",
        "            async with self.session.post(url, json=miz_oki_request, headers=headers, timeout=30) as response:\n",
        "                response.raise_for_status()\n",
        "                return await response.json() # Assume Ads Tool returns MIZ OKI response\n",
        "        except aiohttp.ClientResponseError as http_err:\n",
        "            error_text = await http_err.text()\n",
        "            self.logger.error(f\"HTTP error calling Ads Tool API ({action}) Status {http_err.status}: {error_text}\")\n",
        "            return {\"status\": \"error\", \"error_details\": [{\"code\": \"HTTP_ERROR\", \"status_code\": http_err.status, \"message\": f\"Ads Tool API Error: {error_text}\"}]}\n",
        "        except asyncio.TimeoutError:\n",
        "             self.logger.error(f\"Timeout calling Ads Tool API ({action}) at {url}\")\n",
        "             return {\"status\": \"error\", \"error_details\": [{\"code\": \"TIMEOUT_ERROR\", \"message\": \"Timeout calling Ads Tool API\"}]}\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calling Ads Tool API ({action}): {e}\", exc_info=True)\n",
        "            return {\"status\": \"error\", \"error_details\": [{\"code\": \"INTERNAL_PROXY_ERROR\", \"message\": str(e)}]}\n",
        "\n",
        "    # --- TODO: Implement _get_oidc_token helper if needed ---\n",
        "    # async def _get_oidc_token(self, audience): ...\n",
        "    # --- End TODO ---\n",
        "\n",
        "    async def adjust_bid(self, request: Dict) -> Dict: # Expects MIZ OKI request\n",
        "         payload = request.get(\"payload\", {})\n",
        "         self.logger.info(f\"[ADS TOOL PROXY] Calling adjust_bid for {payload.get('campaign_id')}\")\n",
        "         return await self._make_request(\"adjust_bid\", payload, request.get(\"trace_id\"))\n",
        "\n",
        "    async def update_budget(self, request: Dict) -> Dict: # Expects MIZ OKI request\n",
        "         payload = request.get(\"payload\", {})\n",
        "         self.logger.info(f\"[ADS TOOL PROXY] Calling update_budget for {payload.get('campaign_id')}\")\n",
        "         return await self._make_request(\"update_budget\", payload, request.get(\"trace_id\"))\n",
        "\n",
        "    async def create_ad_creative(self, request: Dict) -> Dict: # Expects MIZ OKI request\n",
        "          payload = request.get(\"payload\", {})\n",
        "          self.logger.info(f\"[ADS TOOL PROXY] Calling create_creative for {payload.get('campaign_id')}\")\n",
        "          # Example: Actual Ads tool might return {\"status\": \"success\", \"payload\": {\"creative_id\": \"new_creative_123\"}}\n",
        "          return await self._make_request(\"create_creative\", payload, request.get(\"trace_id\"))\n",
        "\n",
        "class CrmApiToolProxy: # Renamed to Proxy\n",
        "    \"\"\"Proxy for the deployed CRM Interaction Tool.\"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, service_endpoint: Optional[str]):\n",
        "        self.config = config\n",
        "        self.endpoint = service_endpoint # Fetched from config or passed directly\n",
        "        self.logger = logging.getLogger('MIZ-OKI.CrmApiToolProxy')\n",
        "        self.session: Optional[aiohttp.ClientSession] = None\n",
        "        if not self.endpoint: self.logger.error(\"CRM API endpoint not configured!\")\n",
        "\n",
        "    async def initialize(self): self.session = aiohttp.ClientSession()\n",
        "    async def cleanup(self):\n",
        "        if self.session: await self.session.close()\n",
        "\n",
        "    async def _make_request(self, action: str, payload: Dict, trace_id: Optional[str]) -> Dict:\n",
        "        \"\"\"Makes an async HTTP request to the deployed CRM Tool.\"\"\"\n",
        "        if not self.endpoint or not self.session: return {\"status\": \"error\", \"error_details\": [{\"message\": \"CRM Tool endpoint/session not available\"}]}\n",
        "        url = f\"{self.endpoint.rstrip('/')}/{action}\"\n",
        "        miz_oki_request = {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": f\"crm_{action}_{uuid.uuid4().hex[:8]}\",\n",
        "            \"trace_id\": trace_id,\n",
        "            \"source_component\": \"BusinessAppLayerProxy\",\n",
        "            \"payload\": payload\n",
        "        }\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "        # TODO: Add authentication header\n",
        "        try:\n",
        "            async with self.session.post(url, json=miz_oki_request, headers=headers, timeout=30) as response:\n",
        "                response.raise_for_status()\n",
        "                return await response.json()\n",
        "        except aiohttp.ClientResponseError as http_err:\n",
        "            error_text = await http_err.text()\n",
        "            self.logger.error(f\"HTTP error calling CRM Tool API ({action}) Status {http_err.status}: {error_text}\")\n",
        "            return {\"status\": \"error\", \"error_details\": [{\"code\": \"HTTP_ERROR\", \"status_code\": http_err.status, \"message\": f\"CRM Tool API Error: {error_text}\"}]}\n",
        "        except asyncio.TimeoutError:\n",
        "             self.logger.error(f\"Timeout calling CRM Tool API ({action}) at {url}\")\n",
        "             return {\"status\": \"error\", \"error_details\": [{\"code\": \"TIMEOUT_ERROR\", \"message\": \"Timeout calling CRM Tool API\"}]}\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calling CRM Tool API ({action}): {e}\", exc_info=True)\n",
        "            return {\"status\": \"error\", \"error_details\": [{\"code\": \"INTERNAL_PROXY_ERROR\", \"message\": str(e)}]}\n",
        "\n",
        "    async def update_customer_segment(self, request: Dict) -> Dict: # Expects MIZ OKI request\n",
        "         payload = request.get(\"payload\", {})\n",
        "         self.logger.info(f\"[CRM TOOL PROXY] Calling update_segment for {payload.get('customer_id')}\")\n",
        "         # Example: Actual CRM tool might return {\"status\": \"success\", \"payload\": {\"segment_updated\": True, \"customer_id\": \"...\"}}\n",
        "         return await self._make_request(\"update_segment\", payload, request.get(\"trace_id\"))\n",
        "\n",
        "    async def trigger_email_campaign(self, request: Dict) -> Dict: # Expects MIZ OKI request\n",
        "         payload = request.get(\"payload\", {})\n",
        "         self.logger.info(f\"[CRM TOOL PROXY] Calling trigger_email for {payload.get('email')}\")\n",
        "         # Example: Actual CRM tool might return {\"status\": \"success\", \"payload\": {\"campaign_triggered\": True, \"message_id\": \"...\"}}\n",
        "         return await self._make_request(\"trigger_email\", payload, request.get(\"trace_id\"))\n",
        "\n",
        "# --- Data Pseudonymizer (from Cell 3 rework) ---\n",
        "# Assume DataPseudonymizer class is defined as in Cell 3 rework\n",
        "\n",
        "# --- Privacy Controls Tool (Reworked - Internal Use, Sync OK) ---\n",
        "class PrivacyControlsTool:\n",
        "    \"\"\" Implements data privacy policies internally based on configuration. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig):\n",
        "        if not config: raise InitializationError(\"Config required for PrivacyControlsTool.\")\n",
        "        self.config = config\n",
        "        # Load policies from config, default to requiring pseudonymization\n",
        "        self.policies = config.get(\"privacy_policies\", {\"default\": {\"requires_pseudonymization\": True, \"allowed_fields\": None}})\n",
        "        self.pseudonymizer = DataPseudonymizer(config.miz_salt) # Assumes salt is loaded\n",
        "        self.logger = logging.getLogger('MIZ-OKI.PrivacyControlsTool')\n",
        "        self.logger.info(\"Privacy Controls Tool logic initialized.\")\n",
        "\n",
        "    def apply_policy(self, data: Union[Dict, List], source_profile_id: Optional[str] = None, target_profile_id: str = \"default\") -> Union[Dict, List]:\n",
        "        \"\"\"\n",
        "        Apply privacy policy synchronously based on target profile.\n",
        "        Handles pseudonymization and field filtering.\n",
        "        \"\"\"\n",
        "        target_policy = self.policies.get(target_profile_id, self.policies.get(\"default\", {}))\n",
        "        requires_pseudo = target_policy.get(\"requires_pseudonymization\", True)\n",
        "        allowed_fields = target_policy.get(\"allowed_fields\") # Should be a set or None\n",
        "\n",
        "        if allowed_fields is not None and not isinstance(allowed_fields, set):\n",
        "            allowed_fields = set(allowed_fields) # Ensure it's a set for efficient lookup\n",
        "\n",
        "        processed_data = data\n",
        "        try:\n",
        "            # 1. Apply Pseudonymization\n",
        "            if requires_pseudo:\n",
        "                if isinstance(data, list):\n",
        "                    processed_data = [self.pseudonymizer.pseudonymize_dict(item) if isinstance(item, dict) else item for item in data]\n",
        "                elif isinstance(data, dict):\n",
        "                    processed_data = self.pseudonymizer.pseudonymize_dict(data)\n",
        "                # else: data is unchanged if not list/dict\n",
        "\n",
        "            # 2. Apply Field Filtering (if applicable)\n",
        "            if allowed_fields is not None:\n",
        "                if isinstance(processed_data, list):\n",
        "                    # Filter fields within each dictionary in the list\n",
        "                    processed_data = [{k: v for k, v in item.items() if k in allowed_fields} if isinstance(item, dict) else item for item in processed_data]\n",
        "                elif isinstance(processed_data, dict):\n",
        "                    # Filter fields in the dictionary\n",
        "                    processed_data = {k: v for k, v in processed_data.items() if k in allowed_fields}\n",
        "                # else: filtering doesn't apply to non-dict/list data\n",
        "\n",
        "            return processed_data\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error applying privacy policy: {e}\", exc_info=True)\n",
        "            return data # Return original data on error\n",
        "\n",
        "# --- Brand Equity-Aware Bidding Tool (Reworked Async) ---\n",
        "class BrandEquityAwareBiddingTool:\n",
        "    \"\"\" Optimizes bidding async. Deployed as a service callable via MIZ OKI API. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any, moe_registry_proxy: Any, expert_invoker_proxy: Any, xai_proxy: Optional[Any] = None, ads_platform_tool_proxy: Optional[Any] = None):\n",
        "        if not all([config, kg_tool_proxy, moe_registry_proxy, expert_invoker_proxy]):\n",
        "            raise InitializationError(\"BEABTool requires config and proxies for KG, MoE Registry, and Expert Invoker.\")\n",
        "        self.config = config\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.moe_registry = moe_registry_proxy\n",
        "        self.expert_invoker = expert_invoker_proxy\n",
        "        self.xai = xai_proxy # Optional XAI proxy\n",
        "        self.ads_platform_tool = ads_platform_tool_proxy # Optional Ads Platform proxy\n",
        "        self.default_roas_fallback = 3.0 # Example fallback\n",
        "        self.equity_weight = config.get(\"beab_equity_weight\", 0.2)\n",
        "        self.min_bid_threshold = config.get(\"rtb_min_bid_threshold\", 0.01)\n",
        "        self.logger = logging.getLogger('MIZ-OKI.BEABTool')\n",
        "        self.logger.info(\"BEAB Tool logic initialized (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"BrandEquityAwareBiddingTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    async def _get_brand_equity_score(self, context: Dict, trace_id: Optional[str]) -> float:\n",
        "        \"\"\" Fetches brand equity score via KG Tool API proxy. \"\"\"\n",
        "        # --- TODO: Define REAL Cypher query for brand equity ---\n",
        "        query = \"\"\"\n",
        "        // Placeholder Query: Replace with actual logic based on KG schema\n",
        "        // Example: Look for brand sentiment or loyalty scores related to context\n",
        "        OPTIONAL MATCH (p:Product {id: $product_id})<-[:INTERACTED_WITH]-(u:User)-[:HAS_SENTIMENT]->(s:Sentiment)\n",
        "        WITH avg(s.score) as avg_sentiment\n",
        "        OPTIONAL MATCH (cust:Customer {id: $customer_id})\n",
        "        WITH coalesce(avg_sentiment, 0.5) as sentiment_score, coalesce(cust.loyalty_tier, 1) as loyalty_tier\n",
        "        RETURN sentiment_score * (1 + (loyalty_tier - 1) * 0.1) AS value // Combine metrics\n",
        "        \"\"\"\n",
        "        params = {\"product_id\": context.get(\"product_id\"), \"customer_id\": context.get(\"customer_id\")} # Example parameters\n",
        "        # --- End TODO ---\n",
        "        default_equity = 0.7\n",
        "        try:\n",
        "            if not self.kg_tool: raise RuntimeError(\"KG Tool proxy unavailable.\")\n",
        "            kg_request = {\"payload\": {\"query\": query, \"parameters\": params}, \"trace_id\": trace_id, \"request_id\": f\"kg_get_equity_{trace_id}\"}\n",
        "            kg_response = await self.kg_tool.execute_query(request=kg_request) # Call API proxy\n",
        "            if kg_response.get(\"status\") == \"success\" and (results := kg_response.get(\"payload\", {}).get(\"results\")) and results[0] is not None and 'value' in results[0]:\n",
        "                return float(results[0]['value'])\n",
        "            logger.warning(f\"Failed to get brand equity from KG Tool API, using default {default_equity}. Response: {kg_response}\")\n",
        "            return default_equity\n",
        "        except Exception as e:\n",
        "            logger.error(f\"KG Tool API call failed for equity: {e}\", exc_info=True)\n",
        "            return default_equity\n",
        "\n",
        "    async def _get_roas_prediction(self, context: Dict, trace_id: Optional[str]) -> float:\n",
        "        \"\"\" Gets ROAS prediction via MoE Registry/Invoker API proxies. \"\"\"\n",
        "        try:\n",
        "            if not self.moe_registry or not self.expert_invoker: raise RuntimeError(\"MoE Registry or Expert Invoker proxy unavailable.\")\n",
        "            # Find the appropriate forecasting expert\n",
        "            expert_id = await self.moe_registry.find_expert_for_task(task_type=\"forecasting\", domain=\"roas\", context=context) # MoE API Call via proxy\n",
        "            if not expert_id:\n",
        "                logger.warning(f\"No ROAS forecaster expert found via MoE Registry, using fallback {self.default_roas_fallback}.\")\n",
        "                return self.default_roas_fallback\n",
        "\n",
        "            expert_details = await self.moe_registry.get_expert_details(expert_id) # MoE API Call via proxy\n",
        "            expert_endpoint = expert_details.get(\"endpoint\") if expert_details else None\n",
        "            if not expert_endpoint:\n",
        "                logger.warning(f\"Endpoint not found for ROAS expert '{expert_id}', using fallback {self.default_roas_fallback}.\")\n",
        "                return self.default_roas_fallback\n",
        "\n",
        "            # Prepare input for the expert model\n",
        "            # --- TODO: Define the actual features needed by the ROAS model ---\n",
        "            model_input_payload = {\"features\": context.get(\"campaign_features\", {})} # Example input\n",
        "            # --- End TODO ---\n",
        "\n",
        "            # Call Expert Invoker API proxy\n",
        "            invoker_request = {\n",
        "                \"payload\": {\"endpoint\": expert_endpoint, \"data\": model_input_payload},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"invoker_roas_{trace_id}\"\n",
        "            }\n",
        "            invoker_response = await self.expert_invoker.invoke(request=invoker_request) # Call API proxy\n",
        "\n",
        "            if invoker_response.get(\"status\") == \"success\":\n",
        "                result_payload = invoker_response.get(\"payload\", {})\n",
        "                # --- TODO: Adapt parsing based on actual expert model output format ---\n",
        "                prediction = result_payload.get(\"prediction\")\n",
        "                if isinstance(prediction, list) and prediction: return float(prediction[0])\n",
        "                elif isinstance(prediction, (int, float)): return float(prediction)\n",
        "                # --- End TODO ---\n",
        "                else: logger.warning(f\"ROAS expert '{expert_id}' returned invalid prediction format: {prediction}, using fallback.\")\n",
        "            else:\n",
        "                logger.warning(f\"Expert Invoker API call failed for ROAS expert '{expert_id}': {invoker_response.get('error_details')}, using fallback.\")\n",
        "\n",
        "            return self.default_roas_fallback\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Async ROAS prediction via MoE/Invoker API failed: {e}\", exc_info=True)\n",
        "            return self.default_roas_fallback\n",
        "\n",
        "    async def calculate_adjusted_bid(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Calculates bid async. Expects/Returns MIZ OKI payload. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); base_bid = payload.get(\"base_bid\", 0.05); context = payload.get(\"context\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # Basic input validation\n",
        "        try: base_bid = float(base_bid)\n",
        "        except (ValueError, TypeError): errors.append({\"code\": \"INVALID_BASE_BID\", \"message\": \"'base_bid' must be a number.\"})\n",
        "        if not isinstance(context, dict): errors.append({\"code\": \"INVALID_CONTEXT\", \"message\": \"'context' must be a dictionary.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        decision_id = f\"beab_{context.get('campaign_id', 'unknown')}_{uuid.uuid4().hex[:8]}\"\n",
        "        status = \"pending\"; response_payload = None\n",
        "        log_details = { # Details for logging and potential XAI record\n",
        "            \"decision_id\": decision_id, \"type\": \"bid_adjustment\", \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"context_preview\": {k:v for k,v in context.items() if k in ['campaign_id', 'ad_group_id', 'keyword']}, # Log key context items\n",
        "            \"base_bid\": base_bid, \"status\": \"pending\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Fetch predictions concurrently\n",
        "            predicted_roas, brand_equity = await asyncio.gather(\n",
        "                self._get_roas_prediction(context, trace_id),\n",
        "                self._get_brand_equity_score(context, trace_id)\n",
        "            )\n",
        "\n",
        "            # Calculate adjustment factors\n",
        "            roas_target = self.config.business_impact.roas_target\n",
        "            roas_factor = 1.0 + (predicted_roas - roas_target) / roas_target if roas_target > 0 else 1.0\n",
        "            equity_factor = 1.0 + (brand_equity - 0.7) * self.equity_weight # Adjust weight based on business strategy\n",
        "\n",
        "            # Calculate final bid, ensuring it meets minimum threshold\n",
        "            adjusted_bid = max(self.min_bid_threshold, base_bid * roas_factor * equity_factor)\n",
        "\n",
        "            log_details.update({\n",
        "                \"predicted_roas\": predicted_roas, \"brand_equity\": brand_equity,\n",
        "                \"roas_target\": roas_target, \"roas_factor\": roas_factor,\n",
        "                \"equity_weight\": self.equity_weight, \"equity_factor\": equity_factor,\n",
        "                \"adjusted_bid\": adjusted_bid, \"status\": \"success\"\n",
        "            })\n",
        "            status = \"success\"\n",
        "            response_payload = log_details # Return the calculation details\n",
        "\n",
        "            # Record decision via XAI Tool API proxy (fire-and-forget)\n",
        "            if self.xai and hasattr(self.xai, 'record_decision'):\n",
        "                 xai_record = {\n",
        "                     \"decision_id\": decision_id, \"component\": \"BrandEquityAwareBiddingTool\",\n",
        "                     \"timestamp\": log_details[\"timestamp\"],\n",
        "                     \"context\": context, # Full context\n",
        "                     \"inputs\": {\"base_bid\": base_bid},\n",
        "                     \"decision\": {\"adjusted_bid\": adjusted_bid},\n",
        "                     \"outputs\": log_details, # Include intermediate calculations\n",
        "                     \"trace_id\": trace_id\n",
        "                 }\n",
        "                 xai_request = {\"payload\": {\"record\": xai_record}, \"trace_id\": trace_id}\n",
        "                 asyncio.create_task(self.xai.record_decision(request=xai_request)) # Call API proxy async\n",
        "\n",
        "        except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"BID_CALC_ERROR\", \"message\": str(e)})\n",
        "            log_details.update({\"status\": \"failed\", \"error\": str(e), \"adjusted_bid\": base_bid}) # Log failure, return base bid\n",
        "            logger.error(f\"BEAB Tool: Failed to calculate adjusted bid: {e}\", exc_info=True)\n",
        "            response_payload = log_details # Return log even on failure\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        # --- TODO: Persist log_details via KG Tool API or logging service if needed beyond XAI record ---\n",
        "        return response\n",
        "\n",
        "    async def execute_bid_adjustment(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "         \"\"\" Executes bid adjustment via Ads Platform Tool API proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "         start_time = time.monotonic(); errors = []\n",
        "         # Parse MIZ OKI input\n",
        "         payload = input_data.get(\"payload\", {}); bid_details = payload.get(\"bid_details\", {}) # Expects dict like {\"platform\": \"google_ads\", \"campaign_id\": \"...\", \"ad_group_id\": \"...\", \"new_bid\": 1.23}\n",
        "         trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "         if not self.ads_platform_tool: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"Ads Platform Tool proxy unavailable.\"})\n",
        "         if not isinstance(bid_details, dict) or not bid_details.get(\"new_bid\") or not bid_details.get(\"platform\"):\n",
        "             errors.append({\"code\": \"MISSING_DATA\", \"message\": \"Missing required bid details (platform, new_bid, identifiers) in payload.\"})\n",
        "         if errors:\n",
        "             response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "             response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "             return response\n",
        "\n",
        "         status = \"pending\"; response_payload = None\n",
        "         try:\n",
        "              # Call Ads Platform Tool API proxy\n",
        "              ads_request = {\n",
        "                  \"payload\": bid_details, # Pass details needed by the Ads tool\n",
        "                  \"trace_id\": trace_id, \"request_id\": f\"ads_exec_bid_{request_id}\"\n",
        "              }\n",
        "              ads_response = await self.ads_platform_tool.adjust_bid(request=ads_request) # Call proxy method\n",
        "\n",
        "              if ads_response.get(\"status\") == \"success\":\n",
        "                   status = \"success\"\n",
        "                   response_payload = ads_response.get(\"payload\", {}) # Return payload from Ads tool\n",
        "                   logger.info(f\"Successfully executed bid adjustment via Ads Tool API proxy for {bid_details.get('campaign_id')}.\")\n",
        "              else:\n",
        "                  raise RuntimeError(f\"Ads Platform Tool API proxy failed: {ads_response.get('error_details')}\")\n",
        "\n",
        "         except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"ADS_API_ERROR\", \"message\": str(e)})\n",
        "             logger.error(f\"Failed to execute bid adjustment via Ads Tool API proxy: {e}\", exc_info=True)\n",
        "\n",
        "         response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "# --- Hyperdimensional Personalization Tool (Reworked Async) ---\n",
        "class HyperdimensionalPersonalizationTool:\n",
        "    \"\"\" Generates personalization async via deployed Tool APIs proxies. Deployed as a service. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any, nn_tool_proxy: Any, moe_registry_proxy: Any, expert_invoker_proxy: Any, fm_client_proxy: Optional[Any] = None, crm_tool_proxy: Optional[Any] = None):\n",
        "        if not all([config, kg_tool_proxy, nn_tool_proxy, moe_registry_proxy, expert_invoker_proxy]):\n",
        "            raise InitializationError(\"HPTool requires config and proxies for KG, NN, MoE Registry, and Expert Invoker.\")\n",
        "        self.config = config\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.nn_tool = nn_tool_proxy\n",
        "        self.moe_registry = moe_registry_proxy\n",
        "        self.expert_invoker = expert_invoker_proxy\n",
        "        self.fm_client = fm_client_proxy # Optional for content generation\n",
        "        self.crm_tool = crm_tool_proxy     # Optional for CRM updates\n",
        "        self.default_rec_engine_alias = \"personalization_rec_v2\" # Example alias\n",
        "        self.default_content_gen_alias = config.foundation_models.defaults.llama4_scout\n",
        "        self.logger = logging.getLogger('MIZ-OKI.HyperPersonalizationTool')\n",
        "        self.logger.info(\"HyperPersonalization Tool logic initialized (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"HyperdimensionalPersonalizationTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    async def _get_user_profile_and_embedding(self, user_id: str, trace_id: Optional[str]) -> Tuple[Optional[Dict], Optional[List[float]]]:\n",
        "        \"\"\" Gets profile via KG Tool API proxy and embedding via NN/FM Client Tool API proxy. \"\"\"\n",
        "        if not self.kg_tool or not self.nn_tool:\n",
        "            logger.error(\"HP: Missing KG or NN Tool proxy for profile/embedding retrieval.\")\n",
        "            return None, None\n",
        "        user_data = None; embedding = None\n",
        "        try:\n",
        "            # Call KG Tool API proxy to get user entity data\n",
        "            kg_request = {\"payload\": {\"mizId\": user_id}, \"trace_id\": trace_id, \"request_id\": f\"kg_get_user_{user_id}\"}\n",
        "            kg_response = await self.kg_tool.get_entity_endpoint(request=kg_request) # Call API proxy method\n",
        "            user_data = kg_response.get(\"payload\", {}).get(\"entity_data\") if kg_response.get(\"status\") == \"success\" else None\n",
        "\n",
        "            if not user_data:\n",
        "                logger.warning(f\"HP: User profile not found in KG for user_id: {user_id}\")\n",
        "                return None, None\n",
        "\n",
        "            # --- Construct text representation from user_data for embedding ---\n",
        "            profile_parts = [f\"Segment: {user_data.get('segment', 'Unknown')}\", f\"Interests: {user_data.get('interests', [])}\"]\n",
        "            # Add recent activity summary if available\n",
        "            # profile_parts.append(f\"Recent: {user_data.get('recent_activity_summary', '')}\")\n",
        "            profile_text = \". \".join(p for p in profile_parts if p)\n",
        "            if not profile_text: profile_text = f\"User {user_id}\" # Fallback\n",
        "            # --- End Construct ---\n",
        "\n",
        "            # Call NN/FM Client Tool API proxy for embedding\n",
        "            nn_request = {\n",
        "                \"payload\": {\"data\": profile_text, \"data_type\": \"user_profile\"}, # Pass text and type hint\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"nn_embed_user_{user_id}\"\n",
        "            }\n",
        "            nn_response = await self.nn_tool.get_embedding(input_data=nn_request) # Call API proxy method\n",
        "            embedding = nn_response.get(\"payload\", {}).get(\"embedding\") if nn_response.get(\"status\") == \"success\" else None\n",
        "\n",
        "            if embedding is None:\n",
        "                logger.warning(f\"HP: Failed to get embedding for user {user_id} via NN/FM Tool API.\")\n",
        "\n",
        "            return user_data, embedding\n",
        "        except Exception as e:\n",
        "            logger.error(f\"HP: Error getting profile/embedding for {user_id} via API proxies: {e}\", exc_info=True)\n",
        "            return user_data, embedding # Return potentially partial results\n",
        "\n",
        "    async def get_personalized_recommendations(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Generates recommendations via MoE Invoker API proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); user_id = payload.get(\"user_id\"); k = payload.get(\"k\", 5); context = payload.get(\"context\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not user_id: errors.append({\"code\": \"MISSING_USER_ID\", \"message\": \"'user_id' is required.\"})\n",
        "        if not self.moe_registry or not self.expert_invoker: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"MoE Registry or Expert Invoker proxy unavailable.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        recommendations = []; status = \"pending\"; response_payload = None\n",
        "        try:\n",
        "            profile, embedding = await self._get_user_profile_and_embedding(user_id, trace_id)\n",
        "            if embedding is None: raise ValueError(f\"No profile/embedding found for user {user_id}.\")\n",
        "\n",
        "            # Find recommendation expert via MoE Registry API proxy\n",
        "            expert_id = await self.moe_registry.find_expert_for_task(task_type=\"recommendation\", domain=\"e-commerce\", context={\"user_id\": user_id}) # Example domain\n",
        "            if not expert_id: raise RuntimeError(f\"Recommendation expert not found via MoE Registry.\")\n",
        "\n",
        "            expert_details = await self.moe_registry.get_expert_details(expert_id)\n",
        "            expert_endpoint = expert_details.get(\"endpoint\") if expert_details else None\n",
        "            if not expert_endpoint: raise RuntimeError(f\"Endpoint not found for recommendation expert '{expert_id}'.\")\n",
        "\n",
        "            # --- TODO: Fetch item catalog context if needed by the expert model ---\n",
        "            # item_catalog = await self._fetch_item_catalog(context)\n",
        "            # --- End TODO ---\n",
        "\n",
        "            # Prepare input for the recommendation expert model\n",
        "            expert_input_payload = {\n",
        "                \"user_id\": user_id,\n",
        "                \"user_embedding\": embedding,\n",
        "                \"num_recommendations\": k,\n",
        "                \"context\": context, # Pass additional context (e.g., current page, time)\n",
        "                # \"item_catalog\": item_catalog # Pass item data if needed\n",
        "            }\n",
        "\n",
        "            # Call Expert Invoker API proxy\n",
        "            invoker_request = {\n",
        "                \"payload\": {\"endpoint\": expert_endpoint, \"data\": expert_input_payload},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"invoker_rec_{request_id}\"\n",
        "            }\n",
        "            invoker_response = await self.expert_invoker.invoke(request=invoker_request) # Call API proxy\n",
        "\n",
        "            if invoker_response.get(\"status\") == \"success\":\n",
        "                result_payload = invoker_response.get(\"payload\", {})\n",
        "                # --- TODO: Adapt parsing based on actual expert model output format ---\n",
        "                recommendations = result_payload.get(\"recommendations\", []) # Expecting a list of item IDs or objects\n",
        "                # --- End TODO ---\n",
        "                status = \"success\"; response_payload = {\"recommendations\": recommendations[:k]}\n",
        "                logger.info(f\"HP Recs: Generated {len(recommendations)} recs for {user_id} via MoE API proxy '{expert_id}'.\")\n",
        "            else:\n",
        "                raise RuntimeError(f\"Expert Invoker API call failed for rec expert '{expert_id}': {invoker_response.get('error_details')}\")\n",
        "\n",
        "        except (ValueError, RuntimeError) as vr_e:\n",
        "             status = \"failed\"; errors.append({\"code\": \"REC_FAILED\", \"message\": str(vr_e)}); logger.warning(f\"HP Recs: Recommendation generation failed for {user_id}: {vr_e}\")\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": str(e)}); logger.error(f\"HP Recs: Recommendation generation failed unexpectedly for {user_id}: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    async def generate_personalized_content(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Generates content via FM Client API proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); user_id = payload.get(\"user_id\"); content_type = payload.get(\"content_type\", \"email_subject\"); base_content = payload.get(\"base_content\"); context = payload.get(\"context\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not user_id or not base_content: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'user_id' and 'base_content' are required.\"})\n",
        "        if not self.fm_client: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"FM Client Tool proxy unavailable.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        personalized_content = base_content # Default\n",
        "        status = \"pending\"; response_payload = {\"personalized_content\": base_content}; response_metadata = {} # Default payload\n",
        "\n",
        "        try:\n",
        "            profile, _ = await self._get_user_profile_and_embedding(user_id, trace_id)\n",
        "            if not profile: raise ValueError(f\"No profile found for user {user_id}.\")\n",
        "\n",
        "            # Construct a concise profile summary for the prompt\n",
        "            profile_summary = json.dumps({\n",
        "                k: profile.get(k) for k in ['name', 'segment', 'interests', 'recent_purchase_category'] if profile.get(k) # Example fields\n",
        "            }, default=str, separators=(',', ':'))\n",
        "\n",
        "            # Construct prompt for personalization\n",
        "            prompt = f\"\"\"Personalize the following content for the user based on their profile and the context.\n",
        "User Profile: {profile_summary}\n",
        "Context: {json.dumps(context, default=str)}\n",
        "Content Type: {content_type}\n",
        "Base Content: \"{base_content}\"\n",
        "Personalized Content:\"\"\"\n",
        "\n",
        "            model_alias = self.default_content_gen_alias\n",
        "            logger.debug(f\"HP Content Gen Prompt (first 200): {prompt[:200]}...\")\n",
        "\n",
        "            # Call FM Client API proxy\n",
        "            fm_request = {\n",
        "                \"payload\": {\"prompt\": prompt, \"model_alias\": model_alias, \"temperature\": 0.7, \"max_tokens\": 256}, # Adjust params as needed\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"fm_hp_content_{request_id}\"\n",
        "            }\n",
        "            fm_response = await self.fm_client.generate_text(input_data=fm_request) # Call API proxy\n",
        "\n",
        "            if fm_response.get(\"status\") == \"success\":\n",
        "                generated_text = fm_response.get(\"payload\",{}).get(\"generated_text\")\n",
        "                if generated_text:\n",
        "                    personalized_content = generated_text.strip().strip('\"') # Clean up output\n",
        "                    status = \"success\"\n",
        "                else:\n",
        "                    logger.warning(f\"HP Content Gen: FM Client API proxy returned empty content for {user_id}.\")\n",
        "                    status = \"success_no_change\" # Indicate no change was made\n",
        "                response_metadata = fm_response.get(\"metadata\", {}) # Get metadata from FM call\n",
        "            else:\n",
        "                raise RuntimeError(f\"FM Client API proxy failed: {fm_response.get('error_details')}\")\n",
        "\n",
        "            response_payload[\"personalized_content\"] = personalized_content\n",
        "\n",
        "        except (ValueError, RuntimeError) as vr_e:\n",
        "             status = \"failed\"; errors.append({\"code\": \"CONTENT_GEN_FAILED\", \"message\": str(vr_e)}); logger.warning(f\"HP Content Gen: Failed for {user_id}: {vr_e}\")\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": str(e)}); logger.error(f\"HP Content Gen: Failed unexpectedly for {user_id}: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        # Add processing time, potentially merging with FM metadata if it exists\n",
        "        proc_duration = (time.monotonic() - start_time) * 1000\n",
        "        response[\"metadata\"] = response_metadata # Start with FM metadata\n",
        "        response[\"metadata\"][\"hp_tool_processing_duration_ms\"] = proc_duration\n",
        "        return response\n",
        "\n",
        "    async def update_crm_segment(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "         \"\"\" Updates CRM segment via CRM Tool API proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "         start_time = time.monotonic(); errors = []\n",
        "         # Parse MIZ OKI input\n",
        "         payload = input_data.get(\"payload\", {}); user_id = payload.get(\"user_id\"); segment = payload.get(\"predicted_segment\")\n",
        "         trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "         if not user_id or not segment: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'user_id' and 'predicted_segment' are required.\"})\n",
        "         if not self.crm_tool: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"CRM Tool proxy unavailable.\"})\n",
        "         if errors:\n",
        "             response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "             response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "             return response\n",
        "\n",
        "         status = \"pending\"; response_payload = None\n",
        "         try:\n",
        "              # Call CRM Tool API proxy\n",
        "              crm_request = {\n",
        "                  \"payload\": {\"customer_id\": user_id, \"segment\": segment}, # Adapt payload keys as needed by CRM tool\n",
        "                  \"trace_id\": trace_id, \"request_id\": f\"crm_update_seg_{request_id}\"\n",
        "              }\n",
        "              crm_response = await self.crm_tool.update_customer_segment(request=crm_request) # Call proxy method\n",
        "\n",
        "              if crm_response.get(\"status\") == \"success\":\n",
        "                   status = \"success\"; response_payload = crm_response.get(\"payload\", {})\n",
        "                   logger.info(f\"Successfully updated CRM segment for {user_id} via API proxy.\")\n",
        "              else:\n",
        "                   raise RuntimeError(f\"CRM Tool API proxy failed: {crm_response.get('error_details')}\")\n",
        "\n",
        "         except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"CRM_API_ERROR\", \"message\": str(e)})\n",
        "             logger.error(f\"Failed to update CRM segment for {user_id} via API proxy: {e}\", exc_info=True)\n",
        "\n",
        "         response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "         response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "         return response\n",
        "\n",
        "# --- Adaptive Workflow Evolution Service (Reworked - Uses REAL Vertex Client Proxies) ---\n",
        "class AdaptiveWorkflowEvolutionService:\n",
        "    \"\"\" Defines, initiates, monitors, and adapts Vertex AI Workflows using REAL client library proxies. Deployed as a service. \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any, fm_client_proxy: Any,\n",
        "                 workflows_client_proxy: Optional[Any] = None, # Inject REAL client proxies\n",
        "                 executions_client_proxy: Optional[Any] = None):\n",
        "        if not all([config, kg_tool_proxy, fm_client_proxy]):\n",
        "             raise InitializationError(\"AWEService requires config and proxies for KG and FM tools.\")\n",
        "        self.config = config\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.fm_client = fm_client_proxy\n",
        "        self.project = config.gcp.project_id\n",
        "        self.location = config.gcp.region\n",
        "        self.wf_client = workflows_client_proxy # Use injected client proxy\n",
        "        self.exec_client = executions_client_proxy # Use injected client proxy\n",
        "        # TODO: Use persistent storage (e.g., GCS, DB) for definitions cache in production\n",
        "        self.workflow_definitions_cache = TTLCache(maxsize=100, ttl=3600) # 1 hour TTL cache\n",
        "        self.logger = logging.getLogger('MIZ-OKI.AWEService')\n",
        "        if not self.wf_client or not self.exec_client:\n",
        "            self.logger.critical(\"AWE Service cannot function: Vertex AI Workflow/Execution client proxies missing.\")\n",
        "        elif not VERTEX_WORKFLOWS_SDK_AVAILABLE:\n",
        "             self.logger.warning(\"AWE Service initialized, but Vertex Workflows SDK seems unavailable. Functionality limited.\")\n",
        "        self.logger.info(\"Adaptive Workflow Evolution Service logic initialized (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"AdaptiveWorkflowEvolutionService\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    async def _get_workflow_parent(self) -> str:\n",
        "        \"\"\"Constructs the parent path for workflow resources.\"\"\"\n",
        "        if not self.project or not self.location: raise ValueError(\"GCP Project ID/Location missing.\")\n",
        "        return f\"projects/{self.project}/locations/{self.location}\"\n",
        "\n",
        "    async def define_workflow(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Defines/Updates Vertex AI Workflow via client proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); workflow_id = payload.get(\"workflow_id\"); definition = payload.get(\"definition_source_code\"); description = payload.get(\"description\"); labels = payload.get(\"labels\")\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not self.wf_client or not VERTEX_WORKFLOWS_SDK_AVAILABLE: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"Vertex Workflows client proxy/SDK unavailable.\"})\n",
        "        if not workflow_id or not definition: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'workflow_id' and 'definition_source_code' required.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        self.logger.info(f\"AWE: Defining/Updating Vertex AI Workflow: {workflow_id}\")\n",
        "        status = \"pending\"; response_payload = None\n",
        "        try:\n",
        "            parent = await self._get_workflow_parent()\n",
        "            workflow_name = f\"{parent}/workflows/{workflow_id}\"\n",
        "            # Construct the Workflow object using the SDK's type\n",
        "            workflow_obj_dict = {\n",
        "                \"name\": workflow_name,\n",
        "                \"description\": description or f\"MIZ OKI Managed Workflow: {workflow_id}\",\n",
        "                \"source_contents\": definition,\n",
        "                \"labels\": labels or {\"miz_oki_managed\": \"true\", \"miz_oki_version\": self.config.miz_oki_schema_version}\n",
        "            }\n",
        "            workflow_proto = workflows_v1.Workflow()\n",
        "            json_format.ParseDict(workflow_obj_dict, workflow_proto._pb) # Use _pb for proto access\n",
        "\n",
        "            action = \"unknown\"\n",
        "            try:\n",
        "                 # Try update first - Check existence using get_workflow\n",
        "                 get_request = workflows_v1.GetWorkflowRequest(name=workflow_name)\n",
        "                 await self.wf_client.get_workflow(request=get_request) # Call proxy method\n",
        "                 # If get succeeds, update\n",
        "                 mask = field_mask_pb2.FieldMask(paths=[\"description\", \"source_contents\", \"labels\"])\n",
        "                 update_request = workflows_v1.UpdateWorkflowRequest(workflow=workflow_proto, update_mask=mask)\n",
        "                 operation = await self.wf_client.update_workflow(request=update_request) # Call proxy method\n",
        "                 # Wait for Long-Running Operation (LRO) completion\n",
        "                 # The result() method might block, consider using polling or callbacks for truly async\n",
        "                 await asyncio.to_thread(operation.result, timeout=180)\n",
        "                 action = \"updated\"\n",
        "                 self.logger.info(f\"AWE: Workflow '{workflow_id}' updated successfully.\")\n",
        "            except google_api_exceptions.NotFound:\n",
        "                 # Create if not found\n",
        "                 create_request = workflows_v1.CreateWorkflowRequest(parent=parent, workflow=workflow_proto, workflow_id=workflow_id)\n",
        "                 operation = await self.wf_client.create_workflow(request=create_request) # Call proxy method\n",
        "                 await asyncio.to_thread(operation.result, timeout=180) # Wait for LRO completion\n",
        "                 action = \"created\"\n",
        "                 self.logger.info(f\"AWE: Workflow '{workflow_id}' created successfully.\")\n",
        "\n",
        "            self.workflow_definitions_cache[workflow_id] = definition # Update cache\n",
        "            status = \"success\"\n",
        "            response_payload = {\"workflow_id\": workflow_id, \"action\": action, \"workflow_name\": workflow_name}\n",
        "\n",
        "        except google_api_exceptions.GoogleAPIError as api_e:\n",
        "            status = \"api_error\"; errors.append({\"code\": \"VERTEX_API_ERROR\", \"message\": str(api_e)}); logger.error(f\"AWE API Error defining/updating Vertex Workflow '{workflow_id}': {api_e}\", exc_info=True)\n",
        "        except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"AWE_DEFINE_ERROR\", \"message\": str(e)}); logger.error(f\"AWE Error defining/updating Vertex Workflow '{workflow_id}': {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    async def execute_workflow(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Initiates Vertex AI workflow execution via client proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); workflow_id = payload.get(\"workflow_id\"); initial_context = payload.get(\"initial_context\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not self.exec_client or not VERTEX_WORKFLOWS_SDK_AVAILABLE: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"Vertex Executions client proxy/SDK unavailable.\"})\n",
        "        if not workflow_id: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'workflow_id' is required.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        status = \"pending\"; response_payload = None\n",
        "        run_id = f\"wf_exec_{workflow_id}_{uuid.uuid4().hex[:8]}\" # Internal run ID for logging\n",
        "        self.logger.info(f\"AWE: Starting Vertex AI workflow '{workflow_id}' (Run ID: {run_id}).\")\n",
        "        try:\n",
        "            parent = await self._get_workflow_parent()\n",
        "            workflow_name = f\"{parent}/workflows/{workflow_id}\"\n",
        "\n",
        "            # The initial_context IS the MIZ OKI payload for the *first step* of the target workflow\n",
        "            first_step_miz_oki_input = {\n",
        "                \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "                \"request_id\": f\"req_wf_start_{run_id}\", \"trace_id\": trace_id,\n",
        "                \"workflow_execution_id\": None, \"step_id\": \"start\", # Execution ID filled by Vertex\n",
        "                \"source_component\": \"AdaptiveWorkflowEvolutionService\",\n",
        "                \"target_component\": f\"Workflow:{workflow_id}:Step1\", # Conceptual target\n",
        "                \"payload\": initial_context or {}\n",
        "            }\n",
        "            execution_args = json.dumps(first_step_miz_oki_input) # Argument must be JSON string\n",
        "\n",
        "            execution_proto = Execution(argument=execution_args) # Use real proto type\n",
        "            request = CreateExecutionRequest(parent=workflow_name, execution=execution_proto) # Use real request type\n",
        "\n",
        "            # Call the REAL client proxy method\n",
        "            exec_response = await self.exec_client.create_execution(request=request)\n",
        "            execution_name = exec_response.name # Full execution name: projects/.../executions/...\n",
        "\n",
        "            status = \"success\"\n",
        "            response_payload = {\"execution_name\": execution_name, \"run_id\": run_id}\n",
        "            self.logger.info(f\"AWE Run {run_id}: Vertex AI execution created: {execution_name}\")\n",
        "\n",
        "        except google_api_exceptions.NotFound:\n",
        "            status = \"not_found\"; errors.append({\"code\": \"WORKFLOW_NOT_FOUND\", \"message\": f\"Workflow '{workflow_id}' not found.\"}); logger.error(f\"Workflow '{workflow_id}' not found.\")\n",
        "        except google_api_exceptions.GoogleAPIError as api_e:\n",
        "            status = \"api_error\"; errors.append({\"code\": \"VERTEX_API_ERROR\", \"message\": str(api_e)}); logger.error(f\"AWE API Error initiating workflow run for '{workflow_id}': {api_e}\", exc_info=True)\n",
        "        except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"AWE_EXECUTE_ERROR\", \"message\": str(e)}); logger.error(f\"AWE Error initiating workflow run for '{workflow_id}': {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    async def monitor_and_adapt(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Monitors workflow performance and triggers evolution. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); execution_id_to_monitor = payload.get(\"execution_id\"); workflow_id_to_monitor = payload.get(\"workflow_id\") # Can monitor specific exec or workflow\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        self.logger.info(f\"AWE: Running monitor & adapt cycle (Exec:{execution_id_to_monitor}, WF:{workflow_id_to_monitor})...\")\n",
        "        status = \"pending\"; response_payload = None; evolution_triggered = False\n",
        "\n",
        "        # --- TODO: Implement Real Monitoring Logic ---\n",
        "        # 1. Fetch execution history/metrics:\n",
        "        #    - Use `self.exec_client.list_executions` with filters (status=FAILED, duration > threshold).\n",
        "        #    - Query Cloud Logging for errors associated with workflow/execution IDs.\n",
        "        #    - Query KG Tool API proxy for business impact linked to workflow outcomes.\n",
        "        # 2. Analyze metrics against thresholds/baselines.\n",
        "        # 3. If issue detected:\n",
        "        #    a. Determine problematic workflow_id if only execution_id was given.\n",
        "        #    b. Prepare context for evolution (reason, metrics).\n",
        "        #    c. Trigger evolution asynchronously:\n",
        "        #       asyncio.create_task(self.evolve_workflow(problem_wf_id, evolution_context, trace_id))\n",
        "        #       evolution_triggered = True\n",
        "        # --- Simulation ---\n",
        "        if random.random() < 0.05: # Simulate issue detection\n",
        "            problem_wf_id = workflow_id_to_monitor or (execution_id_to_monitor.split('/workflows/')[1].split('/executions/')[0] if execution_id_to_monitor and '/workflows/' in execution_id_to_monitor else None)\n",
        "            if problem_wf_id:\n",
        "                 self.logger.warning(f\"AWE: Simulated issue with workflow '{problem_wf_id}'. Triggering evolution.\")\n",
        "                 evolution_context = {\"reason\": \"Simulated performance degradation\", \"trigger_trace_id\": trace_id}\n",
        "                 # Trigger evolution asynchronously (fire-and-forget for this monitor cycle)\n",
        "                 asyncio.create_task(self.evolve_workflow(problem_wf_id, evolution_context, trace_id))\n",
        "                 evolution_triggered = True\n",
        "            else: logger.warning(\"AWE: Simulated issue detected but could not determine workflow ID.\")\n",
        "        # --- End Simulation ---\n",
        "\n",
        "        status = \"success\" # Monitoring cycle completed\n",
        "        response_payload = {\"monitoring_complete\": True, \"evolution_triggered\": evolution_triggered}\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    async def evolve_workflow(self, workflow_id: str, context: Optional[Dict] = None, trace_id: Optional[str] = None):\n",
        "        \"\"\" Evolves workflow definition via FM Client API proxy and re-deploys via Vertex API proxy. \"\"\"\n",
        "        if not self.wf_client or not self.fm_client or not VERTEX_WORKFLOWS_SDK_AVAILABLE:\n",
        "            logger.error(f\"AWE: Cannot evolve workflow '{workflow_id}'. Client proxies/SDK unavailable.\"); return\n",
        "        self.logger.info(f\"AWE: Starting evolution process for workflow '{workflow_id}'...\")\n",
        "        context = context or {}\n",
        "        evolution_id = f\"evolve_{workflow_id}_{uuid.uuid4().hex[:6]}\"\n",
        "\n",
        "        try:\n",
        "            # 1. Get current definition via Vertex API proxy\n",
        "            parent = await self._get_workflow_parent()\n",
        "            workflow_name = f\"{parent}/workflows/{workflow_id}\"\n",
        "            get_request = workflows_v1.GetWorkflowRequest(name=workflow_name)\n",
        "            current_wf = await self.wf_client.get_workflow(request=get_request) # Call proxy method\n",
        "            current_source = current_wf.source_contents\n",
        "            self.workflow_definitions_cache[workflow_id] = current_source # Update cache\n",
        "\n",
        "            # 2. Analyze & Suggest Modifications via FM Client API proxy\n",
        "            analysis_prompt = f\"\"\"Analyze the following Vertex AI Workflow YAML definition for potential improvements based on the provided context.\n",
        "Workflow ID: {workflow_id}\n",
        "Reason for Evolution: {context.get('reason', 'General optimization request')}\n",
        "Current Workflow Source:\n",
        "```yaml\n",
        "{current_source}\n",
        "\n",
        "\n",
        " model_alias = self.config.foundation_models.defaults.llama4_maverick # Use powerful model for code generation\n",
        "        fm_request = {\n",
        "            \"payload\": {\"prompt\": analysis_prompt, \"model_alias\": model_alias, \"max_tokens\": 4096, \"temperature\": 0.2}, # Low temp for code\n",
        "            \"trace_id\": trace_id, \"request_id\": f\"fm_evolve_{evolution_id}\"\n",
        "        }\n",
        "        fm_response = await self.fm_client.generate_text(input_data=fm_request) # Call API proxy\n",
        "\n",
        "        if fm_response.get(\"status\") != \"success\":\n",
        "            raise RuntimeError(f\"AWE: LLM suggestion generation failed via FM Client API: {fm_response.get('error_details')}\")\n",
        "\n",
        "        generated_text = fm_response.get(\"payload\", {}).get(\"generated_text\", \"\")\n",
        "\n",
        "        # 3. Extract and Validate YAML\n",
        "        match = re.search(r\"```(?:yaml)?\\s*(.*?)\\s*```\", generated_text, re.DOTALL)\n",
        "        new_source = match.group(1).strip() if match else generated_text.strip() # Extract content within ```yaml ... ```\n",
        "\n",
        "        if not new_source or new_source == current_source.strip():\n",
        "            logger.info(f\"AWE: LLM suggested no changes for workflow '{workflow_id}'. Evolution cycle complete.\")\n",
        "            # --- TODO: Log no-change event ---\n",
        "            return # No changes needed\n",
        "\n",
        "        # --- TODO: Add YAML validation step here ---\n",
        "        try:\n",
        "            import yaml\n",
        "            yaml.safe_load(new_source)\n",
        "            logger.debug(\"Evolved YAML syntax is valid.\")\n",
        "        except ImportError:\n",
        "             logger.warning(\"PyYAML not installed. Skipping YAML validation for evolved workflow.\")\n",
        "        except Exception as yaml_e:\n",
        "             raise ValueError(f\"Generated YAML is invalid: {yaml_e}\")\n",
        "        # --- End TODO ---\n",
        "\n",
        "        # 4. Deploy Update via Vertex API proxy (Calls internal define_workflow method of this service)\n",
        "        self.logger.info(f\"AWE: Deploying LLM-suggested update for workflow '{workflow_id}'.\")\n",
        "        define_request = {\n",
        "            \"payload\": {\n",
        "                \"workflow_id\": workflow_id,\n",
        "                \"definition_source_code\": new_source,\n",
        "                \"description\": f\"Evolved by AWE: {context.get('reason', 'Auto-optimization')} ({datetime.now(datetime.timezone.utc).isoformat()})\",\n",
        "                \"labels\": {\"miz_oki_managed\": \"true\", \"miz_oki_version\": self.config.miz_oki_schema_version, \"awe_evolved\": \"true\"}\n",
        "            },\n",
        "            \"trace_id\": trace_id, \"request_id\": f\"awe_define_{evolution_id}\"\n",
        "        }\n",
        "        define_response = await self.define_workflow(input_data=define_request) # Call internal method\n",
        "\n",
        "        if define_response.get(\"status\") != \"success\":\n",
        "            raise RuntimeError(f\"Failed to deploy evolved workflow '{workflow_id}': {define_response.get('error_details')}\")\n",
        "\n",
        "        self.logger.info(f\"AWE: Successfully evolved and deployed workflow '{workflow_id}'.\")\n",
        "        # --- TODO: Log evolution event to KG or monitoring system ---\n",
        "        # Example: kg_request = {\"payload\": {\"entity_data\": {\"type\": \"WorkflowEvolutionEvent\", ...}}, \"trace_id\": trace_id}\n",
        "        # await self.kg_tool.add_entity_endpoint(request=kg_request)\n",
        "        # --- End TODO ---\n",
        "\n",
        "    except google_api_exceptions.NotFound:\n",
        "        logger.error(f\"AWE Evolve Error: Workflow '{workflow_id}' not found.\")\n",
        "    except ValueError as ve: # Catch validation errors\n",
        "         logger.error(f\"AWE Evolve Error for '{workflow_id}': {ve}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"AWE: Error evolving workflow '{workflow_id}': {e}\", exc_info=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "V9h58nPCqGpR",
        "outputId": "7905d910-e516-492c-c6c3-e5e47d85f17e"
      },
      "id": "V9h58nPCqGpR",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated triple-quoted string literal (detected at line 1025) (<ipython-input-11-a960b2712cbc>, line 958)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-a960b2712cbc>\"\u001b[0;36m, line \u001b[0;32m958\u001b[0m\n\u001b[0;31m    analysis_prompt = f\"\"\"Analyze the following Vertex AI Workflow YAML definition for potential improvements based on the provided context.\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 1025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell7.1\n",
        "--- Other Business App Tool Stubs (Reworked - Conceptual API Calls via Proxies) ---\n",
        "These tools would be deployed as separate services and called via MIZ OKI API.\n",
        "class RealTimeBiddingTool: \"\"\"Handles real-time bidding decisions. Deployed as a service.\"\"\" def init(self, beab_tool_proxy: Any, ads_platform_tool_proxy: Any, config: EnhancedConfig): if not config or not beab_tool_proxy: raise InitializationError(\"RTBTool requires config and BEAB Tool proxy.\") self.beab_tool = beab_tool_proxy self.ads_platform_tool = ads_platform_tool_proxy # Optional for immediate execution self.config = config self.logger = logging.getLogger('MIZ-OKI.RTBTool') self.logger.info(\"RTB Tool logic initialized.\")\n",
        "\n",
        "def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "    \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "    return {\n",
        "        \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "        \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "        \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "        \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "        \"source_component\": \"RealTimeBiddingTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "        \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "    }\n",
        "\n",
        "async def process_bid_request(self, input_data: Dict) -> Dict: # Expects MIZ OKI bid request payload\n",
        "    \"\"\"Processes a bid request, calculates bid, optionally executes.\"\"\"\n",
        "    start_time = time.monotonic(); errors = []\n",
        "    payload = input_data.get(\"payload\", {}); context = payload.get(\"context\", {}); base_bid = payload.get(\"bid_floor\", 0.05)\n",
        "    trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "    status = \"pending\"; response_payload = None\n",
        "\n",
        "    if not self.beab_tool: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"BEAB Tool proxy unavailable.\"})\n",
        "    if errors:\n",
        "        response = self._create_miz_oki_response(input_data, \"config_error\", errors=errors)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    try:\n",
        "        # 1. Calculate Adjusted Bid via BEAB Tool API Proxy\n",
        "        beab_request = {\n",
        "            \"payload\": {\"base_bid\": base_bid, \"context\": context},\n",
        "            \"trace_id\": trace_id, \"request_id\": f\"beab_calc_{request_id}\"\n",
        "        }\n",
        "        beab_response = await self.beab_tool.calculate_adjusted_bid(input_data=beab_request) # Call proxy\n",
        "        calc_log = beab_response.get(\"payload\", {}) # Get the log details from BEAB response\n",
        "        response_payload = {\"calculation_log\": calc_log} # Include BEAB log\n",
        "\n",
        "        if beab_response.get(\"status\") == \"success\":\n",
        "            adjusted_bid = calc_log.get(\"adjusted_bid\", 0)\n",
        "            # Decide whether to bid (e.g., if adjusted bid > floor)\n",
        "            should_bid = adjusted_bid > base_bid # Simple logic example\n",
        "            if should_bid:\n",
        "                status = \"success_bid_calculated\"\n",
        "                response_payload[\"bid_value\"] = adjusted_bid\n",
        "                logger.info(f\"RTB: Calculated bid {adjusted_bid:.4f} for request {request_id}.\")\n",
        "                # --- Optional: Execute bid immediately via Ads Platform Tool API Proxy ---\n",
        "                # if self.ads_platform_tool:\n",
        "                #     exec_payload = {**context, \"new_bid\": adjusted_bid, \"platform\": context.get(\"platform\")} # Adapt payload\n",
        "                #     exec_request = {\"payload\": exec_payload, \"trace_id\": trace_id, \"request_id\": f\"ads_exec_rtb_{request_id}\"}\n",
        "                #     exec_response = await self.ads_platform_tool.adjust_bid(request=exec_request)\n",
        "                #     response_payload[\"execution_status\"] = exec_response.get(\"status\")\n",
        "                #     response_payload[\"execution_details\"] = exec_response.get(\"payload\") or exec_response.get(\"error_details\")\n",
        "                #     if exec_response.get(\"status\") != \"success\": logger.warning(\"RTB: Immediate bid execution failed.\")\n",
        "                # --- End Optional Execution ---\n",
        "            else:\n",
        "                status = \"success_no_bid\"\n",
        "                response_payload[\"bid_value\"] = 0.0\n",
        "                logger.info(f\"RTB: No bid placed for request {request_id}. Adjusted bid {adjusted_bid:.4f} <= base {base_bid:.4f}.\")\n",
        "        else:\n",
        "            # Propagate error from BEAB tool\n",
        "            status = \"failed\"\n",
        "            errors.append({\"code\": \"BEAB_TOOL_ERROR\", \"message\": \"BEAB tool failed to calculate bid.\", \"details\": beab_response.get(\"error_details\")})\n",
        "\n",
        "    except Exception as e:\n",
        "        status = \"internal_error\"; errors.append({\"code\": \"RTB_ERROR\", \"message\": str(e)})\n",
        "        logger.error(f\"RTB Tool Error processing bid request {request_id}: {e}\", exc_info=True)\n",
        "\n",
        "    response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "    response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "    return response\n",
        "class AdOptimizationTool: \"\"\"Optimizes ad campaigns (targeting, budget, creative). Deployed as a service.\"\"\" def init(self, config: EnhancedConfig, moe_registry_proxy: Any, expert_invoker_proxy: Any, fm_client_proxy: Any, ads_platform_tool_proxy: Any, kg_tool_proxy: Any): if not all([config, moe_registry_proxy, expert_invoker_proxy, fm_client_proxy, ads_platform_tool_proxy, kg_tool_proxy]): raise InitializationError(\"AdOptTool requires config and proxies for MoE, Invoker, FM, Ads Platform, and KG.\") self.config = config; self.moe_registry = moe_registry_proxy; self.expert_invoker = expert_invoker_proxy; self.fm_client = fm_client_proxy; self.ads_platform_tool = ads_platform_tool_proxy; self.kg_tool = kg_tool_proxy self.logger = logging.getLogger('MIZ-OKI.AdOptTool') self.logger.info(\"Ad Optimization Tool logic initialized.\")\n",
        "\n",
        "def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "    \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "    return {\n",
        "        \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "        \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "        \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "        \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "        \"source_component\": \"AdOptimizationTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "        \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "    }\n",
        "\n",
        "async def optimize_campaign(self, input_data: Dict) -> Dict: # Expects MIZ OKI payload\n",
        "    \"\"\"Optimizes a specific campaign based on performance data and goals.\"\"\"\n",
        "    start_time = time.monotonic(); errors = []\n",
        "    payload = input_data.get(\"payload\", {}); campaign_id = payload.get(\"campaign_id\"); platform = payload.get(\"platform\")\n",
        "    trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "    if not campaign_id or not platform: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'campaign_id' and 'platform' are required.\"})\n",
        "    # Add dependency checks here\n",
        "    if errors:\n",
        "        response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    status = \"pending\"; response_payload = None; actions_taken = []\n",
        "    try:\n",
        "        # --- TODO: Implement full optimization logic ---\n",
        "        self.logger.info(f\"AdOpt Tool: Starting optimization for {platform}:{campaign_id} (Placeholder Logic)\")\n",
        "        # 1. Fetch Data: Call KG Tool API proxy to get campaign details, performance metrics, goals.\n",
        "        #    kg_request = {\"payload\": {\"mizId\": campaign_id}, \"trace_id\": trace_id}\n",
        "        #    kg_response = await self.kg_tool.get_entity_endpoint(request=kg_request)\n",
        "        #    campaign_data = kg_response.get(\"payload\", {}).get(\"entity_data\") if kg_response.get(\"status\") == \"success\" else {}\n",
        "\n",
        "        # 2. Analyze Performance: Compare metrics against goals. Maybe use an MoE expert via Invoker API proxy.\n",
        "        #    expert_id = await self.moe_registry.find_expert_for_task(task_type=\"performance_analysis\", domain=platform)\n",
        "        #    invoker_request = {\"payload\": {\"endpoint\": ..., \"data\": {\"campaign_data\": campaign_data}}, \"trace_id\": trace_id}\n",
        "        #    analysis_response = await self.expert_invoker.invoke(request=invoker_request)\n",
        "        #    analysis = analysis_response.get(\"payload\", {}) if analysis_response.get(\"status\") == \"success\" else {}\n",
        "\n",
        "        # 3. Decide Actions: Based on analysis, decide on budget, targeting, or creative changes.\n",
        "        budget_change = None; targeting_change = None; creative_needed = False\n",
        "        #    if analysis.get(\"performance_issue\") == \"under_budget\": budget_change = campaign_data.get(\"budget\", 100) * 1.1\n",
        "        #    if analysis.get(\"performance_issue\") == \"poor_targeting\": targeting_change = {\"new_audience\": \"segment_y\"}\n",
        "        #    if analysis.get(\"creative_fatigue\", False): creative_needed = True\n",
        "\n",
        "        # 4. Generate Creatives (if needed): Call FM Client API proxy.\n",
        "        if creative_needed:\n",
        "             fm_request = {\"payload\": {\"prompt\": f\"Generate 3 new ad text variations for campaign {campaign_id} about [product/service]\", \"model_alias\": \"llama4_scout\"}, \"trace_id\": trace_id}\n",
        "             fm_response = await self.fm_client.generate_text(input_data=fm_request)\n",
        "             if fm_response.get(\"status\") == \"success\":\n",
        "                 new_creatives = fm_response.get(\"payload\",{}).get(\"generated_text\")\n",
        "                 # TODO: Call Ads Platform API proxy to upload/create new creatives\n",
        "                 # ads_creative_request = {\"payload\": {\"platform\": platform, \"campaign_id\": campaign_id, \"creatives\": new_creatives}, \"trace_id\": trace_id}\n",
        "                 # creative_response = await self.ads_platform_tool.create_ad_creative(request=ads_creative_request)\n",
        "                 # if creative_response.get(\"status\") == \"success\": actions_taken.append({\"type\": \"create_creative\", \"details\": creative_response.get(\"payload\")})\n",
        "                 actions_taken.append({\"type\": \"generate_creative\", \"status\": \"generated\", \"text_preview\": str(new_creatives)[:50]}) # Placeholder action log\n",
        "\n",
        "        # 5. Execute Budget/Targeting Changes: Call Ads Platform API proxy.\n",
        "        if budget_change is not None:\n",
        "             ads_budget_request = {\"payload\": {\"platform\": platform, \"campaign_id\": campaign_id, \"new_budget\": budget_change}, \"trace_id\": trace_id}\n",
        "             ads_response = await self.ads_platform_tool.update_budget(request=ads_budget_request) # Call proxy\n",
        "             if ads_response.get(\"status\") == \"success\": actions_taken.append({\"type\": \"update_budget\", \"new_budget\": budget_change, \"status\": \"success\"})\n",
        "             else: actions_taken.append({\"type\": \"update_budget\", \"new_budget\": budget_change, \"status\": \"failed\", \"error\": ads_response.get(\"error_details\")})\n",
        "        # Add similar logic for targeting changes...\n",
        "\n",
        "        # --- End Placeholder Logic ---\n",
        "\n",
        "        logger.info(f\"AdOpt Tool: Completed optimization cycle for {campaign_id}. Actions: {len(actions_taken)}\")\n",
        "        status = \"success\" # Assume success if no critical error occurred during placeholder logic\n",
        "        response_payload = {\"campaign_id\": campaign_id, \"actions_taken\": actions_taken}\n",
        "\n",
        "    except Exception as e:\n",
        "        status = \"internal_error\"; errors.append({\"code\": \"ADOPT_ERROR\", \"message\": str(e)})\n",
        "        logger.error(f\"AdOpt Tool Error for {campaign_id}: {e}\", exc_info=True)\n",
        "\n",
        "    response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "    response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "    return response\n",
        "--- Other Business App Tool Stubs (Need full implementation as services/tools) ---\n",
        "class BusinessInsightsTool: \"\"\"Generates business insights and alerts. Deployed as a service.\"\"\" # Needs KG, FM proxies async def generate_predictive_alert(self, input_data: Dict) -> Dict: logger.warning(\"BI Tool generate_predictive_alert not implemented.\"); return {\"status\": \"not_implemented\"}\n",
        "\n",
        "class StrategyToExecutionBridgeTool: \"\"\"Translates strategy to executable actions/workflows. Deployed as a service.\"\"\" # Needs KG, FM, AWE proxies/clients async def translate_strategy(self, input_data: Dict) -> Dict: logger.warning(\"StrategyBridge Tool translate_strategy not implemented.\"); return {\"status\": \"not_implemented\"}\n",
        "\n",
        "--- Initialization (Conceptual - Tools instantiated by framework/orchestrator) ---\n",
        "This would happen in the main application entry point or service factory\n",
        "_ads_platform_tool_proxy: Optional[AdsPlatformApiToolProxy] = None\n",
        "_crm_tool_proxy: Optional[CrmApiToolProxy] = None\n",
        "_privacy_controls_tool: Optional[PrivacyControlsTool] = None\n",
        "_beab_tool: Optional[BrandEquityAwareBiddingTool] = None\n",
        "_hp_tool: Optional[HyperdimensionalPersonalizationTool] = None\n",
        "_awe_service: Optional[AdaptiveWorkflowEvolutionService] = None\n",
        "_rtb_tool: Optional[RealTimeBiddingTool] = None\n",
        "_ad_optimization_tool: Optional[AdOptimizationTool] = None\n",
        "async def initialize_business_apps():\n",
        "global _ads_platform_tool_proxy, _crm_tool_proxy, _privacy_controls_tool, _beab_tool, _hp_tool, _awe_service, _rtb_tool, _ad_optimization_tool\n",
        "if not _config_obj or not _real_dependencies:\n",
        "logger.critical(\"Cannot initialize Business Apps: Config or core dependencies missing.\")\n",
        "return\n",
        "try:\n",
        "_ads_platform_tool_proxy = AdsPlatformApiToolProxy(_config_obj, _config_obj.service_endpoints.ads_platform_api_endpoint)\n",
        "await _ads_platform_tool_proxy.initialize()\n",
        "_crm_tool_proxy = CrmApiToolProxy(_config_obj, _config_obj.service_endpoints.crm_api_endpoint)\n",
        "await _crm_tool_proxy.initialize()\n",
        "_privacy_controls_tool = PrivacyControlsTool(_config_obj) # Sync init ok\n",
        "_beab_tool = BrandEquityAwareBiddingTool(_config_obj, _kg_tool_proxy, _moe_registry_proxy, _expert_invoker_proxy, _xai_proxy, _ads_platform_tool_proxy)\n",
        "_hp_tool = HyperdimensionalPersonalizationTool(_config_obj, _kg_tool_proxy, _nn_tool_proxy, _moe_registry_proxy, _expert_invoker_proxy, _fm_client_proxy, _crm_tool_proxy)\n",
        "_awe_service = AdaptiveWorkflowEvolutionService(_config_obj, _kg_tool_proxy, _fm_client_proxy, _workflows_client_proxy, _workflow_executions_client_proxy) # Inject real clients\n",
        "_rtb_tool = RealTimeBiddingTool(_beab_tool, _ads_platform_tool_proxy)\n",
        "_ad_optimization_tool = AdOptimizationTool(_config_obj, _moe_registry_proxy, _expert_invoker_proxy, _fm_client_proxy, _ads_platform_tool_proxy, _kg_tool_proxy)\n",
        "# Initialize other tools...\n",
        "logger.info(\"Business Application Tools/Services initialized.\")\n",
        "except Exception as e:\n",
        "logger.critical(f\"Business Application Tools initialization failed: {e}\", exc_info=True)\n",
        "# Set all to None on failure\n",
        "_ads_platform_tool_proxy = _crm_tool_proxy = _privacy_controls_tool = _beab_tool = _hp_tool = _awe_service = _rtb_tool = _ad_optimization_tool = None\n",
        "async def cleanup_business_apps():\n",
        "if _ads_platform_tool_proxy: await _ads_platform_tool_proxy.cleanup()\n",
        "if _crm_tool_proxy: await _crm_tool_proxy.cleanup()\n",
        "# Add cleanup for other tools if needed\n",
        "print(\"\\n--- MIZ 3.0 Business Applications Layer Logic (Cell 7 - Reworked) ---\") print(\"AWE Service uses real Vertex AI Client proxies (if available). App Tools use real dependencies/proxies via MIZ OKI APIs.\") print(\"Requires implementation of AWE monitoring/evolution logic, App Tool logic & External API Tools.\") print(\"-----------------------------------------------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "L6rA8HL2qUKx",
        "outputId": "a0ee9f58-bdcb-4f0a-f015-747b42eb3752"
      },
      "id": "L6rA8HL2qUKx",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-12-64148d231762>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-64148d231762>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    --- Other Business App Tool Stubs (Reworked - Conceptual API Calls via Proxies) ---\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "**Cell 8: Learning Flows Implementation (Reworked)**\n",
        "\n",
        "*   **Original Purpose:** Define learning components (KD, CV, DRS, DRL), synchronous logic, placeholder dependencies, inline training.\n",
        "*   **Key Changes:** Logic refactored into async `Tool`/`Service` classes. KD, CV, DRS tools interact with dependencies (FM, LI, Optimizer, Pub/Sub) via injected *proxies/clients*. `DistributedRLManager` triggers *external* MLOps training via Pub/Sub client proxy. Shows async GCS pattern for buffer saving (requires `aio-gcsfs`). All public methods handle MIZ OKI payloads. State persistence (DRL buffer, CV queue state, etc.) needs explicit implementation using external stores (GCS, BQ, Firestore).\n",
        "*   **Reworked Code:**\n",
        "\n",
        "```python\n",
        "# Cell 8: Learning Flows Implementation (Reworked)\n",
        "# Status: KD, CV, DRS use real dependencies/proxies via MIZ OKI APIs.\n",
        "#         DRL Manager uses real PubSub proxy to trigger external MLOps training.\n",
        "#         Async GCS pattern shown (requires aio-gcsfs).\n",
        "#         Placeholders remain for drift/bias detection, DRL buffer persistence/loading.\n",
        "\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "from typing import Dict, Any, Optional, List, Union, Callable, Tuple\n",
        "from collections import deque, defaultdict, Counter # Added Counter\n",
        "import json\n",
        "import uuid\n",
        "import os # Added for GCS path joining\n",
        "import aiofiles # For async file operations if needed locally\n",
        "\n",
        "# --- Assume Real Tool/Client Dependencies are Injected/Available ---\n",
        "# These proxies represent API clients for other deployed MIZ OKI services or GCP clients.\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    # Proxies for other MIZ OKI Tool APIs\n",
        "    if 'foundation_model_client' not in globals(): raise NameError(\"foundation_model_client proxy not found\") # Cell 18 Proxy\n",
        "    if 'li_tool' not in globals(): raise NameError(\"li_tool (LearningIntegrationTool instance/proxy) not found\") # Cell 5 Proxy\n",
        "    if 'optimizer_tool' not in globals(): raise NameError(\"optimizer_tool (HolisticOptimizerTool instance/proxy) not found\") # Cell 5 Proxy\n",
        "\n",
        "    # Real/Mock Client for GCP Pub/Sub\n",
        "    if '_pubsub_client' not in globals(): raise NameError(\"_pubsub_client not found\") # Cell 8 needs this\n",
        "\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _fm_client_proxy = foundation_model_client\n",
        "    _li_tool_proxy = li_tool\n",
        "    _optimizer_tool_proxy = optimizer_tool\n",
        "    _pubsub_client_proxy = _pubsub_client # Use real/mock client proxy\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real/conceptual dependencies in Cell 8 (Reworked).\")\n",
        "\n",
        "    # Check for aio-gcsfs for async GCS operations\n",
        "    try:\n",
        "        import aio_gcsfs; import gcsfs\n",
        "        AIO_GCS_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        AIO_GCS_AVAILABLE = False\n",
        "        logging.warning(\"aio-gcsfs/gcsfs not installed. Async GCS operations will be simulated.\")\n",
        "\n",
        "except NameError as e:\n",
        "    logger.warning(f\"Dependency Error in Cell 8 ({e}). Using Mocks/Placeholders.\")\n",
        "    _real_dependencies = False\n",
        "    AIO_GCS_AVAILABLE = False\n",
        "    # --- Mock/Placeholder Setup ---\n",
        "    class MockFMClientTool: async def generate_text(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"generated_text\": \"Mock\"}}; async def analyze(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {'sentiment': 'positive'}}\n",
        "    class MockLITool: async def integrate_learning(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"integration_id\": \"mock_li\"}}\n",
        "    class MockOptimizerTool: async def get_current_objective_priorities(self, input_data): await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"priorities\": {'mock': 0.5}}}\n",
        "    class MockPubSubClient: async def publish(self, topic, data_bytes): await asyncio.sleep(0.01); return f\"msg_{uuid.uuid4().hex[:8]}\"\n",
        "    # Define minimal config if needed\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ:\n",
        "        from dataclasses import dataclass, field\n",
        "        @dataclass class MockGcpConfig: project_id:Optional[str]=\"mock-proj\"; region:str=\"mock-region\"; gcs_bucket_name:Optional[str]=\"mock-bucket\"\n",
        "        @dataclass class MockFmDefaults: llama4_maverick: str = \"mock-llama\"; feedback_analyzer_model: str = \"mock-analyzer\"\n",
        "        @dataclass class MockFmConfig: defaults: MockFmDefaults = field(default_factory=MockFmDefaults)\n",
        "        @dataclass class MockLearningFlowsConfig: kd: Dict = field(default_factory=lambda: {\"output_gcs_prefix\": \"kd/\"}); cv: Dict = field(default_factory=lambda: {\"feedback_queue_maxsize\": 100, \"monitoring_interval_seconds\": 60, \"feedback_analyzer_model_alias\": \"mock-analyzer\"}); drs: Dict = field(default_factory=lambda: {'base_weights': {}, \"update_interval_seconds\": 60, \"objective_influence_factor\": 0.3}); drl: Dict = field(default_factory=lambda: {\"buffer_size\": 100, \"buffer_save_interval_sec\": 60, \"min_buffer_for_train\": 10, \"buffer_gcs_prefix\": \"rl/\"})\n",
        "        @dataclass class MockConfig: gcp: MockGcpConfig = field(default_factory=MockGcpConfig); mlops_trigger_topic:str=\"mock-topic\"; mlops_rl_train_topic:str=\"mock-rl\"; foundation_models: MockFmConfig = field(default_factory=MockFmConfig); learning_flows: MockLearningFlowsConfig = field(default_factory=MockLearningFlowsConfig); miz_oki_schema_version: str = \"3.0\"; def get(self, key, default=None): parts=key.split('.'); val=self; try: [val := getattr(val, p) for p in parts]; return val; except: return default\n",
        "        _config_obj = MockConfig()\n",
        "\n",
        "    _fm_client_proxy = MockFMClientTool(); _li_tool_proxy = MockLITool(); _optimizer_tool_proxy = MockOptimizerTool(); _pubsub_client_proxy = MockPubSubClient()\n",
        "    # --- End Mock/Placeholder Setup ---\n",
        "\n",
        "logger = logging.getLogger('MIZ-OKI.LearningFlows')\n",
        "\n",
        "# --- Knowledge Distillation Tool (Reworked Async - Uses Real FM Client Proxy) ---\n",
        "class KnowledgeDistillationTool:\n",
        "    \"\"\" Handles distilling knowledge async via FM Client API proxy. Deployed as a service. \"\"\"\n",
        "    def __init__(self, fm_client_proxy: Any, config: EnhancedConfig):\n",
        "        if not config or not fm_client_proxy:\n",
        "            raise InitializationError(\"KnowledgeDistillationTool requires config and FM Client proxy.\")\n",
        "        self.fm_client = fm_client_proxy\n",
        "        self.config = config\n",
        "        self.teacher_model_alias = config.get('learning_flows.kd.teacher_model_alias', config.foundation_models.defaults.llama4_maverick)\n",
        "        self.output_bucket = config.gcp.gcs_bucket_name\n",
        "        self.output_prefix = config.get('learning_flows.kd.output_gcs_prefix', 'kd_outputs/')\n",
        "        self.logger = logging.getLogger('MIZ-OKI.KnowledgeDistillationTool')\n",
        "        self.logger.info(f\"KnowledgeDistillation Tool logic initialized with teacher: {self.teacher_model_alias} (Reworked).\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"KnowledgeDistillationTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    async def _save_teacher_outputs_gcs(self, target_gcs_path: str, outputs: List[Any]) -> bool:\n",
        "        \"\"\" Saves teacher outputs to GCS asynchronously using aio-gcsfs if available. \"\"\"\n",
        "        self.logger.info(f\"Saving {len(outputs)} teacher outputs to {target_gcs_path}...\")\n",
        "        if not AIO_GCS_AVAILABLE:\n",
        "            logger.warning(f\"Async GCS save to {target_gcs_path} simulated (aio-gcsfs not installed).\")\n",
        "            await asyncio.sleep(0.05) # Simulate I/O\n",
        "            return True # Simulate success for placeholder\n",
        "\n",
        "        try:\n",
        "            # Use sync GCSFS for checking/creating directory (less critical path)\n",
        "            gcs_dir = os.path.dirname(target_gcs_path)\n",
        "            fs = gcsfs.GCSFileSystem(project=self.config.gcp.project_id)\n",
        "            if not fs.exists(gcs_dir):\n",
        "                fs.makedirs(gcs_dir)\n",
        "                logger.info(f\"Created GCS directory: {gcs_dir}\")\n",
        "\n",
        "            # Use async aio-gcsfs for writing the file content\n",
        "            afs = aio_gcsfs.GCSFileSystem(project=self.config.gcp.project_id)\n",
        "            # Serialize each output item as a JSON line\n",
        "            output_str = \"\\n\".join(json.dumps(item, default=str) for item in outputs)\n",
        "            async with afs.open(target_gcs_path, 'wb') as f:\n",
        "                await f.write(output_str.encode('utf-8'))\n",
        "            self.logger.info(f\"Teacher outputs saved successfully to GCS: {target_gcs_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save teacher outputs to GCS async ({target_gcs_path}): {e}\", exc_info=True)\n",
        "            return False\n",
        "\n",
        "    async def distill_knowledge(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Performs teacher generation async. Expects/Returns MIZ OKI payload. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); student_model_details = payload.get(\"student_model_details\", {}); dataset_ref = payload.get(\"dataset_ref\", {}); distillation_params = payload.get(\"distillation_params\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        student_name = student_model_details.get(\"name\", f\"kd_student_{uuid.uuid4().hex[:6]}\")\n",
        "        kd_run_id = f\"kd_run_{student_name}_{uuid.uuid4().hex[:8]}\"\n",
        "        self.logger.info(f\"Starting async KD teacher generation for student: {student_name} (RunID: {kd_run_id})\")\n",
        "\n",
        "        if not self.fm_client: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"FM Client Tool proxy unavailable.\"})\n",
        "        if not self.output_bucket: errors.append({\"code\": \"CONFIG_ERROR\", \"message\": \"GCS output bucket not configured.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"config_error\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        teacher_output_path = f\"gs://{self.output_bucket}/{self.output_prefix.strip('/')}/{student_name}_{kd_run_id}_teacher.jsonl\"\n",
        "        teacher_outputs = []; status = \"pending\"; response_payload = None\n",
        "\n",
        "        try:\n",
        "            # --- TODO: Implement robust data loading based on dataset_ref ---\n",
        "            # If dataset_ref['gcs_uri'] exists, load data from GCS asynchronously.\n",
        "            # For now, using 'inputs_preview' as the source data.\n",
        "            input_items = dataset_ref.get(\"inputs_preview\", [])\n",
        "            if not input_items or not isinstance(input_items, list):\n",
        "                raise ValueError(\"No valid input data provided in 'dataset_ref.inputs_preview'.\")\n",
        "            # --- End TODO ---\n",
        "\n",
        "            self.logger.info(f\"KD {kd_run_id}: Generating teacher outputs using {self.teacher_model_alias} for {len(input_items)} items...\")\n",
        "\n",
        "            # Generate teacher predictions async via FM Client API proxy\n",
        "            teacher_tasks = []\n",
        "            batch_size = 20 # Batch calls to FM API\n",
        "            for i in range(0, len(input_items), batch_size):\n",
        "                batch_inputs = input_items[i:i+batch_size]\n",
        "                # --- TODO: Construct appropriate prompts based on task type ---\n",
        "                # Example for summarization:\n",
        "                prompts = [f\"Input: {inp}\\nSummary:\" for inp in batch_inputs]\n",
        "                # --- End TODO ---\n",
        "                fm_request = {\n",
        "                    \"payload\": {\"prompt\": prompts, \"model_alias\": self.teacher_model_alias, \"max_tokens\": 256}, # Example params\n",
        "                    \"trace_id\": trace_id, \"request_id\": f\"kd_fm_batch_{i//batch_size}_{kd_run_id}\"\n",
        "                }\n",
        "                teacher_tasks.append(self.fm_client.generate_text(input_data=fm_request)) # Call API proxy\n",
        "\n",
        "            batch_responses = await asyncio.gather(*teacher_tasks, return_exceptions=True)\n",
        "\n",
        "            # Process results, pairing input with output\n",
        "            output_pairs = []\n",
        "            input_idx = 0\n",
        "            for fm_response in batch_responses:\n",
        "                if isinstance(fm_response, Exception):\n",
        "                    errors.append({\"code\": \"FM_API_ERROR\", \"message\": f\"FM API call failed during batch: {fm_response}\"})\n",
        "                    # Skip results for this failed batch\n",
        "                    input_idx += batch_size # Approximate skip\n",
        "                    continue\n",
        "                if fm_response.get(\"status\") == \"success\":\n",
        "                    generated_texts = fm_response.get(\"payload\", {}).get(\"generated_text\", [])\n",
        "                    # Ensure generated_texts is a list, even if single prompt was sent\n",
        "                    if not isinstance(generated_texts, list): generated_texts = [generated_texts]\n",
        "                    batch_input_slice = input_items[input_idx : input_idx + len(generated_texts)]\n",
        "                    for inp, output_text in zip(batch_input_slice, generated_texts):\n",
        "                        output_pairs.append({\"input\": inp, \"teacher_prediction\": output_text})\n",
        "                    input_idx += len(generated_texts)\n",
        "                else:\n",
        "                    errors.append({\"code\": \"FM_TOOL_ERROR\", \"message\": f\"FM Tool API failed: {fm_response.get('error_details')}\"})\n",
        "                    input_idx += batch_size # Approximate skip\n",
        "\n",
        "            teacher_outputs = output_pairs\n",
        "            if not teacher_outputs:\n",
        "                # If there were input items but no outputs (e.g., all API calls failed), raise error\n",
        "                if input_items and not errors: errors.append({\"code\": \"KD_NO_OUTPUTS\", \"message\": \"Teacher generation yielded no outputs despite valid inputs.\"})\n",
        "                if not errors: errors.append({\"code\": \"KD_NO_OUTPUTS\", \"message\": \"Teacher generation yielded no outputs.\"})\n",
        "                raise RuntimeError(\"Teacher generation yielded no valid outputs.\")\n",
        "\n",
        "\n",
        "            # Save teacher outputs to GCS (async)\n",
        "            save_success = await self._save_teacher_outputs_gcs(teacher_output_path, teacher_outputs)\n",
        "            if not save_success:\n",
        "                raise RuntimeError(f\"Failed to save teacher outputs to {teacher_output_path}\")\n",
        "\n",
        "            status = \"success\" if not errors else \"partial_success\"\n",
        "            response_payload = {\"teacher_output_path\": teacher_output_path, \"outputs_generated\": len(teacher_outputs)}\n",
        "            self.logger.info(f\"KD {kd_run_id}: Teacher generation finished. Status: {status}. Outputs at: {teacher_output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"KD_ERROR\", \"message\": str(e)})\n",
        "            self.logger.error(f\"KD teacher generation FAILED (RunID: {kd_run_id}): {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "# --- Continuous Validation Service (Reworked Async - Uses Real FM/LI Tool Proxies) ---\n",
        "class ContinuousValidationService:\n",
        "    \"\"\" Monitors performance, detects issues, processes feedback async via Tool APIs. Deployed as a long-running service or scheduled job. \"\"\"\n",
        "    def __init__(self, fm_client_proxy: Any, learning_integrator_tool_proxy: Any, config: EnhancedConfig):\n",
        "        if not config or not fm_client_proxy or not learning_integrator_tool_proxy:\n",
        "            raise InitializationError(\"ContinuousValidationService requires config and proxies for FM Client and LI Tool.\")\n",
        "        self.fm_client = fm_client_proxy\n",
        "        self.learning_integrator_tool = learning_integrator_tool_proxy\n",
        "        self.config = config\n",
        "        # Use asyncio.Queue for in-memory buffer; consider persistent queue (Pub/Sub, Task Queues) for production\n",
        "        self.feedback_queue = asyncio.Queue(maxsize=config.get('learning_flows.cv.feedback_queue_maxsize', 1000))\n",
        "        self.monitoring_interval = config.get('learning_flows.cv.monitoring_interval_seconds', 300)\n",
        "        self.drift_threshold = config.get('learning_flows.cv.drift_detection_threshold', 0.1)\n",
        "        self.bias_threshold = config.get('learning_flows.cv.bias_detection_threshold', 0.05)\n",
        "        self.feedback_analyzer_alias = config.get('learning_flows.cv.feedback_analyzer_model_alias', config.foundation_models.defaults.feedback_analyzer_model)\n",
        "        self._monitor_task: Optional[asyncio.Task] = None\n",
        "        self.logger = logging.getLogger('MIZ-OKI.ContinuousValidationService')\n",
        "        self.logger.info(\"ContinuousValidation Service logic initialized (Reworked).\")\n",
        "\n",
        "    async def start_monitoring(self):\n",
        "        \"\"\"Starts the background monitoring loop.\"\"\"\n",
        "        if self._monitor_task is None or self._monitor_task.done():\n",
        "            self.logger.info(f\"Starting CV monitor loop (interval: {self.monitoring_interval}s).\")\n",
        "            self._monitor_task = asyncio.create_task(self._monitoring_loop())\n",
        "        else:\n",
        "            self.logger.warning(\"CV Monitoring task already running.\")\n",
        "\n",
        "    async def stop_monitoring(self):\n",
        "        \"\"\"Stops the background monitoring loop gracefully.\"\"\"\n",
        "        if self._monitor_task and not self._monitor_task.done():\n",
        "            self.logger.info(\"Stopping CV monitor loop...\")\n",
        "            self._monitor_task.cancel()\n",
        "            try:\n",
        "                await self._monitor_task\n",
        "            except asyncio.CancelledError:\n",
        "                self.logger.info(\"CV monitoring loop stopped.\")\n",
        "            finally:\n",
        "                self._monitor_task = None\n",
        "        else:\n",
        "             self.logger.info(\"CV monitoring loop not running.\")\n",
        "\n",
        "    async def _monitoring_loop(self):\n",
        "         \"\"\"The core background loop for periodic validation checks.\"\"\"\n",
        "         while True:\n",
        "             try:\n",
        "                 await self.run_validation_cycle()\n",
        "                 await asyncio.sleep(self.monitoring_interval)\n",
        "             except asyncio.CancelledError:\n",
        "                 self.logger.info(\"CV monitoring loop cancelled.\")\n",
        "                 break\n",
        "             except Exception as e:\n",
        "                 # Log error but continue loop\n",
        "                 self.logger.error(f\"Error in CV monitoring loop: {e}\", exc_info=True)\n",
        "                 # Optional: Implement backoff before retrying\n",
        "                 await asyncio.sleep(self.monitoring_interval * 0.5) # Shorter sleep after error\n",
        "\n",
        "    async def add_feedback(self, input_data: Dict[str, Any]): # Expects MIZ OKI payload\n",
        "        \"\"\" Adds feedback item (MIZ OKI payload) to the processing queue asynchronously. \"\"\"\n",
        "        try:\n",
        "            # Basic validation of input_data structure\n",
        "            if not isinstance(input_data, dict) or \"payload\" not in input_data:\n",
        "                 logger.warning(\"CV: Received invalid feedback format (missing payload). Discarding.\")\n",
        "                 return\n",
        "            await self.feedback_queue.put(input_data) # No need for put_nowait if called from async context\n",
        "            self.logger.debug(f\"Added feedback to CV queue. Size: {self.feedback_queue.qsize()}\")\n",
        "        except asyncio.QueueFull:\n",
        "            logger.warning(f\"CV feedback queue is full (max: {self.feedback_queue.maxsize}). Discarding new feedback.\")\n",
        "        except Exception as e:\n",
        "             logger.error(f\"CV: Error adding feedback to queue: {e}\", exc_info=True)\n",
        "\n",
        "    async def _detect_drift(self, context: Dict) -> bool:\n",
        "        \"\"\" Placeholder for data or model drift detection logic. \"\"\"\n",
        "        # --- TODO: Implement Drift Detection ---\n",
        "        # - Fetch baseline data/predictions (e.g., from GCS, BQ, KG).\n",
        "        # - Fetch current data/predictions.\n",
        "        # - Use statistical tests (e.g., KS test, Population Stability Index) or monitoring services.\n",
        "        # - Compare distributions or model performance metrics.\n",
        "        # - Return True if drift > self.drift_threshold.\n",
        "        # --- End TODO ---\n",
        "        logger.debug(\"CV: Checking for drift async (Placeholder - Needs implementation).\")\n",
        "        await asyncio.sleep(random.uniform(0.05, 0.15)) # Simulate check time\n",
        "        return random.random() < 0.05 # Simulate 5% chance of drift\n",
        "\n",
        "    async def _detect_bias(self, context: Dict) -> bool:\n",
        "        \"\"\" Placeholder for bias detection logic. \"\"\"\n",
        "        # --- TODO: Implement Bias Detection ---\n",
        "        # - Requires labeled data with sensitive attributes (e.g., demographics).\n",
        "        # - Fetch predictions for different subgroups.\n",
        "        # - Calculate fairness metrics (e.g., demographic parity, equalized odds).\n",
        "        # - Compare metrics against self.bias_threshold.\n",
        "        # - Return True if bias detected.\n",
        "        # --- End TODO ---\n",
        "        logger.debug(\"CV: Checking for bias async (Placeholder - Needs implementation).\")\n",
        "        await asyncio.sleep(random.uniform(0.05, 0.15)) # Simulate check time\n",
        "        return random.random() < 0.02 # Simulate 2% chance of bias\n",
        "\n",
        "    async def _process_feedback_queue_items(self, trace_id: Optional[str] = None):\n",
        "        \"\"\" Processes feedback from the queue via FM Client API proxy and triggers LI Tool API proxy. \"\"\"\n",
        "        processed_feedback_logs = []; items_processed = 0; max_items_per_cycle = 100 # Limit processing per cycle\n",
        "        if not self.fm_client or not self.learning_integrator_tool:\n",
        "            logger.error(\"CV cannot process feedback: FMClient or LI Tool proxy missing.\")\n",
        "            return # Exit if dependencies missing\n",
        "\n",
        "        logger.debug(f\"CV: Processing feedback queue (Current size: {self.feedback_queue.qsize()})...\")\n",
        "        while not self.feedback_queue.empty() and items_processed < max_items_per_cycle:\n",
        "            try:\n",
        "                item_input_data = await self.feedback_queue.get() # Get item from queue\n",
        "                items_processed += 1\n",
        "                item_payload = item_input_data.get(\"payload\", {})\n",
        "                feedback_text = item_payload.get('feedback')\n",
        "                feedback_type = item_payload.get('type', 'general')\n",
        "                item_request_id = item_input_data.get('request_id', uuid.uuid4().hex[:6]) # Get ID for logging\n",
        "\n",
        "                # Analyze unstructured text feedback via FM Client API proxy\n",
        "                if isinstance(feedback_text, str) and feedback_type == 'unstructured':\n",
        "                    logger.debug(f\"CV: Analyzing feedback via FM Client API proxy: '{feedback_text[:50]}...'\")\n",
        "                    try:\n",
        "                        # Prepare MIZ OKI request for FM Client Tool\n",
        "                        fm_request = {\n",
        "                            \"payload\": {\"text\": feedback_text, \"model_alias\": self.feedback_analyzer_alias},\n",
        "                            \"trace_id\": trace_id, \"request_id\": f\"fm_analyze_fb_{item_request_id}\"\n",
        "                        }\n",
        "                        fm_response = await self.fm_client.analyze(input_data=fm_request) # Call API proxy\n",
        "\n",
        "                        if fm_response.get(\"status\") == \"success\":\n",
        "                            item_payload['analysis'] = fm_response.get(\"payload\", {}) # Add analysis results\n",
        "                        else:\n",
        "                            item_payload['analysis'] = {'error': f\"FM API proxy failed: {fm_response.get('error_details')}\"}\n",
        "                            logger.warning(f\"CV: FM Client API proxy failed to analyze feedback: {fm_response.get('error_details')}\")\n",
        "                    except Exception as fm_e:\n",
        "                        logger.error(f\"CV: Exception calling FM Client API proxy for feedback analysis: {fm_e}\", exc_info=True)\n",
        "                        item_payload['analysis'] = {'error': f\"Exception during FM API call: {fm_e}\"}\n",
        "\n",
        "                processed_feedback_logs.append(item_payload) # Add processed payload to batch\n",
        "                self.feedback_queue.task_done() # Mark item as processed\n",
        "            except asyncio.QueueEmpty:\n",
        "                break # Should not happen with await get() unless queue becomes empty concurrently\n",
        "            except Exception as q_e:\n",
        "                logger.error(f\"CV: Error processing item from feedback queue: {q_e}\", exc_info=True)\n",
        "                # Optionally put item back? Or log and discard? Logging and discarding for now.\n",
        "                self.feedback_queue.task_done() # Ensure task_done is called even on error\n",
        "\n",
        "        # Trigger Learning Integrator Tool API Proxy if feedback was processed\n",
        "        if processed_feedback_logs:\n",
        "            logger.info(f\"CV: Processed {len(processed_feedback_logs)} feedback items. Triggering LI Tool API proxy.\")\n",
        "            try:\n",
        "                 # Prepare MIZ OKI request for LI Tool\n",
        "                 li_request = {\n",
        "                     \"payload\": {\n",
        "                         \"knowledge_type\": 'feedback_batch',\n",
        "                         \"knowledge_data\": processed_feedback_logs,\n",
        "                         \"source\": 'continuous_validation',\n",
        "                         \"importance\": 0.7 # Example importance\n",
        "                     },\n",
        "                     \"trace_id\": trace_id, \"request_id\": f\"li_integrate_fb_{trace_id or uuid.uuid4().hex[:6]}\"\n",
        "                 }\n",
        "                 li_response = await self.learning_integrator_tool.integrate_learning(input_data=li_request) # Call API proxy\n",
        "\n",
        "                 if li_response.get(\"status\") == \"success\":\n",
        "                     logger.info(\"CV: Feedback integration triggered successfully via LI Tool API proxy.\")\n",
        "                 else:\n",
        "                     logger.error(f\"CV: LI Tool API proxy call failed: {li_response.get('error_details')}\")\n",
        "            except Exception as li_e:\n",
        "                logger.error(f\"CV: Failed to trigger LI Tool API proxy for feedback batch: {li_e}\", exc_info=True)\n",
        "        else:\n",
        "            logger.debug(\"CV: No feedback items processed in this cycle.\")\n",
        "\n",
        "    async def run_validation_cycle(self):\n",
        "        \"\"\" Runs one validation cycle: checks drift/bias, processes feedback queue, triggers LI on issues. \"\"\"\n",
        "        cv_id = f\"cv_cycle_{uuid.uuid4().hex[:8]}\"; start_time = time.monotonic(); trace_id = f\"trace_{cv_id}\"\n",
        "        self.logger.info(f\"Starting CV cycle async (ID: {cv_id})...\")\n",
        "        issues_found = False; drift_detected = False; bias_detected = False\n",
        "\n",
        "        try:\n",
        "            # Run checks concurrently with feedback processing\n",
        "            drift_task = self._detect_drift({}); bias_task = self._detect_bias({})\n",
        "            feedback_task = self._process_feedback_queue_items(trace_id=trace_id)\n",
        "\n",
        "            # Gather detection results, handling potential exceptions\n",
        "            detection_results = await asyncio.gather(drift_task, bias_task, return_exceptions=True)\n",
        "            drift_detected = detection_results[0] if not isinstance(detection_results[0], Exception) else False\n",
        "            bias_detected = detection_results[1] if not isinstance(detection_results[1], Exception) else False\n",
        "            if isinstance(detection_results[0], Exception): logger.error(f\"CV {cv_id}: Drift detection failed: {detection_results[0]}\")\n",
        "            if isinstance(detection_results[1], Exception): logger.error(f\"CV {cv_id}: Bias detection failed: {detection_results[1]}\")\n",
        "\n",
        "            # Ensure feedback processing finishes\n",
        "            await feedback_task\n",
        "\n",
        "            # Check if issues were detected\n",
        "            if drift_detected: logger.warning(f\"CV {cv_id}: Drift DETECTED (Threshold: {self.drift_threshold}).\"); issues_found = True\n",
        "            if bias_detected: logger.warning(f\"CV {cv_id}: Bias DETECTED (Threshold: {self.bias_threshold}).\"); issues_found = True\n",
        "\n",
        "            # Trigger Learning Integrator Tool API Proxy if issues found\n",
        "            if issues_found:\n",
        "                if self.learning_integrator_tool:\n",
        "                    logger.info(f\"CV {cv_id}: Triggering LI Tool API proxy due to detected issues.\")\n",
        "                    try:\n",
        "                         # Prepare MIZ OKI request for LI Tool\n",
        "                         li_request = {\n",
        "                             \"payload\": {\n",
        "                                 \"knowledge_type\": 'validation_alert',\n",
        "                                 \"knowledge_data\": {'drift_detected': drift_detected, 'bias_detected': bias_detected, 'cycle_id': cv_id},\n",
        "                                 \"source\": 'continuous_validation_alert',\n",
        "                                 \"importance\": 0.9 # High importance for validation alerts\n",
        "                             },\n",
        "                             \"trace_id\": trace_id, \"request_id\": f\"li_alert_{cv_id}\"\n",
        "                         }\n",
        "                         li_response = await self.learning_integrator_tool.integrate_learning(input_data=li_request) # Call API proxy\n",
        "\n",
        "                         if li_response.get(\"status\") == \"success\": logger.info(f\"CV {cv_id}: LI trigger successful via API proxy.\")\n",
        "                         else: logger.error(f\"CV {cv_id}: LI Tool API proxy call failed: {li_response.get('error_details')}\")\n",
        "                    except Exception as li_e:\n",
        "                        logger.error(f\"CV {cv_id}: Failed to trigger LI Tool API proxy for validation alert: {li_e}\", exc_info=True)\n",
        "                else:\n",
        "                    logger.error(f\"CV {cv_id}: Issues detected but LI Tool proxy unavailable!\")\n",
        "\n",
        "        except Exception as cycle_e:\n",
        "             logger.error(f\"CV cycle {cv_id} encountered an error: {cycle_e}\", exc_info=True)\n",
        "\n",
        "        duration = (time.monotonic() - start_time) * 1000\n",
        "        self.logger.info(f\"CV cycle {cv_id} finished in {duration:.2f} ms. Issues found: {issues_found}\")\n",
        "\n",
        "# --- Dynamic Reward System Tool (Reworked Async - Uses Real PO Tool Proxy) ---\n",
        "class DynamicRewardSystemTool:\n",
        "    \"\"\" Calculates rewards async based on outcomes and current objective priorities fetched via PO Tool API proxy. Deployed as a service. \"\"\"\n",
        "    def __init__(self, optimizer_tool_proxy: Any, config: EnhancedConfig):\n",
        "        if not config or not optimizer_tool_proxy:\n",
        "            raise InitializationError(\"DynamicRewardSystemTool requires config and Optimizer Tool proxy.\")\n",
        "        self.optimizer_tool = optimizer_tool_proxy\n",
        "        self.config = config\n",
        "        self.base_reward_weights = config.get('learning_flows.drs.base_weights', {'task_completion': 1.0, 'efficiency': 0.5})\n",
        "        self.objective_influence_factor = config.get('learning_flows.drs.objective_influence_factor', 0.3)\n",
        "        self.update_interval = config.get('learning_flows.drs.update_interval_seconds', 600)\n",
        "        self.current_reward_weights = self.base_reward_weights.copy() # Start with base weights\n",
        "        self._last_objective_update_time = 0\n",
        "        self._update_lock = asyncio.Lock()\n",
        "        self.logger = logging.getLogger('MIZ-OKI.DynamicRewardSystemTool')\n",
        "        self.logger.info(f\"DynamicRewardSystem Tool logic initialized (Reworked). Base weights: {self.base_reward_weights}\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"DynamicRewardSystemTool\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    async def _adjust_weights_for_objectives(self, trace_id: Optional[str] = None):\n",
        "        \"\"\" Adjusts reward weights based on current objective priorities fetched via PO Tool API proxy. \"\"\"\n",
        "        now = time.monotonic()\n",
        "        async with self._update_lock: # Prevent concurrent updates\n",
        "            if now - self._last_objective_update_time < self.update_interval:\n",
        "                logger.debug(\"DRS: Skipping objective weight adjustment, too soon.\")\n",
        "                return # Too soon to update\n",
        "\n",
        "            if not self.optimizer_tool or not hasattr(self.optimizer_tool, 'get_current_objective_priorities'):\n",
        "                logger.warning(\"DRS: Optimizer Tool proxy unavailable or method missing. Cannot adjust weights.\")\n",
        "                self._last_objective_update_time = now # Still update time to prevent rapid retries\n",
        "                return\n",
        "\n",
        "            self.logger.info(\"DRS: Adjusting reward weights based on current objectives...\")\n",
        "            try:\n",
        "                # Call PO Tool API proxy\n",
        "                po_request = {\"miz_oki_version\": \"3.0\", \"trace_id\": trace_id, \"request_id\": f\"po_get_prio_{trace_id or uuid.uuid4().hex[:6]}\"}\n",
        "                po_response = await self.optimizer_tool.get_current_objective_priorities(input_data=po_request) # Call proxy method\n",
        "\n",
        "                if po_response.get(\"status\") == \"success\":\n",
        "                    current_objective_priorities = po_response.get(\"payload\", {}).get(\"priorities\", {}) # Expects {'ObjectiveName': priority_score}\n",
        "                    self.logger.info(f\"DRS: Fetched objective priorities async via PO Tool API proxy: {current_objective_priorities}\")\n",
        "\n",
        "                    # --- Recalculate weights (Sync logic ok) ---\n",
        "                    adjusted_weights = self.base_reward_weights.copy()\n",
        "                    total_priority_score = sum(p for p in current_objective_priorities.values() if isinstance(p, (int, float)) and p > 0)\n",
        "\n",
        "                    if total_priority_score > 0:\n",
        "                        # Normalize priorities\n",
        "                        normalized_priorities = {obj: score / total_priority_score for obj, score in current_objective_priorities.items() if isinstance(score, (int, float)) and score > 0}\n",
        "\n",
        "                        # Adjust base weights based on priorities\n",
        "                        # --- TODO: Implement mapping logic from objectives to reward metrics ---\n",
        "                        # This requires knowing which metrics contribute to which objectives.\n",
        "                        # Example: If 'ImproveQuality' objective has high priority, boost 'quality' reward weight.\n",
        "                        # Example: If 'ReduceCost' objective has high priority, boost 'efficiency' reward weight.\n",
        "                        for objective, priority in normalized_priorities.items():\n",
        "                            # Find metrics related to this objective (e.g., from config or KG)\n",
        "                            related_metrics = self.config.get(f\"objective_metric_mapping.{objective}\", []) # Example config path\n",
        "                            for metric in related_metrics:\n",
        "                                if metric in adjusted_weights:\n",
        "                                    adjustment = priority * self.objective_influence_factor\n",
        "                                    adjusted_weights[metric] = self.base_reward_weights[metric] * (1 + adjustment)\n",
        "                                    logger.debug(f\"DRS: Adjusted weight for '{metric}' by factor {1+adjustment} due to objective '{objective}' priority {priority:.2f}\")\n",
        "                        # --- End TODO ---\n",
        "\n",
        "                    self.current_reward_weights = adjusted_weights\n",
        "                    self.logger.info(f\"DRS: Adjusted reward weights via PO Tool API proxy: {self.current_reward_weights}\")\n",
        "                else:\n",
        "                    logger.error(f\"DRS: Failed to get objective priorities from PO Tool API proxy: {po_response.get('error_details')}\")\n",
        "                    # Keep current weights on failure\n",
        "\n",
        "                self._last_objective_update_time = now # Update time even on failure to prevent rapid retries\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"DRS: Failed to adjust reward weights via PO Tool API proxy: {e}\", exc_info=True)\n",
        "                # Revert to base weights on unexpected error? Or keep current? Keeping current for now.\n",
        "                # self.current_reward_weights = self.base_reward_weights.copy()\n",
        "                self._last_objective_update_time = now # Update time\n",
        "\n",
        "    async def calculate_reward(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Calculates reward async based on outcome metrics and dynamic weights. Expects/Returns MIZ OKI payload. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); outcome_metrics = payload.get(\"outcome_metrics\", {}) # e.g., {\"task_completion\": 1.0, \"efficiency\": 0.8, \"quality\": 0.9}\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not isinstance(outcome_metrics, dict): errors.append({\"code\": \"INVALID_PAYLOAD\", \"message\": \"'payload.outcome_metrics' must be a dictionary.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        status = \"pending\"; response_payload = None; total_reward = 0.0\n",
        "        try:\n",
        "            # Ensure weights are up-to-date based on objectives\n",
        "            await self._adjust_weights_for_objectives(trace_id=trace_id)\n",
        "\n",
        "            # Calculate weighted reward\n",
        "            calculation_details = {}\n",
        "            for metric, value in outcome_metrics.items():\n",
        "                weight = self.current_reward_weights.get(metric)\n",
        "                if weight is not None:\n",
        "                     try:\n",
        "                         metric_value = float(value)\n",
        "                         # Ensure reward component is not NaN or Inf\n",
        "                         if np.isnan(metric_value) or np.isinf(metric_value): raise ValueError(\"Metric value is NaN or Inf\")\n",
        "                         if np.isnan(weight) or np.isinf(weight): raise ValueError(\"Weight is NaN or Inf\")\n",
        "\n",
        "                         reward_component = metric_value * weight\n",
        "                         if np.isnan(reward_component) or np.isinf(reward_component): raise ValueError(\"Calculated reward component is NaN or Inf\")\n",
        "\n",
        "                         total_reward += reward_component\n",
        "                         calculation_details[metric] = {\"value\": metric_value, \"weight\": weight, \"reward\": reward_component}\n",
        "                     except (ValueError, TypeError) as val_err:\n",
        "                         logger.warning(f\"DRS: Invalid value '{value}' or weight '{weight}' for metric '{metric}'. Skipping. Error: {val_err}\")\n",
        "                         calculation_details[metric] = {\"value\": value, \"weight\": weight, \"error\": f\"Invalid value/weight: {val_err}\"}\n",
        "                else:\n",
        "                     logger.debug(f\"DRS: No weight defined for metric '{metric}'. Skipping.\")\n",
        "                     calculation_details[metric] = {\"value\": value, \"weight\": None, \"reward\": 0}\n",
        "\n",
        "            # Ensure final reward is not NaN or Inf\n",
        "            if np.isnan(total_reward) or np.isinf(total_reward):\n",
        "                logger.error(f\"DRS: Final calculated reward is NaN or Inf. Resetting to 0. Details: {calculation_details}\")\n",
        "                total_reward = 0.0\n",
        "                errors.append({\"code\": \"REWARD_NAN_INF\", \"message\": \"Calculated reward resulted in NaN or Inf.\"})\n",
        "                status = \"failed\"\n",
        "            else:\n",
        "                status = \"success\"\n",
        "\n",
        "            response_payload = {\"reward\": total_reward, \"calculation_details\": calculation_details, \"weights_used\": self.current_reward_weights}\n",
        "            self.logger.debug(f\"DRS: Calculated reward via Tool API: {total_reward:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"REWARD_CALC_ERROR\", \"message\": str(e)})\n",
        "             logger.error(f\"DRS: Failed to calculate reward: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "# --- Distributed Reinforcement Learning Manager (Reworked Async - Real PubSub Proxy, External Training) ---\n",
        "class DistributedRLManager:\n",
        "    \"\"\" Manages RL agent experiences & triggers external MLOps training via Pub/Sub client proxy. Deployed as a service. \"\"\"\n",
        "    def __init__(self, reward_system_tool_proxy: Any, pubsub_client_proxy: Any, config: EnhancedConfig):\n",
        "        if not config or not reward_system_tool_proxy or not pubsub_client_proxy:\n",
        "            raise InitializationError(\"DistributedRLManager requires config and proxies for DRS Tool and PubSub Client.\")\n",
        "        self.reward_system_tool = reward_system_tool_proxy\n",
        "        self.pubsub_client = pubsub_client_proxy\n",
        "        self.config = config\n",
        "        self.agents: Dict[str, Any] = {} # Stores ADK agent references (e.g., service URLs or IDs) - Needs registration mechanism\n",
        "\n",
        "        # --- TODO: Implement robust buffer persistence ---\n",
        "        # Option 1: Use BigQuery table (good for querying, potential write limits)\n",
        "        # Option 2: Partitioned GCS files (e.g., hourly/daily JSONL files) - Shown in placeholder save method\n",
        "        # Option 3: Dedicated time-series DB or buffer service\n",
        "        self.experience_buffer = deque(maxlen=config.get('learning_flows.drl.buffer_size', 50000)) # In-memory placeholder ONLY\n",
        "        self.buffer_save_path_prefix = f\"gs://{config.gcp.gcs_bucket_name}/{config.get('learning_flows.drl.buffer_gcs_prefix', 'rl_buffer/')}\"\n",
        "        # --- End TODO ---\n",
        "\n",
        "        self.save_interval_seconds = config.get('learning_flows.drl.buffer_save_interval_sec', 600)\n",
        "        self.min_buffer_for_train = config.get('learning_flows.drl.min_buffer_for_train', 1000)\n",
        "        self.mlops_rl_train_topic_name = config.mlops_rl_train_topic\n",
        "        self.project = config.gcp.project_id\n",
        "\n",
        "        self._save_task: Optional[asyncio.Task] = None\n",
        "        self.logger = logging.getLogger('MIZ-OKI.DistributedRLManager')\n",
        "        self.logger.info(f\"DistributedRLManager initialized (Reworked). Buffer Size: {self.experience_buffer.maxlen} (In-Memory Placeholder)\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"DistributedRLManager\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    # --- Agent Registration (Needs a proper mechanism in deployment) ---\n",
        "    def register_agent(self, agent_id: str, agent_ref: Any):\n",
        "        \"\"\"Registers an RL agent (e.g., its service endpoint or ID).\"\"\"\n",
        "        # In a real system, this might involve service discovery or a configuration update.\n",
        "        self.agents[agent_id] = agent_ref\n",
        "        self.logger.info(f\"Registered RL agent: {agent_id}\")\n",
        "\n",
        "    def unregister_agent(self, agent_id: str):\n",
        "        \"\"\"Unregisters an RL agent.\"\"\"\n",
        "        if agent_id in self.agents:\n",
        "            del self.agents[agent_id]\n",
        "            self.logger.info(f\"Unregistered RL agent: {agent_id}\")\n",
        "    # --- End Agent Registration ---\n",
        "\n",
        "    async def add_experience(self, input_data: Dict[str, Any]): # Expects MIZ OKI payload\n",
        "        \"\"\" Adds experience tuple (in payload) to the buffer asynchronously. \"\"\"\n",
        "        payload = input_data.get(\"payload\", {})\n",
        "        exp_tuple = payload.get(\"experience\") # Expects (agent_id, state, action, reward, next_state, done, info)\n",
        "\n",
        "        # Validate experience tuple structure\n",
        "        if not isinstance(exp_tuple, (list, tuple)) or len(exp_tuple) < 6:\n",
        "            logger.warning(f\"DRL Mgr: Received invalid experience data format: {type(exp_tuple)}. Discarding.\")\n",
        "            return # Discard invalid data\n",
        "\n",
        "        agent_id = exp_tuple[0]\n",
        "        # Optional: Check if agent is registered (might be too slow for high-throughput)\n",
        "        # if agent_id not in self.agents:\n",
        "        #     logger.warning(f\"DRL Mgr: Received experience from unregistered agent '{agent_id}'. Discarding.\")\n",
        "        #     return # Discard data from unknown agents\n",
        "\n",
        "        # --- TODO: Add to persistent buffer store ---\n",
        "        # Example: Append to a BigQuery table or write to a temporary file before batch GCS upload.\n",
        "        # For now, using the in-memory deque placeholder.\n",
        "        try:\n",
        "            # Add timestamp to the experience tuple before storing\n",
        "            timestamped_exp = (*exp_tuple, datetime.now(datetime.timezone.utc).isoformat())\n",
        "            self.experience_buffer.append(timestamped_exp)\n",
        "            # logger.debug(f\"DRL Mgr: Collected experience from {agent_id}. Buffer size: {len(self.experience_buffer)}\")\n",
        "        except Exception as buf_e:\n",
        "             logger.error(f\"DRL Mgr: Error adding experience to buffer: {buf_e}\", exc_info=True)\n",
        "        # --- End TODO ---\n",
        "\n",
        "    async def _save_buffer_to_gcs_async(self) -> Optional[str]:\n",
        "        \"\"\" Saves buffer snapshot to GCS asynchronously. Needs robust implementation. Returns GCS path on success. \"\"\"\n",
        "        # --- TODO: Implement robust, incremental saving using async GCS writes (aio-gcsfs) ---\n",
        "        # - Read from persistent buffer source (e.g., BQ table, recent GCS files).\n",
        "        # - Aggregate data for the training job.\n",
        "        # - Write aggregated data to a new GCS file.\n",
        "        # - Handle serialization, partitioning for large buffers.\n",
        "        # --- Placeholder Implementation (Saves in-memory deque) ---\n",
        "        buffer_copy = list(self.experience_buffer) # Get snapshot\n",
        "        if not buffer_copy:\n",
        "            logger.info(\"DRL Mgr: Experience buffer is empty. Nothing to save.\")\n",
        "            return None\n",
        "\n",
        "        # Create a timestamped filename\n",
        "        ts = datetime.now(datetime.timezone.utc).strftime('%Y%m%d_%H%M%S_%f')\n",
        "        filename = f\"rl_experience_{ts}.jsonl\"\n",
        "        gcs_path = os.path.join(self.buffer_save_path_prefix.rstrip('/'), filename)\n",
        "        self.logger.info(f\"DRL Mgr: Saving {len(buffer_copy)} experiences to {gcs_path}...\")\n",
        "\n",
        "        if not AIO_GCS_AVAILABLE:\n",
        "            logger.warning(f\"Async GCS save to {gcs_path} simulated (aio-gcsfs not installed).\")\n",
        "            await asyncio.sleep(0.1) # Simulate I/O\n",
        "            return gcs_path # Simulate success\n",
        "\n",
        "        try:\n",
        "            # Use sync GCSFS for checking/creating directory (less critical path)\n",
        "            gcs_dir = os.path.dirname(gcs_path)\n",
        "            fs = gcsfs.GCSFileSystem(project=self.config.gcp.project_id)\n",
        "            if not fs.exists(gcs_dir):\n",
        "                fs.makedirs(gcs_dir)\n",
        "                logger.info(f\"Created GCS directory: {gcs_dir}\")\n",
        "\n",
        "            # Use async aio-gcsfs for writing the file content\n",
        "            afs = aio_gcsfs.GCSFileSystem(project=self.config.gcp.project_id)\n",
        "            # Serialize each experience tuple as a JSON line\n",
        "            # Ensure all elements are JSON serializable (e.g., numpy arrays converted to lists)\n",
        "            def serialize_experience(exp):\n",
        "                serializable_exp = []\n",
        "                for item in exp:\n",
        "                    if isinstance(item, np.ndarray): serializable_exp.append(item.tolist())\n",
        "                    elif isinstance(item, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)): serializable_exp.append(int(item))\n",
        "                    elif isinstance(item, (np.float_, np.float16, np.float32, np.float64)): serializable_exp.append(float(item))\n",
        "                    elif isinstance(item, (np.bool_)): serializable_exp.append(bool(item))\n",
        "                    elif isinstance(item, (datetime, datetime.date)): serializable_exp.append(item.isoformat())\n",
        "                    else: serializable_exp.append(item)\n",
        "                # Use default=str as a fallback for other non-serializable types\n",
        "                return json.dumps(serializable_exp, default=str)\n",
        "\n",
        "            output_str = \"\\n\".join(serialize_experience(item) for item in buffer_copy)\n",
        "            async with afs.open(gcs_path, 'wb') as f:\n",
        "                await f.write(output_str.encode('utf-8'))\n",
        "            self.logger.info(f\"DRL Mgr: Experience buffer snapshot saved to {gcs_path}.\")\n",
        "            return gcs_path\n",
        "        except Exception as e:\n",
        "            logger.error(f\"DRL Mgr: Failed to save experience buffer to {gcs_path}: {e}\", exc_info=True)\n",
        "            return None\n",
        "        # --- End Placeholder ---\n",
        "\n",
        "    async def start_buffer_saving(self):\n",
        "        \"\"\"Starts the periodic background task for saving the experience buffer.\"\"\"\n",
        "        if self._save_task is None or self._save_task.done():\n",
        "            self.logger.info(f\"Starting periodic RL buffer save task (Interval: {self.save_interval_seconds}s)...\")\n",
        "            self._save_task = asyncio.create_task(self._buffer_save_loop())\n",
        "        else:\n",
        "             self.logger.warning(\"DRL buffer save task already running.\")\n",
        "\n",
        "    async def stop_buffer_saving(self):\n",
        "        \"\"\"Stops the periodic background buffer saving task.\"\"\"\n",
        "         if self._save_task and not self._save_task.done():\n",
        "             self.logger.info(\"Stopping DRL buffer save task...\")\n",
        "             self._save_task.cancel()\n",
        "             try: await self._save_task\n",
        "             except asyncio.CancelledError: pass # Expected exception on cancellation\n",
        "             finally: self._save_task = None\n",
        "             self.logger.info(\"DRL buffer save task stopped.\")\n",
        "         else:\n",
        "              self.logger.info(\"DRL buffer save task not running.\")\n",
        "\n",
        "    async def _buffer_save_loop(self):\n",
        "         \"\"\"Background loop for periodically saving the buffer.\"\"\"\n",
        "         while True:\n",
        "             try:\n",
        "                 await asyncio.sleep(self.save_interval_seconds)\n",
        "                 await self._save_buffer_to_gcs_async()\n",
        "             except asyncio.CancelledError:\n",
        "                 self.logger.info(\"DRL buffer save loop cancelled.\")\n",
        "                 break\n",
        "             except Exception as e:\n",
        "                 logger.error(f\"Error in DRL buffer save loop: {e}\", exc_info=True)\n",
        "                 # Optional: Implement backoff before retrying save\n",
        "                 await asyncio.sleep(self.save_interval_seconds * 0.5) # Shorter sleep after error\n",
        "\n",
        "    async def trigger_training(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\" Triggers external MLOps RL training pipeline via Pub/Sub client proxy. Expects/Returns MIZ OKI. \"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); force = payload.get(\"force\", False)\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not self.pubsub_client: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"PubSub client proxy unavailable.\"})\n",
        "        if not self.project: errors.append({\"code\": \"CONFIG_ERROR\", \"message\": \"GCP Project ID not configured.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"config_error\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        status = \"pending\"; response_payload = None\n",
        "        try:\n",
        "            # --- TODO: Get current buffer length from persistent store ---\n",
        "            buffer_len = len(self.experience_buffer) # Using placeholder length\n",
        "            # --- End TODO ---\n",
        "\n",
        "            if not force and buffer_len < self.min_buffer_for_train:\n",
        "                msg = f\"Skipping training trigger. Buffer size {buffer_len} < {self.min_buffer_for_train}.\"\n",
        "                logger.info(f\"DRL Mgr: {msg}\")\n",
        "                status = \"skipped\"; response_payload = {\"message\": msg}\n",
        "            else:\n",
        "                self.logger.info(f\"DRL Mgr: Triggering MLOps RL training pipeline via Pub/Sub proxy (Buffer size: {buffer_len}, Force: {force}).\")\n",
        "                # Ensure latest data is saved before triggering training\n",
        "                latest_snapshot_uri = await self._save_buffer_to_gcs_async()\n",
        "                if not latest_snapshot_uri:\n",
        "                    raise RuntimeError(\"Failed to save experience buffer to GCS before triggering training.\")\n",
        "\n",
        "                pipeline_name = \"miz3_rl_agent_training_pipeline\" # Should be configurable\n",
        "                # Parameters for the Vertex AI Pipeline job\n",
        "                pipeline_params = {\n",
        "                    \"project\": self.project,\n",
        "                    \"location\": self.config.gcp.region,\n",
        "                    \"experience_data_uri\": latest_snapshot_uri, # Pass GCS path to training data\n",
        "                    \"agent_ids\": list(self.agents.keys()), # Pass IDs of agents to train/update\n",
        "                    \"timestamp_trigger\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "                    # Add other necessary pipeline parameters (e.g., hyperparameters, model output paths)\n",
        "                    \"output_model_dir\": f\"gs://{self.config.gcp.gcs_bucket_name}/rl_models/{datetime.now(datetime.timezone.utc).strftime('%Y%m%d%H%M%S')}/\"\n",
        "                }\n",
        "                # MIZ OKI formatted message for Pub/Sub\n",
        "                message_data = {\n",
        "                    \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "                    \"event_type\": \"trigger_mlops_pipeline\",\n",
        "                    \"payload\": {\"pipeline_name\": pipeline_name, \"parameters\": pipeline_params},\n",
        "                    \"metadata\": {\"trace_id\": trace_id, \"source_component\": \"DistributedRLManager\"}\n",
        "                }\n",
        "                message_bytes = json.dumps(message_data).encode('utf-8')\n",
        "                rl_train_topic_full_path = f\"projects/{self.project}/topics/{self.mlops_rl_train_topic_name}\"\n",
        "\n",
        "                # Call Pub/Sub Client Proxy method\n",
        "                message_id = await self.pubsub_client.publish(rl_train_topic_full_path, message_bytes)\n",
        "\n",
        "                status = \"success\"\n",
        "                response_payload = {\"message_id\": message_id, \"pipeline_triggered\": pipeline_name, \"data_snapshot\": latest_snapshot_uri}\n",
        "                self.logger.info(f\"DRL Mgr: Triggered MLOps RL pipeline via Pub/Sub proxy. Topic: {rl_train_topic_full_path}, Message ID: {message_id}.\")\n",
        "                # --- TODO: Optionally clear the buffer after successful trigger/save, depending on persistence strategy ---\n",
        "                # self.experience_buffer.clear() # If using in-memory only\n",
        "                # --- End TODO ---\n",
        "\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"TRAINING_TRIGGER_ERROR\", \"message\": str(e)})\n",
        "             logger.error(f\"DRL Mgr: Failed to trigger training pipeline: {e}\", exc_info=True)\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "# --- Initialization (Conceptual - Services instantiated by framework/orchestrator) ---\n",
        "# kd_tool: Optional[KnowledgeDistillationTool] = None\n",
        "# cv_service: Optional[ContinuousValidationService] = None\n",
        "# drs_tool: Optional[DynamicRewardSystemTool] = None\n",
        "# drl_manager: Optional[DistributedRLManager] = None\n",
        "\n",
        "# async def initialize_learning_flows():\n",
        "#      global kd_tool, cv_service, drs_tool, drl_manager\n",
        "#      if not _config_obj or not _real_dependencies:\n",
        "#          logger.critical(\"Cannot initialize Learning Flows: Config or dependencies missing.\")\n",
        "#          return\n",
        "#      try:\n",
        "#          kd_tool = KnowledgeDistillationTool(_fm_client_proxy, _config_obj)\n",
        "#          cv_service = ContinuousValidationService(_fm_client_proxy, _li_tool_proxy, _config_obj)\n",
        "#          drs_tool = DynamicRewardSystemTool(_optimizer_tool_proxy, _config_obj)\n",
        "#          drl_manager = DistributedRLManager(drs_tool, _pubsub_client_proxy, _config_obj)\n",
        "\n",
        "#          # Start background tasks if deployed as services\n",
        "#          await cv_service.start_monitoring()\n",
        "#          await drl_manager.start_buffer_saving()\n",
        "\n",
        "#          logger.info(\"Learning Flow Tools/Services initialized.\")\n",
        "#      except Exception as e:\n",
        "#           logger.critical(f\"Learning Flow Tools initialization failed: {e}\", exc_info=True)\n",
        "#           kd_tool = cv_service = drs_tool = drl_manager = None # Set all to None on failure\n",
        "\n",
        "# async def cleanup_learning_flows():\n",
        "#       if cv_service: await cv_service.stop_monitoring()\n",
        "#       if drl_manager: await drl_manager.stop_buffer_saving()\n",
        "#       # Add cleanup for other components if needed\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 Learning Flows Layer Logic (Cell 8 - Reworked) ---\")\n",
        "print(\"KD, CV, DRS use real dependencies/proxies via MIZ OKI APIs. DRL Manager uses real PubSub proxy.\")\n",
        "print(\"Async GCS pattern shown (requires aio-gcsfs). Training logic externalized via MLOps Pub/Sub triggers.\")\n",
        "print(\"Requires implementation of drift/bias detection, robust DRL buffer persistence.\")\n",
        "print(\"------------------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "E11EKtyaqiMZ",
        "outputId": "b23e4144-f8e6-4eeb-e1c8-43c3e60916c6"
      },
      "id": "E11EKtyaqiMZ",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-13-f54153018474>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-f54153018474>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    **Cell 8: Learning Flows Implementation (Reworked)**\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: System Integration and Testing (Reworked)\n",
        "# Status: Uses unittest.IsolatedAsyncioTestCase. Mocks refined for Vertex AI Workflow client.\n",
        "#         Emphasizes need for MIZ OKI payload validation and better mocks/error path tests.\n",
        "\n",
        "import unittest\n",
        "import logging\n",
        "import asyncio\n",
        "from unittest.mock import patch, MagicMock, AsyncMock # Standard mocking libraries\n",
        "import uuid\n",
        "import time\n",
        "import random\n",
        "from typing import Dict, Any, Optional, List, Union\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "# --- Pydantic for MIZ OKI Payload Validation (Conceptual) ---\n",
        "# Import Pydantic if available for schema validation in tests\n",
        "try:\n",
        "    from pydantic import BaseModel, Field, ValidationError, validator\n",
        "    PYDANTIC_AVAILABLE = True\n",
        "\n",
        "    # Define a basic Pydantic model for MIZ OKI Payloads (adapt as needed)\n",
        "    class MizOkiPayload(BaseModel):\n",
        "        miz_oki_version: str = \"3.0\"\n",
        "        request_id: str\n",
        "        trace_id: Optional[str] = None\n",
        "        workflow_execution_id: Optional[str] = None\n",
        "        step_id: Optional[str] = None\n",
        "        timestamp: str # ISO format string\n",
        "        source_component: Optional[str] = None\n",
        "        target_component: Optional[str] = None\n",
        "        status: Optional[str] = None # For responses\n",
        "        payload: Optional[Dict[str, Any]] = None\n",
        "        error_details: Optional[List[Dict[str, Any]]] = None\n",
        "        metadata: Optional[Dict[str, Any]] = None\n",
        "\n",
        "        @validator('timestamp')\n",
        "        def validate_timestamp_isoformat(cls, v):\n",
        "            try:\n",
        "                datetime.datetime.fromisoformat(v.replace('Z', '+00:00'))\n",
        "                return v\n",
        "            except ValueError:\n",
        "                raise ValueError('Timestamp must be in ISO 8601 format')\n",
        "\n",
        "except ImportError:\n",
        "    PYDANTIC_AVAILABLE = False\n",
        "    class BaseModel: pass; class Field: pass; class ValidationError(Exception): pass; validator = lambda x: x # Dummy decorator\n",
        "    logging.warning(\"Pydantic not installed. Cannot perform MIZ OKI payload validation in tests.\")\n",
        "\n",
        "# --- Assume necessary components/mocks are importable or defined ---\n",
        "# Using refined mocks from previous cells' rework analysis\n",
        "\n",
        "# Mock Config (Simple dict for testing)\n",
        "class MockEnhancedConfig(dict):\n",
        "     def __init__(self, data): super().__init__(data)\n",
        "     def get(self, key, default=None): # Simplified get for testing\n",
        "          try: return self[key]\n",
        "          except KeyError: return default\n",
        "\n",
        "# Mock KG Tool API Service Proxy\n",
        "class MockKGTool:\n",
        "    _decision_logs = {}\n",
        "    async def execute_query(self, request: Dict): logger.info(f\"Mock KG Tool API: execute_query async\"); await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"results\": [{'mock_result': 1}]}}\n",
        "    async def save_decision_record(self, request: Dict): record = request.get(\"payload\",{}); decision_id = record.get('decision_id'); logger.info(f\"Mock KG Tool API: save_decision_record async for {decision_id}\"); await asyncio.sleep(0.01); self._decision_logs[decision_id] = record; return {\"status\": \"success\"}\n",
        "    async def retrieve_decision_record(self, request: Dict): decision_id = request.get(\"payload\",{}).get(\"decision_id\"); logger.info(f\"Mock KG Tool API: retrieve_decision_record async for {decision_id}\"); await asyncio.sleep(0.01); record = self._decision_logs.get(decision_id); return {\"status\": \"success\" if record else \"not_found\", \"payload\": {\"decision_record\": record}}\n",
        "    async def get_entity_endpoint(self, request: Dict): logger.info(\"Mock KG Tool API: get_entity async\"); await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"entity_data\": {\"mock_prop\": \"value\"}}}\n",
        "    async def search_vector_endpoint(self, request: Dict): logger.info(\"Mock KG Tool API: search_vector_index async\"); await asyncio.sleep(0.01); return {\"status\": \"success\", \"payload\": {\"results\": [(\"mock_id\", 0.9, {})]}}\n",
        "    # Add other methods used by tests if necessary\n",
        "\n",
        "# Mock FM Client API Service Proxy\n",
        "class MockFoundationModelClient:\n",
        "    async def generate_text(self, input_data: Dict): logger.info(f\"Mock FMClient API: generate_text async\"); await asyncio.sleep(0.05); return {\"status\": \"success\", \"payload\": {\"generated_text\": \"Mock async generation\"}, \"metadata\": {\"provider\": \"mock\"}}\n",
        "    async def generate_embedding(self, input_data: Dict): logger.info(f\"Mock FMClient API: generate_embedding async\"); await asyncio.sleep(0.02); return {\"status\": \"success\", \"payload\": {\"embedding\": [random.random()]*10}, \"metadata\": {\"provider\": \"mock\"}}\n",
        "    async def analyze(self, input_data: Dict): logger.info(f\"Mock FMClient API: analyze async\"); await asyncio.sleep(0.03); return {\"status\": \"success\", \"payload\": {'sentiment': 'neutral'}, \"metadata\": {\"provider\": \"mock\"}}\n",
        "    # Add other methods used by tests if necessary\n",
        "\n",
        "# Mock Vertex Workflow Executions Client (Refined from Cell 16 Rework)\n",
        "# Define dummy state enum if SDK not available\n",
        "if not VERTEX_WORKFLOWS_SDK_AVAILABLE:\n",
        "    class ExecutionState: ACTIVE=\"ACTIVE\"; SUCCEEDED=\"SUCCEEDED\"; FAILED=\"FAILED\"; CANCELLED=\"CANCELLED\"; SUSPENDED=\"SUSPENDED\"\n",
        "    class DummyProto: pass\n",
        "    class executions_v1: ExecutionState = ExecutionState; class ExecutionsAsyncClient: pass; class Execution(DummyProto): pass; class CreateExecutionRequest: pass; class GetExecutionRequest: pass; class CancelExecutionRequest: pass; class ListExecutionsRequest: pass; class ListExecutionsResponse: pass; ExecutionView = type('Enum', (), {'BASIC': 1, 'FULL': 2})()\n",
        "    class google_api_exceptions: class NotFound(Exception): pass; class FailedPrecondition(Exception): pass; class GoogleAPIError(Exception): pass\n",
        "\n",
        "class MockVertexWorkflowClient:\n",
        "    _executions = {}\n",
        "    # Use real State enum if SDK available, else mock\n",
        "    State = executions_v1.Execution.State if VERTEX_WORKFLOWS_SDK_AVAILABLE else ExecutionState\n",
        "    ExecutionView = executions_v1.ExecutionView if VERTEX_WORKFLOWS_SDK_AVAILABLE else type('Enum', (), {'BASIC': 1, 'FULL': 2})()\n",
        "    ExecutionProto = executions_v1.Execution if VERTEX_WORKFLOWS_SDK_AVAILABLE else MagicMock # Use MagicMock as proto fallback\n",
        "    CreateExecutionRequest = executions_v1.CreateExecutionRequest if VERTEX_WORKFLOWS_SDK_AVAILABLE else MagicMock\n",
        "    GetExecutionRequest = executions_v1.GetExecutionRequest if VERTEX_WORKFLOWS_SDK_AVAILABLE else MagicMock\n",
        "    CancelExecutionRequest = executions_v1.CancelExecutionRequest if VERTEX_WORKFLOWS_SDK_AVAILABLE else MagicMock\n",
        "    ListExecutionsRequest = executions_v1.ListExecutionsRequest if VERTEX_WORKFLOWS_SDK_AVAILABLE else MagicMock\n",
        "    ListExecutionsResponse = executions_v1.ListExecutionsResponse if VERTEX_WORKFLOWS_SDK_AVAILABLE else MagicMock\n",
        "\n",
        "    async def create_execution(self, request) -> ExecutionProto:\n",
        "        parent = request.parent\n",
        "        # execution_input = request.execution # Accessing proto fields directly might fail with MagicMock\n",
        "        input_arg = getattr(request.execution, 'argument', '{}') # Safely get argument\n",
        "\n",
        "        project, location, workflow = parent.split('/')[1], parent.split('/')[3], parent.split('/')[5]\n",
        "        exec_id_suffix = uuid.uuid4().hex[:12]; exec_name = f\"projects/{project}/locations/{location}/workflows/{workflow}/executions/{exec_id_suffix}\"\n",
        "        logger.info(f\"MOCK CreateExecution: {exec_name}\")\n",
        "        await asyncio.sleep(0.05)\n",
        "        status = self.State.ACTIVE\n",
        "        # Simulate suspension based on input payload for testing\n",
        "        try:\n",
        "            miz_oki_input = json.loads(input_arg)\n",
        "            if miz_oki_input.get(\"payload\", {}).get(\"force_suspend\") or (random.random() < 0.1 and miz_oki_input.get(\"payload\", {}).get(\"allow_suspend\")):\n",
        "                 status = self.State.SUSPENDED\n",
        "                 logger.info(f\"MOCK Execution {exec_name} starting in SUSPENDED state.\")\n",
        "        except json.JSONDecodeError: pass # Ignore if argument isn't valid JSON\n",
        "\n",
        "        start_time_dt = datetime.now(datetime.timezone.utc)\n",
        "        self._executions[exec_name] = {\"name\": exec_name, \"state\": status, \"argument\": input_arg, \"start_time\": start_time_dt, \"result\": None, \"error\": None}\n",
        "        # Return a mock object mimicking the Execution proto structure\n",
        "        mock_proto = MagicMock(spec=self.ExecutionProto)\n",
        "        mock_proto.name = exec_name\n",
        "        mock_proto.state = status\n",
        "        mock_proto.argument = input_arg\n",
        "        mock_proto.start_time = start_time_dt\n",
        "        mock_proto.result = None\n",
        "        mock_proto.error = None\n",
        "        return mock_proto\n",
        "\n",
        "    async def get_execution(self, request) -> ExecutionProto:\n",
        "        exec_name = request.name; logger.info(f\"MOCK GetExecution: {exec_name}\"); await asyncio.sleep(0.02)\n",
        "        exec_data = self._executions.get(exec_name)\n",
        "        if not exec_data: raise google_api_exceptions.NotFound(f\"Exec {exec_name} not found\")\n",
        "\n",
        "        # Simulate completion/failure progression for ACTIVE state\n",
        "        if exec_data[\"state\"] == self.State.ACTIVE:\n",
        "            if random.random() < 0.3: # 30% chance to succeed\n",
        "                 exec_data[\"state\"] = self.State.SUCCEEDED\n",
        "                 exec_data[\"result\"] = json.dumps({\"output\": \"mock_success\", \"final_status\": \"OK\"}) # Simulate MIZ OKI output\n",
        "                 logger.info(f\"MOCK Execution {exec_name} transitioned to SUCCEEDED.\")\n",
        "            elif random.random() < 0.1: # 10% chance to fail (of remaining 70%)\n",
        "                 exec_data[\"state\"] = self.State.FAILED\n",
        "                 exec_data[\"error\"] = {\"message\": \"Simulated step failure\"} # Mimic error structure\n",
        "                 logger.info(f\"MOCK Execution {exec_name} transitioned to FAILED.\")\n",
        "            # else: remains ACTIVE\n",
        "\n",
        "        # Return a mock object mimicking the Execution proto structure\n",
        "        mock_proto = MagicMock(spec=self.ExecutionProto)\n",
        "        for k, v in exec_data.items(): setattr(mock_proto, k, v)\n",
        "        # Handle proto-specific attributes if SDK is available\n",
        "        if VERTEX_WORKFLOWS_SDK_AVAILABLE: mock_proto._pb = exec_data # Allow access to underlying dict if needed\n",
        "        return mock_proto\n",
        "\n",
        "    async def cancel_execution(self, request) -> ExecutionProto:\n",
        "        exec_name = request.name; logger.info(f\"MOCK CancelExecution: {exec_name}\"); await asyncio.sleep(0.05)\n",
        "        if exec_name in self._executions and self._executions[exec_name][\"state\"] in [self.State.ACTIVE, self.State.SUSPENDED]:\n",
        "            self._executions[exec_name][\"state\"] = self.State.CANCELLED\n",
        "            logger.info(f\"MOCK Execution {exec_name} transitioned to CANCELLED.\")\n",
        "            mock_proto = MagicMock(spec=self.ExecutionProto)\n",
        "            for k, v in self._executions[exec_name].items(): setattr(mock_proto, k, v)\n",
        "            if VERTEX_WORKFLOWS_SDK_AVAILABLE: mock_proto._pb = self._executions[exec_name]\n",
        "            return mock_proto\n",
        "        logger.warning(f\"MOCK CancelExecution: Exec {exec_name} not found or already finished.\")\n",
        "        raise google_api_exceptions.FailedPrecondition(f\"Exec {exec_name} not cancellable\")\n",
        "\n",
        "    async def list_executions(self, request) -> ListExecutionsResponse:\n",
        "        logger.info(f\"MOCK ListExecutions: parent={request.parent}, filter={request.filter}\"); await asyncio.sleep(0.05)\n",
        "        results = []\n",
        "        parent_prefix = request.parent + \"/executions/\"\n",
        "        for name, data in self._executions.items():\n",
        "            if name.startswith(parent_prefix):\n",
        "                 matches_filter = True\n",
        "                 # Simple state filter parsing\n",
        "                 if request.filter and 'state = \"' in request.filter:\n",
        "                      try:\n",
        "                          expected_state_name = request.filter.split('\"')[1]\n",
        "                          expected_state = getattr(self.State, expected_state_name)\n",
        "                          matches_filter = (data.get(\"state\") == expected_state)\n",
        "                      except (IndexError, KeyError, AttributeError):\n",
        "                          logger.warning(f\"Could not parse state filter: {request.filter}\")\n",
        "                          matches_filter = False\n",
        "                 if matches_filter:\n",
        "                      mock_proto = MagicMock(spec=self.ExecutionProto)\n",
        "                      for k, v in data.items(): setattr(mock_proto, k, v)\n",
        "                      if VERTEX_WORKFLOWS_SDK_AVAILABLE: mock_proto._pb = data\n",
        "                      results.append(mock_proto)\n",
        "        # Return mock response object\n",
        "        mock_response = MagicMock(spec=self.ListExecutionsResponse)\n",
        "        mock_response.executions = results\n",
        "        return mock_response\n",
        "\n",
        "    # Mock for Pub/Sub signal pattern (used by Human API test)\n",
        "    async def publish_approval(self, topic, data): # Keep simple mock for testing API logic\n",
        "        logger.info(f\"MOCK Publishing approval signal to {topic}: {data}\")\n",
        "        await asyncio.sleep(0.02); exec_id = data.get(\"execution_id\")\n",
        "        if exec_id in self._executions and self._executions[exec_id][\"state\"] == self.State.SUSPENDED:\n",
        "             new_state = self.State.ACTIVE if data.get(\"approved\") else self.State.FAILED\n",
        "             self._executions[exec_id][\"state\"] = new_state\n",
        "             logger.info(f\"MOCK Execution {exec_id} transitioned to {new_state} due to signal.\")\n",
        "             return True\n",
        "        logger.warning(f\"MOCK Approval signal ignored: Exec {exec_id} not found or not SUSPENDED.\")\n",
        "        return False\n",
        "\n",
        "# Mock ADK Tool API Service Proxy\n",
        "class MockAdkTool:\n",
        "    def __init__(self, tool_name=\"mock_tool\"):\n",
        "        self.tool_name = tool_name\n",
        "\n",
        "    async def __call__(self, request: Dict): # Expects MIZ OKI request dict\n",
        "        logger.info(f\"Mock ADK Tool API '{self.tool_name}': Called async with request: {request.get('request_id')}\")\n",
        "        await asyncio.sleep(random.uniform(0.05, 0.15))\n",
        "        # Simulate potential failure\n",
        "        if request.get(\"payload\",{}).get(\"force_fail\") or (request.get(\"payload\",{}).get(\"fail_sometimes\") and random.random() < 0.3):\n",
        "            logger.warning(f\"Mock ADK Tool '{self.tool_name}': Simulating failure.\")\n",
        "            return {\n",
        "                \"miz_oki_version\": request.get(\"miz_oki_version\", \"3.0\"),\n",
        "                \"request_id\": request.get(\"request_id\"), \"trace_id\": request.get(\"trace_id\"),\n",
        "                \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "                \"source_component\": self.tool_name, \"target_component\": request.get(\"source_component\"),\n",
        "                \"status\": \"failed\",\n",
        "                \"error_details\": [{\"code\": \"SIMULATED_ERROR\", \"message\": f\"Simulated failure in {self.tool_name}\"}]\n",
        "            }\n",
        "        # Simulate success\n",
        "        return {\n",
        "            \"miz_oki_version\": request.get(\"miz_oki_version\", \"3.0\"),\n",
        "            \"request_id\": request.get(\"request_id\"), \"trace_id\": request.get(\"trace_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": self.tool_name, \"target_component\": request.get(\"source_component\"),\n",
        "            \"status\": \"success\",\n",
        "            \"payload\": {\"tool_result\": f\"Processed by {self.tool_name}\", \"input_payload_preview\": str(request.get(\"payload\"))[:100]}\n",
        "        }\n",
        "\n",
        "# --- Setup Logging for Tests ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('MIZ-OKI.IntegrationTest')\n",
        "\n",
        "# --- Test Suite (Reworked) ---\n",
        "class TestMIZOKIIntegrationVertexAsync(unittest.IsolatedAsyncioTestCase):\n",
        "    \"\"\"Integration tests using Vertex AI/ADK architecture (with mocks).\"\"\"\n",
        "\n",
        "    async def asyncSetUp(self):\n",
        "        logger.info(\"Setting up MIZ OKI Vertex/ADK Async Integration Test Suite...\")\n",
        "        self.maxDiff = None # Show full diff on assertion failure\n",
        "        self.config_data = { # Simple dict for test config\n",
        "            'gcp': {'project_id': 'test-proj-vertex', 'region': 'test-region-vertex', 'gcs_bucket_name': 'test-bucket'},\n",
        "            'kg': {'storage_type': 'neo4j', 'vector_index_name': 'test_index'},\n",
        "            'foundation_models': {'defaults': {'llama4_scout': 'mock-llama3-8b'}, 'keys': {'vertex': 'mock_auth'}},\n",
        "            'xai': {'storage_type': 'kg'},\n",
        "            'system_thresholds': {'human_review_confidence_threshold': 0.75},\n",
        "            'vertex_ai': {'default_workflow_id': 'test-workflow-v1'},\n",
        "            'miz_oki_schema_version': '3.0' # Added\n",
        "        }\n",
        "        # Inject mock instances (representing deployed API services/clients)\n",
        "        self.kg_tool = MockKGTool()\n",
        "        self.fm_client = MockFoundationModelClient()\n",
        "        self.workflow_client = MockVertexWorkflowClient() # Use refined mock\n",
        "        self.data_processing_tool = MockAdkTool(\"data_processing_tool\")\n",
        "        self.analysis_tool = MockAdkTool(\"analysis_tool\")\n",
        "        # Clear mock state before each test\n",
        "        self.workflow_client._executions.clear()\n",
        "        self.kg_tool._decision_logs.clear()\n",
        "\n",
        "    async def asyncTearDown(self):\n",
        "        logger.info(\"Tearing down MIZ OKI Vertex/ADK Async Integration Test Suite.\")\n",
        "\n",
        "    def _validate_miz_oki(self, data: Dict, is_request: bool = False):\n",
        "        \"\"\"Uses Pydantic to validate MIZ OKI structure if available.\"\"\"\n",
        "        if not PYDANTIC_AVAILABLE: return # Skip if Pydantic not installed\n",
        "        try:\n",
        "            MizOkiPayload(**data) # Validate against the Pydantic model\n",
        "            # Add more specific checks if needed (e.g., payload content based on status)\n",
        "        except ValidationError as e:\n",
        "            self.fail(f\"MIZ OKI Payload Validation Failed:\\n{e}\\nPayload: {json.dumps(data, indent=2)}\")\n",
        "\n",
        "    # --- Test Cases ---\n",
        "    def test_01_config_loading_access(self):\n",
        "        logger.info(\"Running test: test_01_config_loading_access\")\n",
        "        # Test accessing config data used in setup\n",
        "        self.assertEqual(self.config_data['gcp']['project_id'], 'test-proj-vertex')\n",
        "        logger.info(\"Config access test passed.\")\n",
        "\n",
        "    async def test_02_infrastructure_kg_tool_connectivity(self):\n",
        "        logger.info(\"Running test: test_02_infrastructure_kg_tool_connectivity\")\n",
        "        # Simulate a MIZ OKI request to the KG Tool proxy\n",
        "        request_data = {\"payload\": {\"query\": \"MATCH (n) RETURN count(n)\"}}\n",
        "        response = await self.kg_tool.execute_query(request=request_data)\n",
        "        self._validate_miz_oki(response) # Validate response structure\n",
        "        self.assertEqual(response.get(\"status\"), \"success\")\n",
        "        self.assertIsInstance(response.get(\"payload\", {}).get(\"results\"), list)\n",
        "        logger.info(\"KG Tool API connectivity test passed.\")\n",
        "\n",
        "    async def test_03_fmclient_invocation_async(self):\n",
        "        logger.info(\"Running test: test_03_fmclient_invocation_async\")\n",
        "        # Test text generation\n",
        "        gen_request = {\"payload\": {\"prompt\": \"Test prompt\", \"model_alias\": \"mock-model\"}}\n",
        "        response_gen = await self.fm_client.generate_text(input_data=gen_request)\n",
        "        self._validate_miz_oki(response_gen)\n",
        "        self.assertEqual(response_gen.get(\"status\"), \"success\")\n",
        "        self.assertIn(\"Mock async generation\", response_gen.get(\"payload\", {}).get(\"generated_text\", \"\"))\n",
        "        # Test embedding\n",
        "        emb_request = {\"payload\": {\"data\": \"Test embed\", \"model_alias\": \"mock-emb\"}}\n",
        "        response_emb = await self.fm_client.generate_embedding(input_data=emb_request)\n",
        "        self._validate_miz_oki(response_emb)\n",
        "        self.assertEqual(response_emb.get(\"status\"), \"success\")\n",
        "        self.assertIsInstance(response_emb.get(\"payload\", {}).get(\"embedding\"), list)\n",
        "        logger.info(\"FM Client API async invocation test passed.\")\n",
        "\n",
        "    async def test_04_vertex_workflow_start_and_status(self):\n",
        "        logger.info(\"Running test: test_04_vertex_workflow_start_and_status\")\n",
        "        workflow_id = self.config_data['vertex_ai']['default_workflow_id']\n",
        "        input_payload = {\"input_param\": \"value1\", \"allow_suspend\": False} # Prevent suspension for this test\n",
        "        # Construct the MIZ OKI payload that the AWE service (or similar) would pass\n",
        "        miz_oki_input_for_workflow = {\n",
        "            \"miz_oki_version\": \"3.0\", \"request_id\": \"req-04\", \"trace_id\": \"trace-04\",\n",
        "            \"source_component\": \"TestRunner\", \"target_component\": f\"Workflow:{workflow_id}:Step1\",\n",
        "            \"payload\": input_payload\n",
        "        }\n",
        "        # Prepare the request for the Vertex AI client proxy\n",
        "        request = self.workflow_client.CreateExecutionRequest(\n",
        "            parent=f\"projects/{self.config_data['gcp']['project_id']}/locations/{self.config_data['gcp']['region']}/workflows/{workflow_id}\",\n",
        "            execution=self.workflow_client.ExecutionProto(argument=json.dumps(miz_oki_input_for_workflow)) # Pass MIZ OKI as argument string\n",
        "        )\n",
        "        execution = await self.workflow_client.create_execution(request=request)\n",
        "        execution_name = execution.name\n",
        "        self.assertTrue(execution_name.startswith(f\"projects/{self.config_data['gcp']['project_id']}/locations/\"))\n",
        "        self.assertTrue(execution_name.endswith(execution.name.split('/')[-1])) # Check format\n",
        "        logger.info(f\"Workflow started with execution Name: {execution_name}\")\n",
        "\n",
        "        # Wait briefly and check status\n",
        "        await asyncio.sleep(0.1)\n",
        "        status_request = self.workflow_client.GetExecutionRequest(name=execution_name)\n",
        "        status_execution = await self.workflow_client.get_execution(request=status_request)\n",
        "        self.assertEqual(status_execution.name, execution_name)\n",
        "        # Check against possible states (ACTIVE, SUCCEEDED, FAILED - SUSPENDED prevented by input)\n",
        "        self.assertIn(status_execution.state, [self.workflow_client.State.ACTIVE, self.workflow_client.State.SUCCEEDED, self.workflow_client.State.FAILED])\n",
        "        logger.info(f\"Retrieved workflow status: {status_execution.state.name if hasattr(status_execution.state, 'name') else status_execution.state}\")\n",
        "        logger.info(\"Vertex Workflow start and status test passed.\")\n",
        "\n",
        "    async def test_05_vertex_workflow_human_approval_signal(self):\n",
        "        logger.info(\"Running test: test_05_vertex_workflow_human_approval_signal\")\n",
        "        workflow_id = \"approval-workflow-v1\"\n",
        "        input_payload = {\"requires_approval\": True, \"force_suspend\": True} # Force suspension\n",
        "        miz_oki_input_for_workflow = {\"payload\": input_payload, \"trace_id\": \"trace-05\"}\n",
        "        # Start workflow, forcing suspension via mock logic\n",
        "        request = self.workflow_client.CreateExecutionRequest(\n",
        "            parent=f\"projects/{self.config_data['gcp']['project_id']}/locations/{self.config_data['gcp']['region']}/workflows/{workflow_id}\",\n",
        "            execution=self.workflow_client.ExecutionProto(argument=json.dumps(miz_oki_input_for_workflow))\n",
        "        )\n",
        "        execution = await self.workflow_client.create_execution(request=request)\n",
        "        execution_id = execution.name\n",
        "\n",
        "        # Verify it's suspended\n",
        "        status_request = self.workflow_client.GetExecutionRequest(name=execution_id)\n",
        "        status_execution = await self.workflow_client.get_execution(request=status_request)\n",
        "        self.assertEqual(status_execution.state, self.workflow_client.State.SUSPENDED, \"Workflow did not suspend as expected.\")\n",
        "        logger.info(f\"Workflow {execution_id} correctly suspended.\")\n",
        "\n",
        "        # Simulate approval signal (as if sent via Pub/Sub and handled by Human API backend calling the mock)\n",
        "        approval_signal_topic = \"mock-approval-topic\"\n",
        "        signal_payload = {\"execution_id\": execution_id, \"approved\": True, \"comments\": \"Approved by test\", \"approver\": \"test_user\"}\n",
        "        resumed = await self.workflow_client.publish_approval(approval_signal_topic, signal_payload) # Call mock method directly\n",
        "        self.assertTrue(resumed, \"Mock approval signal should indicate resumption.\")\n",
        "\n",
        "        # Verify it resumed (became ACTIVE)\n",
        "        status_after = await self.workflow_client.get_execution(request=status_request)\n",
        "        self.assertEqual(status_after.state, self.workflow_client.State.ACTIVE, \"Workflow should be ACTIVE after approval.\")\n",
        "        logger.info(\"Vertex Workflow human approval signal test passed.\")\n",
        "\n",
        "    async def test_06_conceptual_workflow_step_tool_call(self):\n",
        "        logger.info(\"Running test: test_06_conceptual_workflow_step_tool_call\")\n",
        "        # Simulate input coming from previous workflow step (MIZ OKI format)\n",
        "        miz_oki_input = {\n",
        "            \"miz_oki_version\": \"3.0\", \"request_id\": \"req-step-06\", \"trace_id\": \"trace-06\",\n",
        "            \"workflow_execution_id\": \"exec-123\", \"step_id\": \"data_processing_step\",\n",
        "            \"source_component\": \"WorkflowOrchestrator\", \"target_component\": \"data_processing_tool\",\n",
        "            \"payload\": {\"data_uri\": \"gs://bucket/data.csv\", \"param\": 123}\n",
        "        }\n",
        "        self._validate_miz_oki(miz_oki_input, is_request=True) # Validate input\n",
        "\n",
        "        # Call the mock tool API proxy\n",
        "        response = await self.data_processing_tool(request=miz_oki_input)\n",
        "        self._validate_miz_oki(response) # Validate output\n",
        "        self.assertEqual(response.get(\"status\"), \"success\")\n",
        "        # Check if the tool correctly identified the caller\n",
        "        self.assertEqual(response.get(\"target_component\"), miz_oki_input[\"source_component\"])\n",
        "        self.assertEqual(response.get(\"payload\", {}).get(\"input_payload_preview\"), str(miz_oki_input[\"payload\"])[:100])\n",
        "        logger.info(\"Conceptual workflow step tool API call test passed.\")\n",
        "\n",
        "    async def test_07_workflow_failure_status(self):\n",
        "        logger.info(\"Running test: test_07_workflow_failure_status\")\n",
        "        workflow_id = \"failing-workflow-v1\"\n",
        "        miz_oki_input_for_workflow = {\"payload\": {}, \"trace_id\": \"trace-07\"}\n",
        "        request = self.workflow_client.CreateExecutionRequest(\n",
        "            parent=f\"projects/{self.config_data['gcp']['project_id']}/locations/{self.config_data['gcp']['region']}/workflows/{workflow_id}\",\n",
        "            execution=self.workflow_client.ExecutionProto(argument=json.dumps(miz_oki_input_for_workflow))\n",
        "        )\n",
        "        execution = await self.workflow_client.create_execution(request=request); exec_id = execution.name\n",
        "\n",
        "        # Force mock to fail state\n",
        "        if exec_id in self.workflow_client._executions:\n",
        "            self.workflow_client._executions[exec_id]['state'] = self.workflow_client.State.FAILED\n",
        "            self.workflow_client._executions[exec_id]['error'] = {'message': 'Simulated step failure'}\n",
        "        else:\n",
        "            self.fail(f\"Mock execution {exec_id} not found for forcing failure.\")\n",
        "\n",
        "        status_request = self.workflow_client.GetExecutionRequest(name=exec_id)\n",
        "        status = await self.workflow_client.get_execution(request=status_request)\n",
        "        self.assertEqual(status.state, self.workflow_client.State.FAILED)\n",
        "        # Check error details if SDK provides them (mock stores it)\n",
        "        self.assertIsNotNone(status.error)\n",
        "        self.assertEqual(status.error.get('message'), 'Simulated step failure')\n",
        "        logger.info(\"Workflow failure status test passed.\")\n",
        "\n",
        "    async def test_08_tool_api_error_handling(self):\n",
        "        logger.info(\"Running test: test_08_tool_api_error_handling\")\n",
        "        # Simulate input for a tool call\n",
        "        miz_oki_input = {\n",
        "            \"miz_oki_version\": \"3.0\", \"request_id\": \"req-step-08\", \"trace_id\": \"trace-08\",\n",
        "            \"payload\": {\"data_uri\": \"gs://bucket/data.csv\", \"force_fail\": True} # Use flag for mock tool\n",
        "        }\n",
        "        # Call the mock tool API proxy, expecting failure\n",
        "        response = await self.data_processing_tool(request=miz_oki_input)\n",
        "        self._validate_miz_oki(response) # Validate error response structure\n",
        "        self.assertEqual(response.get(\"status\"), \"failed\") # Changed from 'error' to 'failed' based on mock\n",
        "        self.assertIsNotNone(response.get(\"error_details\"))\n",
        "        self.assertEqual(response[\"error_details\"][0][\"code\"], \"SIMULATED_ERROR\")\n",
        "        logger.info(\"Tool API error handling test passed.\")\n",
        "\n",
        "    async def test_09_complex_workflow_simulation(self):\n",
        "        logger.info(\"Running test: test_09_complex_workflow_simulation\")\n",
        "        # Simulate sequence of tool calls as if orchestrated by a workflow\n",
        "        miz_oki_context = {\"miz_oki_version\": \"3.0\", \"request_id\": \"req-complex-09\", \"trace_id\": \"trace-09\", \"workflow_execution_id\": \"exec-complex\"}\n",
        "\n",
        "        # Step 1: Data Processing Tool\n",
        "        step1_input = {**miz_oki_context, \"step_id\": \"step1_data_proc\", \"payload\": {\"input_data\": \"start\"}}\n",
        "        self._validate_miz_oki(step1_input, is_request=True)\n",
        "        step1_response = await self.data_processing_tool(request=step1_input)\n",
        "        self._validate_miz_oki(step1_response)\n",
        "        self.assertEqual(step1_response.get(\"status\"), \"success\")\n",
        "        processed_data = step1_response.get(\"payload\", {}).get(\"tool_result\")\n",
        "\n",
        "        # Step 2: Analysis Tool (using output of Step 1)\n",
        "        step2_input = {**miz_oki_context, \"step_id\": \"step2_analysis\", \"payload\": {\"processed_data\": processed_data}}\n",
        "        self._validate_miz_oki(step2_input, is_request=True)\n",
        "        step2_response = await self.analysis_tool(request=step2_input)\n",
        "        self._validate_miz_oki(step2_response)\n",
        "        self.assertEqual(step2_response.get(\"status\"), \"success\")\n",
        "        analysis_result = step2_response.get(\"payload\", {}).get(\"tool_result\")\n",
        "\n",
        "        # Step 3: Generation Tool (using output of Step 2)\n",
        "        step3_input = {**miz_oki_context, \"step_id\": \"step3_generate\", \"payload\": {\"prompt\": f\"Generate report based on: {analysis_result}\", \"model_alias\": \"mock-model\"}}\n",
        "        self._validate_miz_oki(step3_input, is_request=True)\n",
        "        step3_response = await self.fm_client.generate_text(input_data=step3_input)\n",
        "        self._validate_miz_oki(step3_response)\n",
        "        self.assertEqual(step3_response.get(\"status\"), \"success\")\n",
        "        self.assertIsNotNone(step3_response.get(\"payload\", {}).get(\"generated_text\"))\n",
        "\n",
        "        logger.info(\"Complex workflow simulation API call sequence test passed.\")\n",
        "\n",
        "    async def test_10_xai_logging_retrieval(self):\n",
        "        logger.info(\"Running test: test_10_xai_logging_retrieval\")\n",
        "        decision_id = f\"xai_test_{uuid.uuid4().hex[:6]}\"\n",
        "        record_to_save = {\"decision_id\": decision_id, \"component\": \"TestComponent\", \"decision\": {\"action\": \"approve\"}, \"chain_of_thought\": [\"Step A\", \"Step B\"], \"timestamp\": datetime.now(datetime.timezone.utc).isoformat()}\n",
        "        trace_id = \"trace-xai-10\"\n",
        "\n",
        "        # Simulate XAI service calling KG Tool API proxy to save\n",
        "        save_request = {\"payload\": {\"record\": record_to_save}, \"trace_id\": trace_id}\n",
        "        save_response = await self.kg_tool.save_decision_record(request=save_request) # Call mock proxy method\n",
        "        self.assertEqual(save_response.get(\"status\"), \"success\")\n",
        "\n",
        "        # Simulate XAI service calling KG Tool API proxy to retrieve\n",
        "        retrieve_request = {\"payload\": {\"decision_id\": decision_id}, \"trace_id\": trace_id}\n",
        "        retrieve_response = await self.kg_tool.retrieve_decision_record(request=retrieve_request) # Call mock proxy method\n",
        "        self._validate_miz_oki(retrieve_response) # Check response structure\n",
        "        self.assertEqual(retrieve_response.get(\"status\"), \"success\")\n",
        "        retrieved_record = retrieve_response.get(\"payload\", {}).get(\"decision_record\")\n",
        "        self.assertIsNotNone(retrieved_record)\n",
        "        self.assertEqual(retrieved_record.get(\"decision_id\"), decision_id)\n",
        "        # Compare relevant fields (ignoring potential timestamp differences if not mocked precisely)\n",
        "        self.assertEqual(retrieved_record.get(\"component\"), record_to_save[\"component\"])\n",
        "        self.assertEqual(retrieved_record.get(\"decision\"), record_to_save[\"decision\"])\n",
        "        self.assertEqual(retrieved_record.get(\"chain_of_thought\"), record_to_save[\"chain_of_thought\"])\n",
        "\n",
        "        logger.info(\"XAI logging and retrieval via KG Tool API test passed.\")\n",
        "\n",
        "    # --- TODO: Add More Tests ---\n",
        "    # - Test MIZ OKI payload validation failures (if Pydantic available)\n",
        "    # - Test specific error conditions from Vertex AI API (NotFound, PermissionDenied) using mock exceptions\n",
        "    # - Test interactions between more components (e.g., PO -> HDE -> KG) via mocked APIs\n",
        "    # - Test edge cases (empty inputs, large payloads if mocks support it)\n",
        "    # - Test workflow cancellation\n",
        "\n",
        "# --- Main execution block ---\n",
        "if __name__ == '__main__':\n",
        "    # Ensure asyncio event loop is managed correctly for testing\n",
        "    # unittest.main() handles this when run directly\n",
        "    unittest.main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "n6zD48ym13OP",
        "outputId": "d9bd1167-9dd9-4942-b7ce-844cf5c361da"
      },
      "id": "n6zD48ym13OP",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-14-0228133b0c76>, line 47)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-0228133b0c76>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    class BaseModel: pass; class Field: pass; class ValidationError(Exception): pass; validator = lambda x: x # Dummy decorator\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Business Impact Monitoring (Reworked)\n",
        "# Status: Uses real KG Tool API proxy and real Vertex Workflow Executions client proxy.\n",
        "#         Includes robust workflow polling and improved error handling.\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "import random\n",
        "import datetime\n",
        "import uuid\n",
        "import json # Added for workflow interaction\n",
        "import time # Added for polling\n",
        "from typing import Dict, Any, Optional, List, Union\n",
        "import numpy as np # Added for nan handling\n",
        "\n",
        "# --- Assume necessary components are injected or globally available ---\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    # Use KG Tool API Proxy (representing deployed Cell 3 service)\n",
        "    if 'kg_tool_service_instance' not in globals(): raise NameError(\"kg_tool_service_instance proxy not found\")\n",
        "\n",
        "    # Use REAL Vertex Workflow Executions Client Proxy (representing client from Cell 16)\n",
        "    if '_workflow_executions_client' not in globals(): raise NameError(\"_workflow_executions_client not found\")\n",
        "\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _kg_tool_proxy = kg_tool_service_instance\n",
        "    _vertex_workflow_client_proxy = _workflow_executions_client\n",
        "\n",
        "    # Import real exceptions/types if SDK available (check from Cell 16/Cell 7)\n",
        "    if 'VERTEX_WORKFLOWS_SDK_AVAILABLE' not in globals(): VERTEX_WORKFLOWS_SDK_AVAILABLE = False # Assume false if not defined\n",
        "    if VERTEX_WORKFLOWS_SDK_AVAILABLE:\n",
        "         from google.api_core import exceptions as google_api_exceptions\n",
        "         from google.cloud.workflows import executions_v1\n",
        "         Execution = executions_v1.Execution\n",
        "         ExecutionState = executions_v1.Execution.State\n",
        "         CreateExecutionRequest = executions_v1.CreateExecutionRequest\n",
        "         GetExecutionRequest = executions_v1.GetExecutionRequest\n",
        "         logger.debug(\"Using real Vertex AI SDK types/exceptions for Cell 10.\")\n",
        "    else: # Use mock exceptions/types if SDK was unavailable\n",
        "         from unittest.mock import MagicMock\n",
        "         google_api_exceptions = MagicMock(); google_api_exceptions.NotFound = type('NotFound', (Exception,), {}); google_api_exceptions.GoogleAPIError = type('GoogleAPIError', (Exception,), {})\n",
        "         executions_v1 = MagicMock(); Execution = MagicMock(); ExecutionState = MagicMock(); ExecutionState.ACTIVE=\"ACTIVE\"; ExecutionState.SUCCEEDED = \"SUCCEEDED\"; ExecutionState.FAILED = \"FAILED\"; ExecutionState.CANCELLED = \"CANCELLED\"; ExecutionState.SUSPENDED=\"SUSPENDED\"; CreateExecutionRequest = MagicMock(); GetExecutionRequest = MagicMock()\n",
        "         logger.warning(\"Using mock Vertex AI SDK types/exceptions for Cell 10.\")\n",
        "\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real dependencies in Cell 10 (Reworked).\")\n",
        "\n",
        "except NameError as e:\n",
        "    logger.warning(f\"Dependency Error in Cell 10 ({e}). Using Mocks/Placeholders.\")\n",
        "    _real_dependencies = False\n",
        "    # --- Mock/Placeholder Setup ---\n",
        "    class MockKGTool: async def execute_query(self, request): await asyncio.sleep(0.02); return {\"status\": \"success\", \"payload\": {\"results\": [{'value': random.uniform(5, 10)}]}}\n",
        "    class MockVertexWorkflowClient:\n",
        "        _mock_result = {}; State = ExecutionState\n",
        "        async def create_execution(self, request): await asyncio.sleep(0.05); self._mock_result = {\"status\": \"success\", \"payload\": {\"metric_value\": random.uniform(0.04, 0.07)}}; return MagicMock(name=f\"metric_exec_{uuid.uuid4().hex[:8]}\")\n",
        "        async def get_execution(self, request): await asyncio.sleep(0.01); return MagicMock(state=self.State.SUCCEEDED, result=json.dumps(self._mock_result)) # Simulate immediate success for mock\n",
        "    from dataclasses import dataclass, field\n",
        "    @dataclass class MockGcpConfig: project_id: str = \"mock-project\"; region: str = \"mock-region\"\n",
        "    @dataclass class MockBusinessImpact: kpis: Dict = field(default_factory=lambda: {'ROAS': {'data_source': 'kg', 'query': 'ROAS_QUERY', 'target': 8.0}, 'ConversionRate': {'data_source': 'workflow', 'workflow_id': 'conv_rate_wf', 'target': 0.05}})\n",
        "    @dataclass class MockEnhancedConfig: gcp: MockGcpConfig = field(default_factory=MockGcpConfig); business_impact: MockBusinessImpact = field(default_factory=MockBusinessImpact); miz_oki_schema_version: str = \"3.0\"; def get(self, key, default=None): return getattr(self, key, default)\n",
        "    _config_obj = MockEnhancedConfig(); _kg_tool_proxy = MockKGTool(); _vertex_workflow_client_proxy = MockVertexWorkflowClient()\n",
        "    google_api_exceptions = MagicMock(); google_api_exceptions.NotFound = type('NotFound', (Exception,), {}); google_api_exceptions.GoogleAPIError = type('GoogleAPIError', (Exception,), {})\n",
        "    executions_v1 = MagicMock(); Execution = MagicMock(); ExecutionState = MagicMock(); ExecutionState.ACTIVE=\"ACTIVE\"; ExecutionState.SUCCEEDED = \"SUCCEEDED\"; ExecutionState.FAILED = \"FAILED\"; ExecutionState.CANCELLED = \"CANCELLED\"; ExecutionState.SUSPENDED=\"SUSPENDED\"; CreateExecutionRequest = MagicMock(); GetExecutionRequest = MagicMock()\n",
        "    # --- End Mock Setup ---\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('MIZ-OKI.BusinessImpact')\n",
        "\n",
        "class BusinessImpactDashboard:\n",
        "    \"\"\"Monitors KPIs asynchronously using real KG Tool API proxy and Vertex Workflow Client proxy.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EnhancedConfig, kg_tool_proxy: Any, workflow_client_proxy: Optional[Any] = None):\n",
        "        if not config or not kg_tool_proxy:\n",
        "             raise InitializationError(\"BusinessImpactDashboard requires config and KG Tool proxy.\")\n",
        "        self.config = config\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.workflow_client = workflow_client_proxy # Use the injected client proxy\n",
        "        self.project = config.gcp.project_id\n",
        "        self.location = config.gcp.region\n",
        "        self.target_metrics: Dict[str, Dict] = {}\n",
        "        # Use object dtype initially to handle mixed types and NAs gracefully\n",
        "        self.actual_metrics_history = pd.DataFrame(columns=['time_period']).astype({'time_period': 'object'})\n",
        "        if not all([self.workflow_client, self.project, self.location]):\n",
        "             logger.warning(\"BusinessImpactDashboard initialized without a valid Vertex Workflow client or GCP project/location. Workflow-based KPIs will fail.\")\n",
        "        self._load_config_metrics()\n",
        "        logger.info(f\"BusinessImpactDashboard initialized (Reworked). Tracking: {list(self.target_metrics.keys())}\")\n",
        "\n",
        "    def _load_config_metrics(self):\n",
        "        \"\"\"Loads KPI definitions from the configuration.\"\"\"\n",
        "        kpis_section = self.config.business_impact.kpis if hasattr(self.config, 'business_impact') else {}\n",
        "        if not kpis_section:\n",
        "            logger.warning(\"No KPIs found in config 'business_impact.kpis'. Dashboard will be empty.\")\n",
        "            return\n",
        "\n",
        "        self.target_metrics = kpis_section\n",
        "        metric_names = list(self.target_metrics.keys())\n",
        "        # Add columns to DataFrame if they don't exist, preserving existing data\n",
        "        for metric in metric_names:\n",
        "            if metric not in self.actual_metrics_history.columns:\n",
        "                self.actual_metrics_history[metric] = pd.Series(dtype='float64') # Use float for metrics\n",
        "        logger.info(f\"Loaded {len(self.target_metrics)} target KPI configurations.\")\n",
        "\n",
        "    async def _fetch_metric_value(self, metric_name: str, metric_config: Dict, time_period: str, trace_id: Optional[str] = None) -> Optional[float]:\n",
        "        \"\"\"Fetches a single metric value via KG Tool API proxy or Vertex Workflow Client proxy.\"\"\"\n",
        "        data_source = metric_config.get('data_source')\n",
        "        metric_value = None\n",
        "        source_identifier = \"N/A\"\n",
        "        request_id = f\"metric_fetch_{metric_name}_{uuid.uuid4().hex[:6]}\"\n",
        "        logger.debug(f\"Fetching metric '{metric_name}' for period '{time_period}' using source '{data_source}'...\")\n",
        "\n",
        "        try:\n",
        "            if data_source == 'kg':\n",
        "                query = metric_config.get('query')\n",
        "                source_identifier = f\"KG Tool Query: {query[:50]}...\"\n",
        "                if query and self.kg_tool:\n",
        "                    # Call KG Tool API proxy\n",
        "                    kg_request = {\"payload\": {\"query\": query, \"parameters\": {\"time_period\": time_period}}, \"trace_id\": trace_id, \"request_id\": request_id} # Pass time_period if query needs it\n",
        "                    kg_response = await self.kg_tool.execute_query(request=kg_request) # Call proxy method\n",
        "                    if kg_response.get(\"status\") == \"success\" and (results := kg_response.get(\"payload\", {}).get(\"results\")):\n",
        "                        # Expecting query to return a single row with a 'value' column\n",
        "                        if results[0] is not None and 'value' in results[0] and results[0]['value'] is not None:\n",
        "                            metric_value = float(results[0]['value'])\n",
        "                            logger.debug(f\"Metric '{metric_name}' fetched from KG: {metric_value}\")\n",
        "                        else: logger.warning(f\"KG query for '{metric_name}' returned invalid result structure or null value: {results[0]}\")\n",
        "                    else: logger.warning(f\"KG Tool API query for '{metric_name}' failed or returned no results: {kg_response.get('error_details')}\")\n",
        "                else: logger.warning(f\"KG Tool proxy or query missing for metric: {metric_name}\")\n",
        "\n",
        "            elif data_source == 'workflow' or data_source == 'agent': # Treat 'agent' as a workflow call\n",
        "                workflow_id = metric_config.get('workflow_id')\n",
        "                source_identifier = f\"Vertex Workflow: {workflow_id}\"\n",
        "                if workflow_id and self.workflow_client and VERTEX_WORKFLOWS_SDK_AVAILABLE:\n",
        "                    parent = f\"projects/{self.project}/locations/{self.location}/workflows/{workflow_id}\"\n",
        "                    # Prepare MIZ OKI input for the metric calculation workflow\n",
        "                    wf_input_payload = {'time_period': time_period, 'metric_name': metric_name}\n",
        "                    miz_oki_input = {\n",
        "                        \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "                        \"request_id\": request_id, \"trace_id\": trace_id,\n",
        "                        \"source_component\": \"BusinessImpactDashboard\", \"target_component\": workflow_id,\n",
        "                        \"payload\": wf_input_payload\n",
        "                    }\n",
        "                    execution_args = json.dumps(miz_oki_input)\n",
        "                    execution_proto = Execution(argument=execution_args) # Use real proto type\n",
        "                    request = CreateExecutionRequest(parent=parent, execution=execution_proto) # Use real request type\n",
        "\n",
        "                    try:\n",
        "                        # Start execution via REAL client proxy\n",
        "                        exec_response = await self.workflow_client.create_execution(request=request)\n",
        "                        execution_name = exec_response.name\n",
        "                        logger.info(f\"Started metric workflow {execution_name} for '{metric_name}'. Polling for result...\")\n",
        "\n",
        "                        # --- Robust Polling ---\n",
        "                        max_wait_seconds = 300; poll_interval = 10; start_poll = time.monotonic(); final_status = None\n",
        "                        while time.monotonic() - start_poll < max_wait_seconds:\n",
        "                             await asyncio.sleep(poll_interval)\n",
        "                             try:\n",
        "                                 status_request = GetExecutionRequest(name=execution_name) # Use real request type\n",
        "                                 current_exec = await self.workflow_client.get_execution(request=status_request) # Call proxy\n",
        "                                 final_status = current_exec.state\n",
        "                             except google_api_exceptions.NotFound:\n",
        "                                 logger.error(f\"Workflow execution {execution_name} disappeared during polling.\")\n",
        "                                 final_status = ExecutionState.FAILED # Treat as failure\n",
        "                                 break\n",
        "                             except Exception as poll_err:\n",
        "                                 logger.warning(f\"Error polling workflow {execution_name}: {poll_err}. Retrying...\")\n",
        "                                 continue # Continue polling\n",
        "\n",
        "                             if final_status in [ExecutionState.SUCCEEDED, ExecutionState.FAILED, ExecutionState.CANCELLED]:\n",
        "                                  logger.info(f\"Workflow {execution_name} finished polling with status: {final_status.name}\")\n",
        "                                  if final_status == ExecutionState.SUCCEEDED:\n",
        "                                       wf_output_str = current_exec.result\n",
        "                                       if wf_output_str:\n",
        "                                            try:\n",
        "                                                # Expecting workflow to return MIZ OKI response JSON string\n",
        "                                                wf_output = json.loads(wf_output_str)\n",
        "                                                if wf_output.get(\"status\") == \"success\" and \"metric_value\" in wf_output.get(\"payload\", {}):\n",
        "                                                     metric_value = float(wf_output[\"payload\"][\"metric_value\"])\n",
        "                                                     logger.debug(f\"Metric '{metric_name}' fetched from workflow: {metric_value}\")\n",
        "                                                else: logger.warning(f\"Workflow '{workflow_id}' succeeded but returned invalid MIZ OKI payload or status: {wf_output_str[:200]}...\")\n",
        "                                            except json.JSONDecodeError as json_e: logger.warning(f\"Workflow '{workflow_id}' succeeded but result was not valid JSON: {json_e}\")\n",
        "                                            except (ValueError, TypeError) as val_e: logger.warning(f\"Workflow '{workflow_id}' succeeded but metric_value was invalid: {val_e}\")\n",
        "                                       else: logger.warning(f\"Workflow '{workflow_id}' succeeded but returned no result string.\")\n",
        "                                  else: logger.warning(f\"Workflow '{workflow_id}' for metric '{metric_name}' did not succeed. Status: {final_status.name}\")\n",
        "                                  break # Exit polling loop\n",
        "                             else: logger.debug(f\"Workflow {execution_name} running ({final_status.name})... polling again in {poll_interval}s\")\n",
        "                        else: # Loop finished without break (timeout)\n",
        "                            logger.error(f\"Timeout ({max_wait_seconds}s) waiting for workflow {execution_name} for metric '{metric_name}'.\")\n",
        "                        # --- End Polling ---\n",
        "                    except google_api_exceptions.NotFound: logger.error(f\"Workflow '{workflow_id}' not found.\")\n",
        "                    except google_api_exceptions.GoogleAPIError as api_e: logger.error(f\"API Error interacting with workflow '{workflow_id}': {api_e}\")\n",
        "                    except Exception as wf_e: logger.error(f\"Error during workflow execution/polling for '{metric_name}': {wf_e}\", exc_info=True)\n",
        "                elif not self.workflow_client or not VERTEX_WORKFLOWS_SDK_AVAILABLE: logger.warning(f\"Vertex Workflow Client proxy/SDK unavailable for metric: {metric_name}\")\n",
        "                else: logger.warning(f\"Workflow ID missing for metric: {metric_name}\")\n",
        "\n",
        "            elif data_source == 'manual' or data_source is None:\n",
        "                source_identifier = \"Manual/None\"; logger.info(f\"Metric '{metric_name}' requires manual input or has no source defined.\"); metric_value = None # Explicitly None\n",
        "            else:\n",
        "                source_identifier = f\"Unsupported: {data_source}\"; logger.error(f\"Unsupported data source '{data_source}' for metric: {metric_name}\")\n",
        "\n",
        "        except asyncio.TimeoutError: logger.error(f\"Timeout fetching metric '{metric_name}' from {source_identifier}.\")\n",
        "        except ConnectionError as ce: logger.error(f\"Connection error fetching metric '{metric_name}': {ce}\")\n",
        "        except ValueError as ve: logger.error(f\"Value error processing metric '{metric_name}': {ve}\")\n",
        "        except Exception as e: logger.error(f\"Failed to fetch metric '{metric_name}' async from {source_identifier}: {e}\", exc_info=True); metric_value = None\n",
        "\n",
        "        # Final validation and conversion to float or None\n",
        "        try:\n",
        "            if metric_value is None or np.isnan(metric_value) or np.isinf(metric_value):\n",
        "                return None\n",
        "            return float(metric_value)\n",
        "        except (ValueError, TypeError):\n",
        "            logger.error(f\"Final value for metric '{metric_name}' is not a valid number: {metric_value}\")\n",
        "            return None\n",
        "\n",
        "    async def import_actual_metrics(self, time_period: str) -> bool:\n",
        "        \"\"\"Imports actual metric values asynchronously for a given time period.\"\"\"\n",
        "        logger.info(f\"Importing actual metrics async for time period: {time_period}\")\n",
        "        new_metrics = {'time_period': time_period}\n",
        "        any_success = False\n",
        "        all_success = True\n",
        "        fetch_tasks = []\n",
        "        metric_names_ordered = list(self.target_metrics.keys())\n",
        "\n",
        "        # Create tasks for fetching each metric value\n",
        "        for metric_name in metric_names_ordered:\n",
        "            if metric_name in self.target_metrics:\n",
        "                fetch_tasks.append(\n",
        "                    self._fetch_metric_value(metric_name, self.target_metrics[metric_name], time_period)\n",
        "                )\n",
        "            else:\n",
        "                logger.warning(f\"Metric '{metric_name}' defined in history columns but not in target_metrics config. Skipping fetch.\")\n",
        "                fetch_tasks.append(asyncio.sleep(0, result=None)) # Add placeholder task\n",
        "\n",
        "        # Execute tasks concurrently\n",
        "        metric_values_or_exceptions = await asyncio.gather(*fetch_tasks, return_exceptions=True)\n",
        "\n",
        "        # Process results\n",
        "        for i, value_or_exception in enumerate(metric_values_or_exceptions):\n",
        "            metric_name = metric_names_ordered[i]\n",
        "            if isinstance(value_or_exception, Exception):\n",
        "                logger.error(f\"Exception fetching metric '{metric_name}': {value_or_exception}\", exc_info=False) # Avoid overly verbose logs for expected failures\n",
        "                new_metrics[metric_name] = pd.NA # Use pandas NA for missing values\n",
        "                all_success = False\n",
        "            elif value_or_exception is None:\n",
        "                logger.warning(f\"Could not retrieve value for metric: {metric_name}\")\n",
        "                new_metrics[metric_name] = pd.NA\n",
        "                all_success = False\n",
        "            else:\n",
        "                new_metrics[metric_name] = value_or_exception\n",
        "                any_success = True\n",
        "\n",
        "        # Update DataFrame safely\n",
        "        try:\n",
        "            # Ensure all expected columns exist before creating the new row DataFrame\n",
        "            new_row_data = {col: new_metrics.get(col, pd.NA) for col in self.actual_metrics_history.columns}\n",
        "            new_row = pd.DataFrame([new_row_data], columns=self.actual_metrics_history.columns)\n",
        "\n",
        "            # Ensure correct dtypes before concat (especially for metrics which should be float)\n",
        "            for metric_name in self.target_metrics.keys():\n",
        "                 if metric_name in new_row.columns:\n",
        "                     new_row[metric_name] = pd.to_numeric(new_row[metric_name], errors='coerce')\n",
        "\n",
        "            # Append the new row\n",
        "            self.actual_metrics_history = pd.concat([self.actual_metrics_history, new_row], ignore_index=True)\n",
        "\n",
        "            # Optional: Persist DataFrame to GCS/BQ here\n",
        "            # self.persist_history()\n",
        "\n",
        "            logger.info(f\"Finished importing metrics async for {time_period}. History size: {len(self.actual_metrics_history)}. AnySuccess: {any_success}, AllSuccess: {all_success}\")\n",
        "        except Exception as df_e:\n",
        "            logger.error(f\"Error updating metrics history DataFrame: {df_e}\", exc_info=True)\n",
        "            return False # Indicate failure to update history\n",
        "\n",
        "        return any_success\n",
        "\n",
        "    def calculate_kpi_trends(self, window: int = 5) -> Dict[str, Optional[float]]:\n",
        "        \"\"\"Calculates simple trends (slope) for KPIs over a rolling window (sync).\"\"\"\n",
        "        trends = {}\n",
        "        if len(self.actual_metrics_history) < 2:\n",
        "            return {metric: None for metric in self.target_metrics} # Not enough data for trend\n",
        "\n",
        "        # Ensure metrics columns are numeric, coercing errors\n",
        "        numeric_df = self.actual_metrics_history.copy()\n",
        "        for metric in self.target_metrics:\n",
        "            if metric in numeric_df.columns:\n",
        "                numeric_df[metric] = pd.to_numeric(numeric_df[metric], errors='coerce')\n",
        "\n",
        "        relevant_history = numeric_df.tail(window)\n",
        "\n",
        "        for metric in self.target_metrics:\n",
        "            if metric in relevant_history.columns:\n",
        "                values = relevant_history[metric].dropna()\n",
        "                if len(values) >= 2:\n",
        "                    # Simple linear regression slope calculation\n",
        "                    x = np.arange(len(values))\n",
        "                    try:\n",
        "                        # Use numpy polyfit for slope\n",
        "                        slope, _ = np.polyfit(x, values, 1)\n",
        "                        trends[metric] = float(slope) if not np.isnan(slope) else None\n",
        "                    except (np.linalg.LinAlgError, ValueError, TypeError):\n",
        "                         trends[metric] = None # Handle cases where polyfit fails\n",
        "                else:\n",
        "                    trends[metric] = None # Not enough data points in window\n",
        "            else:\n",
        "                trends[metric] = None # Metric column doesn't exist\n",
        "        return trends\n",
        "\n",
        "    def generate_dashboard_data(self, time_period: Optional[str] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Generates data structure for dashboard display (sync). Handles NA/NaN for JSON.\"\"\"\n",
        "        if self.actual_metrics_history.empty:\n",
        "            logger.warning(\"Metrics history is empty. Cannot generate dashboard data.\")\n",
        "            return {\"summary\": {}, \"trends\": {}, \"history\": []}\n",
        "\n",
        "        # Select the latest row or a specific time period\n",
        "        if time_period:\n",
        "            latest_data = self.actual_metrics_history[self.actual_metrics_history['time_period'] == time_period].tail(1)\n",
        "            if latest_data.empty:\n",
        "                 logger.warning(f\"No data found for time period '{time_period}'. Using latest overall.\")\n",
        "                 latest_data = self.actual_metrics_history.tail(1)\n",
        "        else:\n",
        "            latest_data = self.actual_metrics_history.tail(1)\n",
        "\n",
        "        if latest_data.empty: # Should not happen if history is not empty, but check anyway\n",
        "             logger.error(\"Cannot select latest data row for dashboard.\")\n",
        "             return {\"summary\": {}, \"trends\": {}, \"history\": []}\n",
        "\n",
        "        latest_row = latest_data.iloc[0]\n",
        "        dashboard_data = {\"summary\": {}, \"trends\": {}, \"history\": []}\n",
        "\n",
        "        # Generate Summary\n",
        "        for metric_name, config in self.target_metrics.items():\n",
        "            actual_raw = latest_row.get(metric_name)\n",
        "            target = config.get('target') # Target can be None\n",
        "            status = \"Data Missing\"; actual_display = \"N/A\"\n",
        "\n",
        "            if pd.notna(actual_raw):\n",
        "                try:\n",
        "                    actual_float = float(actual_raw)\n",
        "                    actual_display = f\"{actual_float:.3f}\" # Format for display\n",
        "                    if target is not None:\n",
        "                        try:\n",
        "                            target_float = float(target)\n",
        "                            lower_is_better = config.get('lower_is_better', False)\n",
        "                            # Add tolerance for meeting target?\n",
        "                            tolerance = config.get('target_tolerance', 0.01) # e.g., 1% tolerance\n",
        "                            if lower_is_better:\n",
        "                                is_meeting = actual_float <= target_float * (1 + tolerance)\n",
        "                            else:\n",
        "                                is_meeting = actual_float >= target_float * (1 - tolerance)\n",
        "                            status = \"Meeting Target\" if is_meeting else (\"Above Target\" if not lower_is_better else \"Below Target\")\n",
        "                        except (ValueError, TypeError):\n",
        "                            status = \"Invalid Target\"; target = str(target) # Display target as string if invalid\n",
        "                    else:\n",
        "                        status = \"No Target Set\"\n",
        "                except (ValueError, TypeError):\n",
        "                     status = \"Invalid Data\"; actual_display = str(actual_raw) # Display raw value if invalid\n",
        "            else:\n",
        "                 status = \"Data Missing\" # Keep status if actual is NA\n",
        "\n",
        "            dashboard_data[\"summary\"][metric_name] = {\n",
        "                \"actual\": actual_display,\n",
        "                \"target\": target, # Keep target as is (could be None)\n",
        "                \"unit\": config.get('unit', ''),\n",
        "                \"status\": status\n",
        "            }\n",
        "\n",
        "        # Generate Trends\n",
        "        dashboard_data[\"trends\"] = self.calculate_kpi_trends()\n",
        "\n",
        "        # Generate History (convert NA/NaN to None for JSON compatibility)\n",
        "        history_df_cleaned = self.actual_metrics_history.replace({pd.NA: None, np.nan: None})\n",
        "        dashboard_data[\"history\"] = history_df_cleaned.to_dict('records')\n",
        "\n",
        "        logger.info(f\"Generated dashboard data structure for period: {latest_row.get('time_period')}.\")\n",
        "        return dashboard_data\n",
        "\n",
        "    def persist_history(self, gcs_path: Optional[str] = None):\n",
        "        \"\"\"Placeholder: Persists the metrics history DataFrame.\"\"\"\n",
        "        # --- TODO: Implement persistence ---\n",
        "        # Option 1: Save to GCS as CSV/Parquet\n",
        "        # Option 2: Append/Upsert to a BigQuery table\n",
        "        if gcs_path:\n",
        "             try:\n",
        "                 # Ensure directory exists (if using local path for testing)\n",
        "                 # os.makedirs(os.path.dirname(gcs_path), exist_ok=True)\n",
        "                 # self.actual_metrics_history.to_csv(gcs_path, index=False)\n",
        "                 logger.info(f\"Placeholder: Persisted metrics history to {gcs_path}\")\n",
        "             except Exception as e:\n",
        "                  logger.error(f\"Failed to persist history to {gcs_path}: {e}\")\n",
        "        else:\n",
        "             logger.debug(\"No persistence path provided for metrics history.\")\n",
        "        # --- End TODO ---\n",
        "\n",
        "# --- Initialization (Conceptual - within API service or main script) ---\n",
        "# dashboard: Optional[BusinessImpactDashboard] = None\n",
        "# if _config_obj and _kg_tool_proxy and _vertex_workflow_client_proxy:\n",
        "#     try:\n",
        "#         dashboard = BusinessImpactDashboard(_config_obj, _kg_tool_proxy, _vertex_workflow_client_proxy)\n",
        "#         # Optional: Load previous history if persisted\n",
        "#         # dashboard.load_history(...)\n",
        "#     except Exception as e:\n",
        "#         logger.critical(f\"Dashboard initialization failed: {e}\", exc_info=True)\n",
        "# else:\n",
        "#     logger.critical(\"Dashboard initialization failed: Missing critical dependencies (Config, KG Tool, Workflow Client).\")\n",
        "\n",
        "# --- Example Usage (Conceptual - e.g., called by a scheduled job or API endpoint) ---\n",
        "# async def run_metric_import_and_get_dashboard():\n",
        "#     if dashboard:\n",
        "#         current_period = datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d\") # Example: daily period\n",
        "#         success = await dashboard.import_actual_metrics(current_period)\n",
        "#         if success:\n",
        "#             dashboard_data = dashboard.generate_dashboard_data()\n",
        "#             # dashboard.persist_history() # Persist after successful import\n",
        "#             return dashboard_data\n",
        "#         else:\n",
        "#             logger.error(f\"Metric import failed for period {current_period}.\")\n",
        "#             return {\"error\": \"Metric import failed\"}\n",
        "#     else:\n",
        "#         return {\"error\": \"Dashboard not initialized\"}\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 Business Impact Dashboard Logic (Cell 10 - Reworked) ---\")\n",
        "print(\"Uses real KG Tool API proxy and real Vertex Workflow Executions client proxy.\")\n",
        "print(\"Includes robust workflow polling and improved error handling.\")\n",
        "print(\"Handles NA/NaN values for DataFrame operations and JSON output.\")\n",
        "print(\"--------------------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "oaioNCZi1-Z0",
        "outputId": "98cd88f4-db70-4fb7-f0c8-f72499126b11"
      },
      "id": "oaioNCZi1-Z0",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-15-a5b4309c8de0>, line 55)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-a5b4309c8de0>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    class MockKGTool: async def execute_query(self, request): await asyncio.sleep(0.02); return {\"status\": \"success\", \"payload\": {\"results\": [{'value': random.uniform(5, 10)}]}}\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Explainable AI (XAI) (Reworked)\n",
        "# Status: Uses real KG Tool API proxy & FM Client API proxy.\n",
        "#         Calls KG Tool proxy for structured storage ('kg' mode).\n",
        "#         Replaced file logging with Cloud Logging recommendation ('log_file' mode).\n",
        "\n",
        "import logging\n",
        "import json\n",
        "import datetime\n",
        "import asyncio\n",
        "from typing import Dict, Any, Optional, List, Union, Callable\n",
        "import os\n",
        "import re # For potential parsing if needed\n",
        "\n",
        "# --- Assume necessary components are injected or globally available ---\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    # Use FM Client API Proxy (representing deployed Cell 18 service)\n",
        "    if 'foundation_model_client' not in globals(): raise NameError(\"foundation_model_client proxy not found\")\n",
        "\n",
        "    # Use KG Tool API Proxy (representing deployed Cell 3 service)\n",
        "    if 'kg_tool_service_instance' not in globals(): raise NameError(\"kg_tool_service_instance proxy not found\")\n",
        "\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _fm_client_proxy = foundation_model_client\n",
        "    _kg_tool_proxy = kg_tool_service_instance\n",
        "    _moe_manager_proxy = None # Not directly needed for CoT/Counterfactual generation here\n",
        "\n",
        "    # Import Cloud Logging client library (optional, based on availability)\n",
        "    try:\n",
        "        from google.cloud import logging as cloud_logging\n",
        "        CLOUD_LOGGING_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        CLOUD_LOGGING_AVAILABLE = False\n",
        "        logging.warning(\"google-cloud-logging library not found. Cloud Logging for XAI unavailable.\")\n",
        "\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real dependencies in Cell 11 (Reworked).\")\n",
        "\n",
        "except NameError as e:\n",
        "    logger.warning(f\"Dependency Error in Cell 11 ({e}). Using Mocks/Placeholders.\")\n",
        "    _real_dependencies = False\n",
        "    CLOUD_LOGGING_AVAILABLE = False\n",
        "    # --- Mock/Placeholder Setup ---\n",
        "    class MockFoundationModelClient: async def generate_text(self, input_data): await asyncio.sleep(0.05); return {\"status\": \"success\", \"payload\": {\"generated_text\": \"Simulated Counterfactual...\"}}\n",
        "    class MockKGTool:\n",
        "        _decision_logs = {}\n",
        "        async def save_decision_record(self, request: Dict): record = request.get(\"payload\",{}).get(\"record\",{}); decision_id = record.get('decision_id'); self._decision_logs[decision_id] = record; return {\"status\": \"success\"}\n",
        "        async def retrieve_decision_record(self, request: Dict): decision_id = request.get(\"payload\",{}).get(\"decision_id\"); record = self._decision_logs.get(decision_id); return {\"status\": \"success\" if record else \"not_found\", \"payload\": {\"decision_record\": record}}\n",
        "    # Define minimal config if needed\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ:\n",
        "        from dataclasses import dataclass, field\n",
        "        @dataclass class MockFmDefaults: llama4_maverick: str = \"mock-mav\"; llama4_scout: str = \"mock-scout\"; feedback_analyzer_model: str = \"mock-analyzer\"\n",
        "        @dataclass class MockFmConfig: defaults: MockFmDefaults = field(default_factory=MockFmDefaults)\n",
        "        @dataclass class MockXaiConfig: storage_type: str = \"kg\"; log_name: str = \"mock_xai_log\"; counterfactual_model_alias: str = \"llama4_maverick\"\n",
        "        @dataclass class MockEnhancedConfig: foundation_models: MockFmConfig = field(default_factory=MockFmConfig); xai: MockXaiConfig = field(default_factory=MockXaiConfig); miz_oki_schema_version: str = \"3.0\"; def get_model_info(self, alias): return {\"provider\": \"mock\", \"model_id\": alias, \"pricing\": {\"prompt\": 0.1, \"completion\": 0.2}}; def get(self, key, default=None): parts=key.split('.'); val=self; try: [val := getattr(val, p) for p in parts]; return val; except: return default\n",
        "        _config_obj = MockEnhancedConfig()\n",
        "    _fm_client_proxy = MockFoundationModelClient(); _kg_tool_proxy = MockKGTool(); _moe_manager_proxy = None\n",
        "    # --- End Mock Setup ---\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('MIZ-OKI.ExplainableAI')\n",
        "\n",
        "class ExplainableAI:\n",
        "    \"\"\"Provides XAI capabilities, storing logs via KG Tool API or Cloud Logging. Deployed as a service.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EnhancedConfig, fm_client_proxy: Optional[Any] = None, kg_tool_proxy: Optional[Any] = None):\n",
        "        if not config: raise InitializationError(\"Config required for ExplainableAI.\")\n",
        "        self.config = config\n",
        "        self.fm_client = fm_client_proxy\n",
        "        self.kg_tool = kg_tool_proxy\n",
        "        self.xai_storage_type = config.xai.storage_type.lower() # 'kg' or 'log_file'\n",
        "        self.log_name = config.xai.log_name # Used if storage_type is 'log_file'\n",
        "        self.counterfactual_model_info = config.get_model_info(config.xai.counterfactual_model_alias)\n",
        "        self.counterfactual_model_alias = self.counterfactual_model_info.get(\"model_id\") if self.counterfactual_model_info else config.foundation_models.defaults.llama4_maverick # Fallback\n",
        "\n",
        "        self._cloud_logging_client = None\n",
        "        self.cloud_logger = None\n",
        "        if self.xai_storage_type == 'log_file':\n",
        "            if CLOUD_LOGGING_AVAILABLE:\n",
        "                try:\n",
        "                    self._cloud_logging_client = cloud_logging.Client(project=config.gcp.project_id)\n",
        "                    self.cloud_logger = self._cloud_logging_client.logger(self.log_name)\n",
        "                    logger.info(f\"Using Cloud Logging for XAI storage (Log Name: {self.log_name}).\")\n",
        "                except Exception as log_init_e:\n",
        "                    logger.error(f\"Failed to initialize Cloud Logging client: {log_init_e}. XAI storage via Cloud Logging might fail.\", exc_info=True)\n",
        "                    self._cloud_logging_client = None; self.cloud_logger = None\n",
        "            else:\n",
        "                logger.warning(\"XAI storage set to 'log_file' but Cloud Logging SDK unavailable. XAI storage will be disabled.\")\n",
        "                self.xai_storage_type = 'none' # Disable storage if SDK missing\n",
        "\n",
        "        # Dependency checks\n",
        "        if not self.fm_client: logger.warning(\"XAI initialized without FoundationModelClient proxy. Counterfactual explanations unavailable.\")\n",
        "        if self.xai_storage_type == 'kg' and not self.kg_tool:\n",
        "            logger.error(\"XAI storage set to 'kg' but KG Tool proxy is missing. Decision logging/retrieval will fail.\")\n",
        "            self.xai_storage_type = 'none' # Disable storage if dependency missing\n",
        "\n",
        "        logger.info(f\"ExplainableAI initialized (Reworked). Storage: {self.xai_storage_type}, Counterfactual Model: {self.counterfactual_model_alias}\")\n",
        "\n",
        "    def _create_miz_oki_response(self, request_data: Dict, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response.\"\"\"\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_data.get(\"request_id\"), \"trace_id\": request_data.get(\"trace_id\"),\n",
        "            \"workflow_execution_id\": request_data.get(\"workflow_execution_id\"), \"step_id\": request_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"ExplainableAI\", \"target_component\": request_data.get(\"source_component\"),\n",
        "            \"status\": status, \"payload\": payload, \"error_details\": errors, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "    def record_decision(self, input_data: Dict[str, Any]): # Expects MIZ OKI payload\n",
        "        \"\"\"\n",
        "        Records decision details asynchronously based on configured storage type.\n",
        "        Expects MIZ OKI payload containing the record in `payload.record`.\n",
        "        This method itself is sync but schedules the async storage task.\n",
        "        \"\"\"\n",
        "        if self.xai_storage_type == 'none':\n",
        "            logger.debug(\"XAI storage is disabled. Skipping decision recording.\")\n",
        "            return\n",
        "\n",
        "        record = input_data.get(\"payload\", {}).get(\"record\", {})\n",
        "        decision_id = record.get('decision_id')\n",
        "        if not decision_id:\n",
        "            logger.error(\"Cannot record decision: missing 'decision_id' in record payload.\")\n",
        "            return # Don't schedule if ID is missing\n",
        "\n",
        "        # Add timestamp if missing\n",
        "        if 'timestamp' not in record:\n",
        "            record['timestamp'] = datetime.now(datetime.timezone.utc).isoformat()\n",
        "        # Add trace info from MIZ OKI header\n",
        "        record['trace_id'] = input_data.get('trace_id')\n",
        "        record['request_id'] = input_data.get('request_id')\n",
        "\n",
        "        logger.info(f\"Scheduling async recording for decision: {decision_id} from {record.get('component')} (Trace: {record['trace_id']})\")\n",
        "        # Schedule the async storage task to run in the background\n",
        "        asyncio.create_task(self._store_record_async(record), name=f\"store_xai_{decision_id}\")\n",
        "\n",
        "    async def _store_record_async(self, record: Dict):\n",
        "        \"\"\"Stores the record asynchronously using KG Tool API proxy or Cloud Logging.\"\"\"\n",
        "        decision_id = record['decision_id']\n",
        "        logger.debug(f\"Attempting async storage for decision {decision_id} using {self.xai_storage_type}.\")\n",
        "        try:\n",
        "            if self.xai_storage_type == 'kg':\n",
        "                if self.kg_tool and hasattr(self.kg_tool, 'save_decision_record'):\n",
        "                    # Call KG Tool API proxy with MIZ OKI payload\n",
        "                    kg_request = {\n",
        "                        \"payload\": {\"record\": record},\n",
        "                        \"trace_id\": record.get(\"trace_id\"), # Pass trace ID\n",
        "                        \"request_id\": f\"kg_save_xai_{decision_id}\"\n",
        "                    }\n",
        "                    kg_response = await self.kg_tool.save_decision_record(request=kg_request) # Call proxy method\n",
        "                    if kg_response.get(\"status\") == \"success\":\n",
        "                        logger.debug(f\"Stored decision {decision_id} via KG Tool API async.\")\n",
        "                    else:\n",
        "                        logger.error(f\"KG Tool API proxy failed to save decision record {decision_id}: {kg_response.get('error_details')}\")\n",
        "                else:\n",
        "                    logger.error(f\"Cannot store decision {decision_id} in KG: KG Tool proxy or method unavailable.\")\n",
        "\n",
        "            elif self.xai_storage_type == 'log_file':\n",
        "                if self.cloud_logger: # Use Cloud Logging if available\n",
        "                    try:\n",
        "                        # log_struct is synchronous, run in thread\n",
        "                        await asyncio.to_thread(self.cloud_logger.log_struct, record, severity='INFO')\n",
        "                        logger.debug(f\"Stored decision {decision_id} to Cloud Logging.\")\n",
        "                    except Exception as cl_e:\n",
        "                        logger.error(f\"Failed to write XAI log to Cloud Logging: {cl_e}\", exc_info=True)\n",
        "                else:\n",
        "                    # Fallback to local file (NOT RECOMMENDED for production)\n",
        "                    # This path should ideally not be hit if Cloud Logging SDK check fails in init\n",
        "                    logger.critical(f\"Cloud Logging unavailable. Cannot store XAI log {decision_id}.\")\n",
        "                    # await asyncio.to_thread(self._append_to_local_log_file, record) # Avoid file I/O in prod\n",
        "\n",
        "            # else: storage type is 'none' or unsupported, do nothing\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to store decision record {decision_id} async: {e}\", exc_info=True)\n",
        "\n",
        "    # --- Local file methods removed as Cloud Logging is the preferred non-KG option ---\n",
        "    # def _append_to_local_log_file(self, record: Dict): ...\n",
        "    # def _find_in_local_log_file(self, decision_id: str) -> Optional[Dict]: ...\n",
        "\n",
        "    async def _retrieve_decision_log_async(self, decision_id: str, trace_id: Optional[str] = None) -> Optional[Dict]:\n",
        "        \"\"\"Retrieves a decision record async via KG Tool API proxy or Cloud Logging.\"\"\"\n",
        "        logger.info(f\"Retrieving decision log async for ID: {decision_id}\")\n",
        "        start_time = time.monotonic(); record = None\n",
        "        try:\n",
        "            if self.xai_storage_type == 'kg':\n",
        "                if self.kg_tool and hasattr(self.kg_tool, 'retrieve_decision_record'):\n",
        "                    # Call KG Tool API proxy\n",
        "                    kg_request = {\n",
        "                        \"payload\": {\"decision_id\": decision_id},\n",
        "                        \"trace_id\": trace_id, \"request_id\": f\"kg_get_xai_{decision_id}\"\n",
        "                    }\n",
        "                    kg_response = await self.kg_tool.retrieve_decision_record(request=kg_request) # Call proxy method\n",
        "                    if kg_response.get(\"status\") == \"success\":\n",
        "                        record = kg_response.get(\"payload\", {}).get(\"decision_record\")\n",
        "                    elif kg_response.get(\"status\") != \"not_found\":\n",
        "                        logger.error(f\"KG Tool API proxy failed to retrieve log {decision_id}: {kg_response.get('error_details')}\")\n",
        "                else:\n",
        "                    logger.error(\"Cannot retrieve log from KG: KG Tool proxy or method unavailable.\")\n",
        "\n",
        "            elif self.xai_storage_type == 'log_file':\n",
        "                if self._cloud_logging_client and CLOUD_LOGGING_AVAILABLE:\n",
        "                    # --- TODO: Implement Cloud Logging query ---\n",
        "                    # Requires google-cloud-logging library\n",
        "                    # This is synchronous and needs careful handling in async context\n",
        "                    logger.warning(\"Cloud Logging retrieval for XAI not implemented. Requires sync-to-async handling.\")\n",
        "                    # Example sync logic (needs to be run in thread):\n",
        "                    # filter_str = f'jsonPayload.decision_id=\"{decision_id}\"'\n",
        "                    # try:\n",
        "                    #     iterator = self._cloud_logging_client.list_entries(filter_=filter_str, order_by=cloud_logging.DESCENDING, max_results=1)\n",
        "                    #     entries = list(iterator) # Blocking call\n",
        "                    #     if entries: record = entries[0].payload\n",
        "                    # except Exception as cl_e: logger.error(f\"Error querying Cloud Logging for {decision_id}: {cl_e}\")\n",
        "                    # --- End TODO ---\n",
        "                else:\n",
        "                     logger.error(\"Cannot retrieve log: Cloud Logging client unavailable.\")\n",
        "\n",
        "            # else: storage type is 'none' or unsupported\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error retrieving decision log {decision_id} async: {e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "        duration = (time.monotonic() - start_time) * 1000\n",
        "        if record: logger.info(f\"Retrieved decision log {decision_id} async in {duration:.2f} ms.\")\n",
        "        else: logger.warning(f\"Decision log {decision_id} not found (storage: {self.xai_storage_type}). Retrieval took {duration:.2f} ms.\")\n",
        "        return record\n",
        "\n",
        "    async def explain_decision(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Generates explanation async. Expects/Returns MIZ OKI payload.\"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); decision_id = payload.get(\"decision_id\"); method = payload.get(\"method\", 'chain_of_thought')\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not decision_id: errors.append({\"code\": \"MISSING_ID\", \"message\": \"'payload.decision_id' is required.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        logger.info(f\"Generating explanation async for decision {decision_id} using method: {method}\")\n",
        "        status = \"pending\"; response_payload = None\n",
        "        explanation = f\"Explanation failed: Method '{method}' processing error.\" # Default error\n",
        "\n",
        "        decision_log = await self._retrieve_decision_log_async(decision_id, trace_id=trace_id)\n",
        "        if not decision_log:\n",
        "            errors.append({\"code\": \"LOG_NOT_FOUND\", \"message\": f\"Decision log not found for ID: {decision_id}.\"})\n",
        "            status = \"not_found\"\n",
        "            response = self._create_miz_oki_response(input_data, status, response_payload, errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        try:\n",
        "            if method == 'chain_of_thought':\n",
        "                cot = decision_log.get('chain_of_thought')\n",
        "                if cot and isinstance(cot, list):\n",
        "                    # Format CoT nicely\n",
        "                    explanation_parts = [f\"Explanation ({method}) for Decision ID: {decision_id}\"]\n",
        "                    explanation_parts.extend([f\"- {step}\" for step in cot])\n",
        "                    explanation = \"\\n\".join(explanation_parts)\n",
        "                    status = \"success\"\n",
        "                else:\n",
        "                    errors.append({\"code\": \"NO_COT\", \"message\": \"No valid Chain of Thought recorded for this decision.\"}); status = \"failed\"\n",
        "\n",
        "            elif method == 'counterfactual':\n",
        "                 if not self.fm_client: errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"FoundationModelClient proxy unavailable for counterfactuals.\"}); status = \"config_error\"\n",
        "                 elif not self.counterfactual_model_alias: errors.append({\"code\": \"CONFIG_ERROR\", \"message\": \"Counterfactual model alias not configured.\"}); status = \"config_error\"\n",
        "                 else:\n",
        "                     logger.info(f\"Generating counterfactual explanation async via FM Client API proxy ({self.counterfactual_model_alias})...\")\n",
        "                     # --- Prepare Prompt ---\n",
        "                     # Include key inputs, the decision made, and potentially the outcome if available\n",
        "                     prompt_context = f\"Decision Context:\\n{json.dumps(decision_log.get('context',{}), indent=2, default=str)}\\n\"\n",
        "                     prompt_inputs = f\"Decision Inputs:\\n{json.dumps(decision_log.get('inputs',{}), indent=2, default=str)}\\n\"\n",
        "                     prompt_decision = f\"Decision Made:\\n{json.dumps(decision_log.get('decision',{}), indent=2, default=str)}\\n\"\n",
        "                     prompt_task = \"Task: Generate a concise counterfactual explanation. What minimal changes to the inputs or context would have led to a different decision? Explain why.\"\n",
        "                     prompt = f\"{prompt_context}\\n{prompt_inputs}\\n{prompt_decision}\\n{prompt_task}\"\n",
        "                     # --- End Prepare Prompt ---\n",
        "                     try:\n",
        "                         # Call FM Client API proxy\n",
        "                         fm_request = {\n",
        "                             \"payload\": {\"prompt\": prompt, \"model_alias\": self.counterfactual_model_alias, \"max_tokens\": 300, \"temperature\": 0.4},\n",
        "                             \"trace_id\": trace_id, \"request_id\": f\"fm_cf_{request_id}\"\n",
        "                         }\n",
        "                         fm_response = await self.fm_client.generate_text(input_data=fm_request) # Call API proxy\n",
        "\n",
        "                         if fm_response.get(\"status\") == \"success\":\n",
        "                             generated_text = fm_response.get(\"payload\",{}).get(\"generated_text\")\n",
        "                             explanation = f\"Counterfactual Explanation for {decision_id}:\\n{generated_text.strip()}\"\n",
        "                             status = \"success\"\n",
        "                         else:\n",
        "                             raise RuntimeError(f\"FM Client API proxy failed: {fm_response.get('error_details')}\")\n",
        "                     except Exception as fm_e:\n",
        "                         errors.append({\"code\": \"COUNTERFACTUAL_ERROR\", \"message\": str(fm_e)}); logger.error(f\"Async counterfactual generation via FM Client API proxy failed: {fm_e}\", exc_info=True); status = \"failed\"\n",
        "\n",
        "            elif method in ['shap', 'lime']:\n",
        "                 # These typically require model access and data, hard to do post-hoc from logs alone\n",
        "                 warning_msg = \"SHAP/LIME methods require access to model state and data at decision time, often impractical post-hoc in distributed systems. Consider pre-computed feature importances stored in the log.\"; errors.append({\"code\": \"UNSUPPORTED_METHOD\", \"message\": warning_msg}); logger.warning(warning_msg); status = \"failed\"\n",
        "            else:\n",
        "                 errors.append({\"code\": \"UNSUPPORTED_METHOD\", \"message\": f\"Unsupported explanation method '{method}'.\"}); status = \"bad_request\"\n",
        "\n",
        "            response_payload = {\"explanation\": explanation} if status == \"success\" else {\"explanation\": None}\n",
        "\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"EXPLAIN_ERROR\", \"message\": str(e)}); logger.exception(f\"Error during async explanation for {decision_id}: {e}\")\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "    async def provide_role_based_explanation(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Generates tailored explanation async via FM Client API proxy. Expects/Returns MIZ OKI.\"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); decision_id = payload.get(\"decision_id\"); role = payload.get(\"role\"); method = payload.get(\"method\", 'chain_of_thought')\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        if not role: errors.append({\"code\": \"MISSING_ROLE\", \"message\": \"'payload.role' is required for tailoring.\"})\n",
        "        if errors:\n",
        "            response = self._create_miz_oki_response(input_data, \"bad_request\", errors=errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        status = \"pending\"; response_payload = None; tailored_explanation = None\n",
        "\n",
        "        # 1. Get base explanation first\n",
        "        base_explain_request = {\n",
        "            \"payload\": {\"decision_id\": decision_id, \"method\": method},\n",
        "            \"trace_id\": trace_id, \"request_id\": f\"xai_base_{request_id}\" # Link requests\n",
        "        }\n",
        "        base_response = await self.explain_decision(base_explain_request)\n",
        "        base_explanation = base_response.get(\"payload\", {}).get(\"explanation\")\n",
        "\n",
        "        if base_response.get(\"status\") != \"success\" or not base_explanation:\n",
        "            # Failed to get base explanation, return the error\n",
        "            status = base_response.get(\"status\", \"failed\")\n",
        "            errors = base_response.get(\"error_details\", [{\"code\": \"BASE_EXPLAIN_FAILED\", \"message\": \"Failed to get base explanation.\"}])\n",
        "            response = self._create_miz_oki_response(input_data, status, response_payload, errors)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        # 2. Tailor the explanation using FM Client API proxy\n",
        "        if not self.fm_client:\n",
        "            # Return base explanation with warning if FM client unavailable\n",
        "            status = \"success\"; response_payload = {\"explanation\": f\"{base_explanation}\\n\\n(Tailoring failed: FM Client unavailable)\"}\n",
        "            response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        logger.info(f\"Tailoring explanation async for role: {role} using FM Client API proxy...\")\n",
        "        # Use a model suitable for instruction following/summarization\n",
        "        tailoring_model_info = self.config.get_model_info(self.config.foundation_models.defaults.feedback_analyzer_model) # Reuse feedback model alias\n",
        "        tailoring_model_alias = tailoring_model_info.get(\"model_id\") if tailoring_model_info else self.config.foundation_models.defaults.llama4_scout\n",
        "\n",
        "        prompt = f\"\"\"Please tailor the following explanation for someone in the '{role}' role. Focus on the aspects most relevant to them and use appropriate language. Keep it concise.\n",
        "\n",
        "Original Explanation:\n",
        "---\n",
        "{base_explanation}\n",
        "---\n",
        "\n",
        "Tailored Explanation for '{role}':\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Call FM Client API proxy\n",
        "            fm_request = {\n",
        "                \"payload\": {\"prompt\": prompt, \"model_alias\": tailoring_model_alias, \"max_tokens\": 1024, \"temperature\": 0.3},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"fm_tailor_{request_id}\"\n",
        "            }\n",
        "            fm_response = await self.fm_client.generate_text(input_data=fm_request) # Call API proxy\n",
        "\n",
        "            if fm_response.get(\"status\") == \"success\":\n",
        "                generated_text = fm_response.get(\"payload\",{}).get(\"generated_text\")\n",
        "                tailored_explanation = generated_text.strip() if generated_text else f\"{base_explanation}\\n\\n(Tailoring failed: Generation returned empty result)\"\n",
        "                status = \"success\"\n",
        "            else:\n",
        "                raise RuntimeError(f\"FM Client API proxy failed for tailoring: {fm_response.get('error_details')}\")\n",
        "\n",
        "            response_payload = {\"explanation\": tailored_explanation}\n",
        "\n",
        "        except Exception as e:\n",
        "             status = \"partial_success\"; errors.append({\"code\": \"TAILORING_ERROR\", \"message\": str(e)})\n",
        "             logger.error(f\"Async tailoring via FM client API proxy failed: {e}\", exc_info=True)\n",
        "             response_payload = {\"explanation\": f\"{base_explanation}\\n\\n(Tailoring failed: {e})\"} # Return base with error\n",
        "\n",
        "        response = self._create_miz_oki_response(input_data, status, response_payload, errors if errors else None)\n",
        "        response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        return response\n",
        "\n",
        "# --- Initialization (Conceptual - Service instantiated by framework/orchestrator) ---\n",
        "# xai: Optional[ExplainableAI] = None\n",
        "# async def initialize_xai_service():\n",
        "#     global xai\n",
        "#     if not _config_obj or not _real_dependencies:\n",
        "#         logger.critical(\"Cannot initialize XAI Service: Config or dependencies missing.\")\n",
        "#         return\n",
        "#     try:\n",
        "#         xai = ExplainableAI(config=_config_obj, fm_client_proxy=_fm_client_proxy, kg_tool_proxy=_kg_tool_proxy)\n",
        "#         logger.info(\"ExplainableAI Service initialized.\")\n",
        "#     except Exception as e:\n",
        "#         logger.critical(f\"XAI Service initialization failed: {e}\", exc_info=True)\n",
        "#         xai = None\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 Explainable AI (XAI) (Reworked) ---\")\n",
        "print(\"Uses real KG Tool API proxy & FM Client API proxy.\")\n",
        "print(\"KG storage uses structured nodes/rels. Cloud Logging is preferred non-KG storage.\")\n",
        "print(\"Handles MIZ OKI payloads for API interaction.\")\n",
        "print(\"-------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "dpQgcF5P2K9_",
        "outputId": "bf09bde0-d2df-4b98-d431-79e552f2d9e3"
      },
      "id": "dpQgcF5P2K9_",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-16-3dc2ca43b83c>, line 47)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-3dc2ca43b83c>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    class MockFoundationModelClient: async def generate_text(self, input_data): await asyncio.sleep(0.05); return {\"status\": \"success\", \"payload\": {\"generated_text\": \"Simulated Counterfactual...\"}}\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: MoA Architecture - Vertex AI / ADK Implementation Architecture (Reworked)\n",
        "# Status: Finalized Descriptive Cell. Clarified component mappings further. Emphasized deployment strategy.\n",
        "# Reasoning: Provides a clear, concise architectural overview guiding the implementation across other cells,\n",
        "#            reflecting the finalized shift to managed GCP services.\n",
        "\n",
        "import logging\n",
        "\n",
        "# Ensure logger is configured (ideally inherited from Cell 1)\n",
        "logger = logging.getLogger('MIZ-OKI.VertexMoAArchitecture')\n",
        "if not logger.hasHandlers():\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Core Architectural Shift Description ---\n",
        "logger.info(\"--- MIZ OKI 3.0 ARCHITECTURE - CELL 15: Vertex AI / ADK Implementation ---\")\n",
        "logger.info(\"This architecture replaces previous custom MoA implementations based on queues/persistence managers.\")\n",
        "logger.info(\"It leverages Google Cloud's Vertex AI platform for scalable, manageable agentic workflow orchestration.\")\n",
        "logger.info(\"Core Pillars: Vertex AI Workflows (Orchestration), Cloud Run/Functions (Tool/Agent Runtime), ADK/LangGraph (Agent Logic), Deployed Tools (Capabilities).\")\n",
        "\n",
        "# --- Mapping of Deprecated Concepts to Current Architecture ---\n",
        "logger.info(\"\\nMapping Deprecated Concepts -> Current Vertex AI / GCP Concepts:\")\n",
        "logger.info(\"  - MIZ_MoA_System / Orchestrator: Replaced by Vertex AI Workflows definitions + Execution Service.\")\n",
        "logger.info(\"  - UnifiedCommunicationSystem: Replaced by native input/output passing & state management within Vertex AI Workflows. Event-driven triggers via Pub/Sub.\")\n",
        "logger.info(\"  - RobustTaskQueue / TaskPersistenceManager: Replaced by Vertex AI Workflow Execution state persistence, automatic retries, logging, and tracing.\")\n",
        "logger.info(\"  - EnhancedBaseAgent / Agent Class: Represents the *logic* implemented within ADK/LangGraph structures, deployed as callable services (Cloud Run/Functions).\")\n",
        "logger.info(\"  - AgentFactory: Replaced by deploying agent/tool services (e.g., Cloud Run, Cloud Functions) and referencing their invocation URLs within Vertex AI Workflow definitions.\")\n",
        "logger.info(\"  - REWOOSystem / BossAgent Planning: Planning logic encapsulated within a dedicated ADK 'PlannerAgent' service or integrated into the API layer (Cell 16) triggering workflows. The agent/API uses the Vertex AI API (`executions_v1.ExecutionsClient`) to start workflow executions.\")\n",
        "\n",
        "# --- Key Vertex AI / GCP Components Utilized ---\n",
        "logger.info(\"\\nLeveraged GCP / Vertex AI Components:\")\n",
        "logger.info(\"  1. Vertex AI Workflows (`workflows_v1`, `executions_v1`): Defines the DAG for complex processes (KU->DM->LI->PO, Business Apps). Handles step sequencing, parallelism, conditionals, error handling, human-in-the-loop callbacks.\")\n",
        "logger.info(\"     -> Managed execution service provides state persistence, retries, logging, tracing.\")\n",
        "logger.info(\"  2. Cloud Run / Cloud Functions: Recommended runtime for deploying individual Tools and Agents (built with ADK/LangGraph or standard Python) as scalable, stateless (or stateful with external DB) microservices.\")\n",
        "logger.info(\"     -> Workflow steps invoke these services via authenticated HTTP calls (invoked via Workflow's built-in HTTP connector or custom connectors).\")\n",
        "logger.info(\"  3. ADK (Agent Development Kit) / LangGraph: Frameworks for building the core logic of individual agents/tools (Python classes/functions). Focuses on agent reasoning, state (often externalized), and tool usage.\")\n",
        "logger.info(\"  4. Deployed Tools/Services (This Codebase): The Python classes (e.g., `KnowledgeGraphToolService`, `SemanticGraphRAGTool`, `BEABTool`) represent the logic deployed within Cloud Run/Functions services.\")\n",
        "logger.info(\"  5. Vertex AI Agent Builder (Console/APIs): Used to manage Tools (registering function specs/OpenAPI specs of deployed services), connect Data sources (Vertex AI Search for grounding), and potentially build simpler conversational agents (less relevant for this backend architecture).\")\n",
        "logger.info(\"  6. Google Cloud Pub/Sub: Used for event-driven workflow triggers (e.g., MLOps completion, external events like new data arrival, internal signals like LI actions, human approvals).\")\n",
        "logger.info(\"  7. Google Secret Manager: Securely stores API keys, database credentials, MIZ_SALT, etc., accessed by services/workflows.\")\n",
        "logger.info(\"  8. Google Cloud Logging & Tracing: Provide observability into Workflow executions and Tool/Agent service calls. MIZ OKI trace IDs facilitate end-to-end tracing.\")\n",
        "logger.info(\"  9. IAM & Service Accounts: Securely manage permissions for workflows calling tools and tools accessing GCP services.\")\n",
        "logger.info(\" 10. Vertex AI Pipelines (KFP): Used for orchestrating ML training/evaluation/deployment workflows (Cell 17), triggered via Pub/Sub.\")\n",
        "logger.info(\" 11. Other GCP Services: Cloud Storage, BigQuery, Neo4j (AuraDB/Managed), Vertex AI Vector Search as needed by specific tools.\")\n",
        "\n",
        "# --- Implementation Strategy Summary ---\n",
        "logger.info(\"\\nImplementation Approach:\")\n",
        "logger.info(\"  - Define business processes and agentic sequences as Vertex AI Workflows (YAML).\")\n",
        "logger.info(\"  - Implement reusable capabilities (KG access, FM calls, Causal AI, Business Logic, External API interactions) as async Python functions/classes within Tool/Service modules (like the reworked Cells 2-8, 10-11, 16, 18).\")\n",
        "logger.info(\"  - Deploy these Tools/Services using Cloud Run (preferred for flexibility) or Cloud Functions, ensuring they handle MIZ OKI JSON payloads via HTTP POST.\")\n",
        "logger.info(\"  - Secure deployed services using IAM invoker roles and authentication within the service code (e.g., verifying OIDC tokens from Workflow calls).\")\n",
        "logger.info(\"  - Configure Workflow steps to make authenticated HTTP calls to the deployed Tool/Agent services, passing MIZ OKI payloads as input and receiving them as output.\")\n",
        "logger.info(\"  - Use Vertex AI Workflow variables and state passing (`assign`, `result`) for context transfer between steps.\")\n",
        "logger.info(\"  - Trigger workflows via the Human Interface API (Cell 16 making calls to `executions_v1.create_execution`), Pub/Sub messages, or Cloud Scheduler.\")\n",
        "logger.info(\"  - Implement MLOps using Vertex AI Pipelines (Cell 17), triggered via Pub/Sub messages published by relevant tools (e.g., AKA/B.O.S.S., DRL Manager).\")\n",
        "logger.info(\"  - Utilize GCP's native logging, tracing, and monitoring, ensuring MIZ OKI trace IDs are propagated.\")\n",
        "\n",
        "# --- Conclusion ---\n",
        "logger.info(\"\\nConclusion: This Vertex AI-native architecture provides a robust, scalable, and maintainable foundation for MIZ OKI 3.0, focusing development effort on workflow definition and the business/AI logic within deployable Tools and Agents, leveraging managed GCP services for orchestration, execution, and observability.\")\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 MoA Architecture Definition Finalized (Cell 15 - Vertex AI Integrated) ---\")\n",
        "# No executable code needed in this cell. It serves as documentation for the architecture."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm7aVSvQ37di",
        "outputId": "e52cf22d-60cd-46fe-d746-5183972a9e31"
      },
      "id": "Xm7aVSvQ37di",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- MIZ 3.0 MoA Architecture Definition Finalized (Cell 15 - Vertex AI Integrated) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Human-Agent Collaboration Interface (API/Spec - AW Pillar) (Reworked)\n",
        "# Status: Uses REAL google-cloud-workflows client proxy (if available) for workflow interactions.\n",
        "#         Calls backend tools via injected proxies (conceptual MIZ OKI APIs).\n",
        "#         Error handling improved. Still requires deployment in API framework & real backend tools.\n",
        "\n",
        "import logging\n",
        "import datetime\n",
        "import json\n",
        "import uuid\n",
        "import asyncio\n",
        "from typing import Dict, Any, Optional, List, Union, Callable\n",
        "import time\n",
        "import random\n",
        "\n",
        "# --- GCP Client Libraries & Dependencies ---\n",
        "# Needed for interacting with Vertex AI Workflow Executions and Pub/Sub\n",
        "try:\n",
        "   from google.cloud import workflows_v1 # Not directly used here, but executions client depends on it\n",
        "   from google.cloud.workflows import executions_v1\n",
        "   from google.cloud.workflows.executions_v1.types import Execution\n",
        "   from google.protobuf import json_format\n",
        "   from google.api_core import exceptions as google_api_exceptions\n",
        "   from google.cloud import pubsub_v1\n",
        "   VERTEX_WORKFLOWS_SDK_AVAILABLE = True\n",
        "   PUBSUB_SDK_AVAILABLE = True\n",
        "   logger.debug(\"Successfully imported google-cloud-workflows/pubsub libraries for Cell 16.\")\n",
        "except ImportError:\n",
        "   logger.warning(\"google-cloud-workflows or google-cloud-pubsub library not found. API interactions will be mocked or limited.\")\n",
        "   VERTEX_WORKFLOWS_SDK_AVAILABLE = False\n",
        "   PUBSUB_SDK_AVAILABLE = False\n",
        "   # Dummy classes from Cell 16 reformation (or Cell 9)\n",
        "   class ExecutionState: ACTIVE=\"ACTIVE\"; SUCCEEDED=\"SUCCEEDED\"; FAILED=\"FAILED\"; CANCELLED=\"CANCELLED\"; SUSPENDED=\"SUSPENDED\"\n",
        "   class DummyProto: pass\n",
        "   class executions_v1: ExecutionState = ExecutionState; class ExecutionsAsyncClient: pass; class Execution(DummyProto): pass; class CreateExecutionRequest: pass; class GetExecutionRequest: pass; class CancelExecutionRequest: pass; class ListExecutionsRequest: pass; class ListExecutionsResponse: pass; ExecutionView = type('Enum', (), {'BASIC': 1, 'FULL': 2})()\n",
        "   class json_format: @staticmethod def MessageToDict(msg, **kwargs): return getattr(msg, '_fields', {})\n",
        "   class google_api_exceptions: class NotFound(Exception): pass; class InvalidArgument(Exception): pass; class PermissionDenied(Exception): pass; class FailedPrecondition(Exception):pass; class GoogleAPIError(Exception): pass\n",
        "   class pubsub_v1: class PublisherClient: pass # Dummy client\n",
        "\n",
        "# --- Assume Real Tool/Client Dependencies are Injected/Available ---\n",
        "# Proxies represent API clients for other deployed MIZ OKI services.\n",
        "# GCP clients represent actual SDK clients or mocks.\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    # Proxies for MIZ OKI Tools\n",
        "    if 'hde_tool' not in globals(): raise NameError(\"hde_tool proxy not found\") # Cell 5 Proxy\n",
        "    if 'agg_tool' not in globals(): raise NameError(\"agg_tool proxy not found\") # Cell 5 Proxy (Goal Generator)\n",
        "    if 'scf_tool' not in globals(): raise NameError(\"scf_tool proxy not found\") # Cell 5 Proxy (Feedback/Validator source)\n",
        "    if 'xai' not in globals(): raise NameError(\"xai proxy not found\") # Cell 11 Proxy\n",
        "\n",
        "    # Real/Mock GCP Clients\n",
        "    if '_workflow_executions_client' not in globals(): raise NameError(\"_workflow_executions_client not found\") # Cell 16 needs this\n",
        "    if '_pubsub_client' not in globals(): raise NameError(\"_pubsub_client not found\") # Cell 16 needs this\n",
        "\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _hde_tool_proxy = hde_tool\n",
        "    _agg_tool_proxy = agg_tool\n",
        "    _validator_tool_proxy = scf_tool # Renaming for clarity in this context\n",
        "    _xai_tool_proxy = xai\n",
        "    _exec_client_proxy = _workflow_executions_client # Use real/mock client proxy\n",
        "    _pubsub_client_proxy = _pubsub_client # Use real/mock client proxy\n",
        "    _real_backend_tools = True\n",
        "    logger.debug(\"Using real/conceptual backend tool proxies and clients in Cell 16 (Reworked).\")\n",
        "\n",
        "except NameError as e:\n",
        "   logger.warning(f\"Dependency Error ({e}). Using Placeholders for backend tools in Cell 16 (Reworked).\")\n",
        "   _real_backend_tools = False\n",
        "   # --- Mock/Placeholder Setup ---\n",
        "   class PlaceholderHDE: async def get_history(self, request): return {\"status\": \"success\", \"payload\": {\"history\": []}}; async def update_decision_log(self, request): return {\"status\": \"success\"}\n",
        "   class PlaceholderAGG: async def get_active_goals(self, request): return {\"status\": \"success\", \"payload\": {\"goals\": []}}; async def add_goal(self, request): return {\"status\": \"success\", \"payload\": {\"goal_id\": f\"goal_{uuid.uuid4().hex[:6]}\"}}\n",
        "   class PlaceholderCV: async def add_feedback(self, input_data): return {\"status\": \"success\"}\n",
        "   class PlaceholderXAI: async def explain_decision(self, input_data): return {\"status\": \"success\", \"payload\": {\"explanation\": \"Placeholder Explanation\"}}; async def provide_role_based_explanation(self, input_data): return {\"status\": \"success\", \"payload\": {\"explanation\": \"Placeholder Role Explanation\"}}\n",
        "   _hde_tool_proxy = PlaceholderHDE(); _agg_tool_proxy = PlaceholderAGG(); _validator_tool_proxy = PlaceholderCV(); _xai_tool_proxy = PlaceholderXAI()\n",
        "   # Define minimal config if needed\n",
        "   if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ:\n",
        "        from dataclasses import dataclass, field\n",
        "        @dataclass class MockGcpConfig: project_id: str = \"mock-project\"; region: str = \"mock-region\"\n",
        "        @dataclass class MockSysThresholds: human_review_confidence_threshold: float = 0.75\n",
        "        @dataclass class MockConfig: gcp: MockGcpConfig = field(default_factory=MockGcpConfig); system_thresholds: MockSysThresholds = field(default_factory=MockSysThresholds); miz_oki_schema_version: str = \"3.0\"; def get(self, key, default=None): return getattr(self, key, default)\n",
        "        _config_obj = MockConfig()\n",
        "   # Use Mock Client from Cell 9 reformation if real one not available\n",
        "   if '_workflow_executions_client' not in globals():\n",
        "        class MockVertexExecClient:\n",
        "            _executions = {}; State = ExecutionState\n",
        "            async def create_execution(self, request): exec_id_suffix = uuid.uuid4().hex[:12]; exec_name = f\"{request.parent}/executions/{exec_id_suffix}\"; self._executions[exec_name] = {\"name\": exec_name, \"state\": self.State.ACTIVE, \"start_time\": datetime.now(datetime.timezone.utc)}; if random.random() < 0.1: self._executions[exec_name][\"state\"] = self.State.SUSPENDED; return MagicMock(name=exec_name, state=self._executions[exec_name][\"state\"], start_time=self._executions[exec_name][\"start_time\"])\n",
        "            async def get_execution(self, request): exec_data = self._executions.get(request.name); if not exec_data: raise google_api_exceptions.NotFound(); if exec_data[\"state\"] == self.State.ACTIVE and random.random() < 0.1: exec_data[\"state\"] = self.State.SUCCEEDED; return MagicMock(**exec_data)\n",
        "            async def cancel_execution(self, request): exec_name = request.name; if exec_name in self._executions and self._executions[exec_name][\"state\"] in [self.State.ACTIVE, self.State.SUSPENDED]: self._executions[exec_name][\"state\"] = self.State.CANCELLED; return MagicMock(**self._executions[exec_name]); raise google_api_exceptions.FailedPrecondition()\n",
        "            async def list_executions(self, request): results = []; parent_prefix = request.parent + \"/executions/\"; # ... (mock list logic) ...; return MagicMock(executions=results)\n",
        "        _exec_client_proxy = MockVertexExecClient()\n",
        "        logger.warning(\"Using Mock Vertex Executions Client for Human API (Reworked).\")\n",
        "   if '_pubsub_client' not in globals(): class MockPubSubClient: async def publish(self, topic, data): return f\"msg_{uuid.uuid4().hex[:8]}\"\n",
        "   _pubsub_client_proxy = _pubsub_client or MockPubSubClient()\n",
        "   # --- End Mock Setup ---\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('MIZ-OKI.HumanAgentInterfaceAPI')\n",
        "\n",
        "class HumanAgentInterfaceAPI:\n",
        "   \"\"\" Backend logic for Human-Agent Collaboration API. Uses real Vertex client proxy if available. \"\"\"\n",
        "   def __init__(self, decision_engine_proxy: Any, goal_generator_proxy: Any, validator_proxy: Any, xai_proxy: Any,\n",
        "                executions_client_proxy: Any, pubsub_client_proxy: Any, config: EnhancedConfig):\n",
        "       if not all([decision_engine_proxy, goal_generator_proxy, validator_proxy, xai_proxy, executions_client_proxy, pubsub_client_proxy, config]):\n",
        "            raise InitializationError(\"HumanAgentInterfaceAPI initialized with MISSING dependencies!\")\n",
        "       self.hde_tool = decision_engine_proxy\n",
        "       self.agg_tool = goal_generator_proxy\n",
        "       self.validator_tool = validator_proxy # For submitting feedback\n",
        "       self.xai_tool = xai_proxy\n",
        "       self.exec_client = executions_client_proxy # Use the injected client proxy\n",
        "       self.pubsub_client = pubsub_client_proxy # Use the injected client proxy\n",
        "       self.config = config\n",
        "       self.project = config.gcp.project_id\n",
        "       self.location = config.gcp.region\n",
        "       self.logger = logging.getLogger('MIZ-OKI.HumanAgentInterfaceAPI')\n",
        "       self.logger.info(\"Human-Agent Interface API logic initialized (Reworked).\")\n",
        "\n",
        "   async def _check_permissions(self, user_id: str, action: str, resource_id: Optional[str] = None) -> bool:\n",
        "       \"\"\" Placeholder for permission checking logic. \"\"\"\n",
        "       # --- TODO: Implement actual permission check ---\n",
        "       # - Integrate with IAM, OAuth scopes, or an internal role-based access control (RBAC) system.\n",
        "       # - Check if user_id has permission to perform 'action' on 'resource_id'.\n",
        "       # - Example: Check if user is in 'MIZ_APPROVERS' group for 'approve_action'.\n",
        "       # --- End TODO ---\n",
        "       self.logger.debug(f\"PERMISSION CHECK (Placeholder): User '{user_id}', Action '{action}', Resource '{resource_id}'. Allowing.\")\n",
        "       await asyncio.sleep(0.001) # Simulate check latency\n",
        "       return True # Default to allow for now\n",
        "\n",
        "   async def _get_parent_path(self) -> str:\n",
        "       \"\"\"Constructs the parent path for workflow resources.\"\"\"\n",
        "       if not self.project or not self.location: raise ValueError(\"GCP Project ID/Location missing in config.\")\n",
        "       return f\"projects/{self.project}/locations/{self.location}\"\n",
        "\n",
        "   def _create_miz_oki_response(self, status: str, payload: Optional[Dict] = None, errors: Optional[List[Dict]] = None, request_data: Optional[Dict]=None) -> Dict:\n",
        "        \"\"\"Helper to construct a standard MIZ OKI response for the API layer.\"\"\"\n",
        "        # Note: This API layer itself might not receive a full MIZ OKI request if called via HTTP,\n",
        "        # but it should return one. We simulate having request context if needed.\n",
        "        req_data = request_data or {}\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": req_data.get(\"request_id\", f\"api_resp_{uuid.uuid4().hex[:8]}\"), # Generate if not passed\n",
        "            \"trace_id\": req_data.get(\"trace_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"HumanAgentInterfaceAPI\",\n",
        "            \"target_component\": req_data.get(\"source_component\"), # Echo back caller if known\n",
        "            \"status\": status,\n",
        "            \"payload\": payload,\n",
        "            \"error_details\": errors,\n",
        "            \"metadata\": {}\n",
        "        }\n",
        "\n",
        "   # --- Decision/Workflow Review & Approval ---\n",
        "   async def get_pending_reviews(self, user_id: str, limit: int = 20) -> Dict[str, Any]:\n",
        "       \"\"\"API Method: Fetches items needing human review (paused workflows, low-confidence decisions).\"\"\"\n",
        "       self.logger.info(f\"API: get_pending_reviews async for user '{user_id}' (limit: {limit})\")\n",
        "       start_time = time.monotonic()\n",
        "       if not await self._check_permissions(user_id, \"view_reviews\"):\n",
        "           return self._create_miz_oki_response(None, \"permission_denied\", errors=[{\"message\": \"Permission denied\"}])\n",
        "\n",
        "       # Fetch from different sources concurrently\n",
        "       fetch_tasks = [\n",
        "           self._fetch_hde_reviews(limit, user_id), # Pass user_id if HDE needs it for filtering\n",
        "           self._fetch_paused_vertex_workflows(limit, user_id) # Pass user_id if filtering needed\n",
        "       ]\n",
        "       results_or_exceptions = await asyncio.gather(*fetch_tasks, return_exceptions=True)\n",
        "\n",
        "       combined_pending = []\n",
        "       errors = []\n",
        "       for i, result in enumerate(results_or_exceptions):\n",
        "           source = \"HDE\" if i == 0 else \"Workflows\"\n",
        "           if isinstance(result, Exception):\n",
        "               error_msg = f\"Error fetching reviews from {source}: {result}\"\n",
        "               self.logger.error(error_msg, exc_info=True)\n",
        "               errors.append({\"code\": f\"FETCH_{source}_ERROR\", \"message\": error_msg})\n",
        "           elif isinstance(result, list):\n",
        "               combined_pending.extend(result)\n",
        "\n",
        "       # Sort by timestamp (descending) and limit\n",
        "       try:\n",
        "           # Ensure timestamp is present and valid for sorting\n",
        "           def get_sort_key(item):\n",
        "               ts_str = item.get('timestamp')\n",
        "               if ts_str:\n",
        "                   try: return datetime.fromisoformat(ts_str.replace('Z', '+00:00'))\n",
        "                   except ValueError: return datetime.min.replace(tzinfo=datetime.timezone.utc) # Put items with bad timestamps first/last\n",
        "               return datetime.min.replace(tzinfo=datetime.timezone.utc) # Items without timestamp first/last\n",
        "\n",
        "           combined_pending.sort(key=get_sort_key, reverse=True)\n",
        "       except Exception as sort_e:\n",
        "           self.logger.warning(f\"Could not sort pending reviews: {sort_e}\")\n",
        "           errors.append({\"code\": \"SORT_ERROR\", \"message\": f\"Sorting failed: {sort_e}\"})\n",
        "\n",
        "       final_list = combined_pending[:limit]\n",
        "       status = \"success\" if not errors else \"partial_success\"\n",
        "       response = self._create_miz_oki_response(None, status, {\"pending_reviews\": final_list}, errors if errors else None)\n",
        "       response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "       return response\n",
        "\n",
        "   async def _fetch_hde_reviews(self, limit: int, user_id: str) -> List[Dict]:\n",
        "       \"\"\"Fetches low-confidence decisions from HDE Tool API proxy.\"\"\"\n",
        "       pending = []; count = 0\n",
        "       if not self.hde_tool or not hasattr(self.hde_tool, 'get_history'):\n",
        "           self.logger.warning(\"HDE Tool proxy unavailable or get_history method missing.\")\n",
        "           return pending\n",
        "       try:\n",
        "           # Call HDE Tool API proxy - request more than limit to allow filtering\n",
        "           hde_request = {\"payload\": {\"limit\": limit * 2, \"status_filter\": \"pending_review\"}} # Example filter\n",
        "           hde_response = await self.hde_tool.get_history(request=hde_request) # Call API proxy\n",
        "\n",
        "           if hde_response.get(\"status\") == \"success\":\n",
        "               history = hde_response.get(\"payload\", {}).get(\"history\", [])\n",
        "               threshold = self.config.system_thresholds.human_review_confidence_threshold\n",
        "               for decision in history:\n",
        "                    if count >= limit: break\n",
        "                    if isinstance(decision, dict) and not decision.get('human_review_status'): # Check if not already reviewed\n",
        "                         confidence = decision.get('final_confidence', 1.0)\n",
        "                         ethics_flagged = isinstance(decision.get('ethics_flag'), dict) # Check if ethics flag exists\n",
        "                         reason = \"\"\n",
        "                         if confidence < threshold: reason += f\"Confidence ({confidence:.2f} < {threshold:.2f})\"\n",
        "                         if ethics_flagged: reason += (\" and \" if reason else \"\") + \"Ethics Flagged\"\n",
        "\n",
        "                         if reason: # Only add if review is needed\n",
        "                             pending.append({\n",
        "                                 \"review_id\": decision.get(\"decision_id\"),\n",
        "                                 \"type\": \"decision\",\n",
        "                                 \"summary\": f\"Review decision '{decision.get('decision_type', 'N/A')}' ({reason})\",\n",
        "                                 \"timestamp\": decision.get('timestamp_start'),\n",
        "                                 \"details_link\": f\"/api/decisions/{decision.get('decision_id')}\" # Conceptual link\n",
        "                             })\n",
        "                             count += 1\n",
        "           else:\n",
        "               logger.error(f\"HDE Tool API get_history failed: {hde_response.get('error_details')}\")\n",
        "       except Exception as e:\n",
        "           self.logger.error(f\"Error fetching HDE reviews via API proxy: {e}\", exc_info=True)\n",
        "       return pending\n",
        "\n",
        "   async def _fetch_paused_vertex_workflows(self, limit: int, user_id: str) -> List[Dict]:\n",
        "       \"\"\" Fetches paused workflows using the REAL Vertex AI Executions Client proxy. \"\"\"\n",
        "       paused_workflows = []\n",
        "       if not self.exec_client or not VERTEX_WORKFLOWS_SDK_AVAILABLE:\n",
        "           self.logger.warning(\"Vertex Executions client proxy/SDK unavailable. Cannot fetch paused workflows.\")\n",
        "           return paused_workflows\n",
        "       try:\n",
        "           parent = await self._get_parent_path()\n",
        "           # Filter for SUSPENDED state\n",
        "           filter_string = f'state = \"{ExecutionState.SUSPENDED.name}\"'\n",
        "           # --- TODO: Add filtering based on user_id if workflows have labels/metadata indicating assignee ---\n",
        "           # Example: filter_string += f' AND labels.assigned_user = \"{user_id}\"'\n",
        "           # --- End TODO ---\n",
        "           request = executions_v1.ListExecutionsRequest(\n",
        "               parent=parent,\n",
        "               view=executions_v1.ExecutionView.BASIC, # BASIC view is usually enough\n",
        "               filter=filter_string,\n",
        "               page_size=limit # Limit results from the API\n",
        "           )\n",
        "           count = 0\n",
        "           # Use the async iterator provided by the client proxy\n",
        "           async for execution in await self.exec_client.list_executions(request=request):\n",
        "                if count >= limit: break\n",
        "                # --- TODO: Add check for specific callback/human step label if possible ---\n",
        "                # This might involve getting FULL view or checking execution args/state\n",
        "                # --- End TODO ---\n",
        "                exec_name_full = execution.name\n",
        "                # Extract short workflow ID and execution ID\n",
        "                parts = exec_name_full.split('/')\n",
        "                wf_id_short = parts[5] if len(parts) > 5 else \"unknown_wf\"\n",
        "                exec_id_short = parts[-1]\n",
        "\n",
        "                paused_workflows.append({\n",
        "                    \"review_id\": exec_name_full, # Use full name as the unique ID\n",
        "                    \"type\": \"workflow\",\n",
        "                    \"summary\": f\"Approve workflow '{wf_id_short}' (Execution: ...{exec_id_short[-6:]})\",\n",
        "                    \"timestamp\": execution.start_time.isoformat() if execution.start_time else None,\n",
        "                    \"details_link\": f\"/api/workflows/executions/{exec_id_short}\" # Conceptual link\n",
        "                })\n",
        "                count += 1\n",
        "           self.logger.info(f\"Fetched {len(paused_workflows)} SUSPENDED Vertex AI workflows via client proxy.\")\n",
        "       except google_api_exceptions.GoogleAPIError as api_e:\n",
        "           self.logger.error(f\"API Error fetching paused Vertex workflows via client proxy: {api_e}\")\n",
        "       except Exception as e:\n",
        "           self.logger.error(f\"Unexpected Error fetching paused Vertex workflows via client proxy: {e}\", exc_info=True)\n",
        "       return paused_workflows\n",
        "\n",
        "   async def approve_action(self, user_id: str, review_id: str, approval_data: Optional[Dict] = None, comments: Optional[str] = None) -> Dict[str, Any]:\n",
        "       \"\"\" API Method: Approves a pending workflow (via PubSub signal) or decision (via HDE API proxy). \"\"\"\n",
        "       self.logger.info(f\"API: approve_action async for '{review_id}' by '{user_id}'\")\n",
        "       start_time = time.monotonic()\n",
        "       if not await self._check_permissions(user_id, \"approve_action\", review_id):\n",
        "           return self._create_miz_oki_response(None, \"permission_denied\", errors=[{\"message\": \"Permission denied\"}])\n",
        "\n",
        "       is_workflow_exec_id = review_id.startswith(\"projects/\") and \"/workflows/\" in review_id and \"/executions/\" in review_id\n",
        "       status = \"error\"; message = \"Approval failed: Item not found or mechanism error.\"; errors = []\n",
        "\n",
        "       # --- Try Workflow Approval via Pub/Sub Signal ---\n",
        "       if is_workflow_exec_id:\n",
        "            if self.pubsub_client and PUBSUB_SDK_AVAILABLE:\n",
        "                try:\n",
        "                    # --- TODO: Get approval topic name from config ---\n",
        "                    approval_signal_topic_name = \"workflow-approvals\" # Example\n",
        "                    approval_signal_topic_path = f\"projects/{self.project}/topics/{approval_signal_topic_name}\"\n",
        "                    # --- End TODO ---\n",
        "                    signal_data = {\n",
        "                        \"execution_id\": review_id,\n",
        "                        \"approved\": True,\n",
        "                        \"approval_data\": approval_data or {}, # Data needed by the workflow to continue\n",
        "                        \"comments\": comments,\n",
        "                        \"approver\": user_id,\n",
        "                        \"timestamp\": datetime.now(datetime.timezone.utc).isoformat()\n",
        "                    }\n",
        "                    # Call REAL Pub/Sub client proxy\n",
        "                    message_id = await self.pubsub_client.publish(approval_signal_topic_path, json.dumps(signal_data).encode('utf-8'))\n",
        "                    self.logger.info(f\"Published APPROVAL signal for execution {review_id} via Pub/Sub proxy. Message ID: {message_id}\")\n",
        "                    status = \"approved\"; message = \"Workflow approval signal sent.\"\n",
        "                    response = self._create_miz_oki_response(None, status, {\"review_id\": review_id, \"message\": message})\n",
        "                    response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "                    return response # Return early if workflow signal sent\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Error sending approval signal for workflow {review_id} via Pub/Sub proxy: {e}\"\n",
        "                    self.logger.error(error_msg, exc_info=True)\n",
        "                    errors.append({\"code\": \"PUBSUB_SIGNAL_ERROR\", \"message\": error_msg})\n",
        "                    # Continue to try HDE update as fallback if applicable\n",
        "            else:\n",
        "                 errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"PubSub client proxy/SDK unavailable for workflow approval.\"})\n",
        "\n",
        "       # --- Fallback/Alternative: Update HDE Decision Log via API proxy ---\n",
        "       if self.hde_tool and hasattr(self.hde_tool, 'update_decision_log'):\n",
        "           try:\n",
        "               update_payload = {\n",
        "                   'human_review_status': 'approved',\n",
        "                   'human_reviewer': user_id,\n",
        "                   'human_review_comments': comments,\n",
        "                   'human_review_timestamp': datetime.now(datetime.timezone.utc).isoformat(),\n",
        "                   'human_approval_data': approval_data # Store any data passed with approval\n",
        "               }\n",
        "               hde_request = {\"payload\": {\"decision_id\": review_id, \"update_data\": update_payload}, \"request_id\": f\"hde_approve_{review_id}\"}\n",
        "               hde_response = await self.hde_tool.update_decision_log(request=hde_request) # Call API proxy\n",
        "\n",
        "               if hde_response.get(\"status\") == \"success\":\n",
        "                   status = \"approved\"; message = \"Decision approved in HDE log.\"\n",
        "                   # If workflow approval failed earlier, status remains 'error' overall\n",
        "                   if errors: status = \"partial_failure\"; message += \" (Workflow signal failed)\"\n",
        "               else:\n",
        "                   error_msg = f\"HDE Tool API update failed for approval: {hde_response.get('error_details')}\"\n",
        "                   logger.error(error_msg)\n",
        "                   errors.append({\"code\": \"HDE_UPDATE_ERROR\", \"message\": error_msg})\n",
        "                   # If workflow also failed, status remains 'error'\n",
        "                   if not is_workflow_exec_id: status = \"failed\" # If only HDE update was attempted and failed\n",
        "\n",
        "           except Exception as e:\n",
        "               error_msg = f\"Error calling HDE Tool API proxy for approval {review_id}: {e}\"\n",
        "               self.logger.error(error_msg, exc_info=True)\n",
        "               errors.append({\"code\": \"HDE_PROXY_ERROR\", \"message\": error_msg})\n",
        "               if not is_workflow_exec_id: status = \"failed\" # If only HDE update was attempted and failed\n",
        "       elif not is_workflow_exec_id: # If it wasn't a workflow ID and HDE tool is missing\n",
        "            errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"HDE Tool proxy unavailable for decision approval.\"})\n",
        "\n",
        "       # Determine final status based on errors\n",
        "       if not errors and status == \"approved\": pass # All good\n",
        "       elif errors and status == \"approved\": status = \"partial_failure\" # Workflow failed, HDE succeeded\n",
        "       else: status = \"failed\" # Either both failed, or only HDE was tried and failed\n",
        "\n",
        "       response = self._create_miz_oki_response(None, status, {\"review_id\": review_id, \"message\": message}, errors if errors else None)\n",
        "       response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "       return response\n",
        "\n",
        "   async def reject_action(self, user_id: str, review_id: str, reason: str, feedback_data: Optional[Dict] = None) -> Dict[str, Any]:\n",
        "       \"\"\" API Method: Rejects a workflow (CANCEL via Vertex API proxy) or decision (via HDE API proxy) and submits feedback (via Validator API proxy). \"\"\"\n",
        "       self.logger.info(f\"API: reject_action async for '{review_id}' by '{user_id}'. Reason: {reason}\")\n",
        "       start_time = time.monotonic()\n",
        "       if not await self._check_permissions(user_id, \"reject_action\", review_id):\n",
        "           return self._create_miz_oki_response(None, \"permission_denied\", errors=[{\"message\": \"Permission denied\"}])\n",
        "       if not reason or not reason.strip():\n",
        "           return self._create_miz_oki_response(None, \"bad_request\", errors=[{\"message\": \"Rejection reason required.\"}])\n",
        "\n",
        "       rejected = False; item_type = \"unknown\"; errors = []\n",
        "\n",
        "       # --- Try Cancelling Vertex AI Workflow via REAL Client Proxy ---\n",
        "       is_workflow_exec_id = review_id.startswith(\"projects/\") and \"/workflows/\" in review_id and \"/executions/\" in review_id\n",
        "       if is_workflow_exec_id:\n",
        "            item_type = \"workflow\"\n",
        "            if self.exec_client and VERTEX_WORKFLOWS_SDK_AVAILABLE:\n",
        "                try:\n",
        "                    request = CancelExecutionRequest(name=review_id) # Use real request type\n",
        "                    await self.exec_client.cancel_execution(request=request) # Call proxy method\n",
        "                    self.logger.info(f\"Workflow execution {review_id} cancellation request sent by {user_id} via client proxy.\")\n",
        "                    rejected = True\n",
        "                except google_api_exceptions.FailedPrecondition as fp_e:\n",
        "                    # This likely means the workflow already finished (SUCCEEDED or FAILED)\n",
        "                    logger.warning(f\"Cannot cancel workflow {review_id} (likely already finished): {fp_e}.\")\n",
        "                    errors.append({\"code\": \"CANCEL_PRECONDITION_FAILED\", \"message\": \"Workflow likely already finished.\"})\n",
        "                except google_api_exceptions.NotFound:\n",
        "                    logger.warning(f\"Workflow execution {review_id} not found for cancellation.\")\n",
        "                    errors.append({\"code\": \"WORKFLOW_NOT_FOUND\", \"message\": \"Workflow execution not found.\"})\n",
        "                except google_api_exceptions.GoogleAPIError as api_e:\n",
        "                    error_msg = f\"API Error cancelling workflow {review_id} via client proxy: {api_e}\"\n",
        "                    self.logger.error(error_msg)\n",
        "                    errors.append({\"code\": \"VERTEX_API_ERROR\", \"message\": error_msg})\n",
        "                except Exception as e_cancel:\n",
        "                    error_msg = f\"Error sending cancel request for workflow {review_id} via client proxy: {e_cancel}\"\n",
        "                    self.logger.error(error_msg, exc_info=True)\n",
        "                    errors.append({\"code\": \"CANCEL_PROXY_ERROR\", \"message\": error_msg})\n",
        "            else:\n",
        "                 errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"Vertex Executions client proxy/SDK unavailable for workflow cancellation.\"})\n",
        "\n",
        "       # --- Fallback/Alternative: Update HDE Decision Log via API proxy ---\n",
        "       if not rejected: # Try HDE update only if workflow cancellation wasn't attempted or failed\n",
        "           item_type = \"decision\"\n",
        "           if self.hde_tool and hasattr(self.hde_tool, 'update_decision_log'):\n",
        "               try:\n",
        "                   update_payload = {\n",
        "                       'human_review_status': 'rejected',\n",
        "                       'human_reviewer': user_id,\n",
        "                       'human_rejection_reason': reason,\n",
        "                       'human_review_timestamp': datetime.now(datetime.timezone.utc).isoformat()\n",
        "                   }\n",
        "                   hde_request = {\"payload\": {\"decision_id\": review_id, \"update_data\": update_payload}, \"request_id\": f\"hde_reject_{review_id}\"}\n",
        "                   hde_response = await self.hde_tool.update_decision_log(request=hde_request) # Call API proxy\n",
        "\n",
        "                   if hde_response.get(\"status\") == \"success\":\n",
        "                       rejected = True\n",
        "                       logger.info(f\"Decision {review_id} marked as rejected in HDE log.\")\n",
        "                   else:\n",
        "                       error_msg = f\"HDE Tool API update failed for rejection: {hde_response.get('error_details')}\"\n",
        "                       logger.error(error_msg)\n",
        "                       errors.append({\"code\": \"HDE_UPDATE_ERROR\", \"message\": error_msg})\n",
        "               except Exception as e:\n",
        "                   error_msg = f\"Error calling HDE Tool API proxy for rejection {review_id}: {e}\"\n",
        "                   self.logger.error(error_msg, exc_info=True)\n",
        "                   errors.append({\"code\": \"HDE_PROXY_ERROR\", \"message\": error_msg})\n",
        "           elif not is_workflow_exec_id: # Only error if HDE was the only option\n",
        "                errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"HDE Tool proxy unavailable for decision rejection.\"})\n",
        "\n",
        "       # --- Submit Feedback Async via Validator/SCF Tool API Proxy ---\n",
        "       if rejected and feedback_data is not None:\n",
        "           if self.validator_tool and hasattr(self.validator_tool, 'add_feedback'):\n",
        "               feedback_entry = {\n",
        "                   \"feedback_id\": f\"fb_rej_{uuid.uuid4().hex[:8]}\",\n",
        "                   \"user_id\": user_id,\n",
        "                   \"item_type\": item_type,\n",
        "                   \"component_id\": review_id, # ID of the rejected item\n",
        "                   \"feedback\": {\"rejection_reason\": reason, **(feedback_data or {})},\n",
        "                   \"type\": \"rejection_feedback\", # Specific feedback type\n",
        "                   \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "                   \"source\": f\"human_rejection:{user_id}\"\n",
        "               }\n",
        "               try:\n",
        "                   # Prepare MIZ OKI request for Validator Tool\n",
        "                   val_request = {\"payload\": feedback_entry, \"request_id\": f\"val_feedback_{review_id}\"}\n",
        "                   # Fire-and-forget the feedback submission\n",
        "                   asyncio.create_task(self.validator_tool.add_feedback(input_data=val_request)) # Call API proxy\n",
        "                   self.logger.info(f\"Async feedback submission task created for rejected {review_id}.\")\n",
        "               except Exception as e:\n",
        "                   error_msg = f\"Error submitting feedback async via Validator API proxy for {review_id}: {e}\"\n",
        "                   self.logger.error(error_msg, exc_info=True)\n",
        "                   errors.append({\"code\": \"FEEDBACK_SUBMIT_ERROR\", \"message\": error_msg})\n",
        "           else:\n",
        "                errors.append({\"code\": \"MISSING_DEPENDENCY\", \"message\": \"Validator Tool proxy unavailable for feedback submission.\"})\n",
        "\n",
        "       # Determine final status\n",
        "       status = \"rejected\" if rejected else \"failed\"\n",
        "       message = \"Action rejected/cancelled.\" if rejected else \"Rejection failed.\"\n",
        "       response = self._create_miz_oki_response(None, status, {\"review_id\": review_id, \"item_type\": item_type, \"message\": message}, errors if errors else None)\n",
        "       response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "       return response\n",
        "\n",
        "   # --- Feedback Submission (Uses Real Validator/SCF Tool Proxy) ---\n",
        "   async def submit_general_feedback(self, user_id: str, component_id: str, feedback_data: Dict) -> Dict[str, Any]:\n",
        "       \"\"\" API Method: Submits general feedback via Validator/SCF Tool API proxy. \"\"\"\n",
        "       self.logger.info(f\"API: submit_general_feedback async by '{user_id}' for component '{component_id}'\")\n",
        "       start_time = time.monotonic()\n",
        "       if not await self._check_permissions(user_id, \"submit_feedback\"):\n",
        "           return self._create_miz_oki_response(None, \"permission_denied\", errors=[{\"message\": \"Permission denied\"}])\n",
        "       if not self.validator_tool or not hasattr(self.validator_tool, 'add_feedback'):\n",
        "           return self._create_miz_oki_response(None, \"service_unavailable\", errors=[{\"message\": \"Feedback system (Validator Tool proxy) unavailable.\"}])\n",
        "       if not isinstance(feedback_data, dict) or not feedback_data:\n",
        "            return self._create_miz_oki_response(None, \"bad_request\", errors=[{\"message\": \"feedback_data (dict) is required.\"}])\n",
        "\n",
        "       status = \"pending\"; response_payload = None; errors = []\n",
        "       try:\n",
        "            feedback_entry = {\n",
        "                \"feedback_id\": f\"fb_gen_{uuid.uuid4().hex[:8]}\",\n",
        "                \"user_id\": user_id,\n",
        "                \"component_id\": component_id,\n",
        "                \"feedback\": feedback_data,\n",
        "                \"type\": feedback_data.get(\"type\", \"general\"), # Allow type override in data\n",
        "                \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "                \"source\": f\"human_general:{user_id}\"\n",
        "            }\n",
        "            # Prepare MIZ OKI request for Validator Tool\n",
        "            val_request = {\"payload\": feedback_entry, \"request_id\": f\"val_gen_fb_{component_id}\"}\n",
        "            val_response = await self.validator_tool.add_feedback(input_data=val_request) # Call API proxy\n",
        "\n",
        "            if val_response.get(\"status\") == \"success\":\n",
        "                status = \"submitted\"; response_payload = {\"feedback_id\": feedback_entry['feedback_id']}\n",
        "            else:\n",
        "                status = \"failed\"; errors.append({\"code\": \"FEEDBACK_SUBMIT_FAILED\", \"message\": f\"Feedback submission failed in Validator Tool: {val_response.get('error_details')}\"})\n",
        "       except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": f\"Internal error submitting feedback: {e}\"})\n",
        "            self.logger.error(f\"Error submitting general feedback via Validator API proxy: {e}\", exc_info=True)\n",
        "\n",
        "       response = self._create_miz_oki_response(None, status, response_payload, errors if errors else None)\n",
        "       response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "       return response\n",
        "\n",
        "   # --- Goal Management (Uses Real AGG Tool Proxy) ---\n",
        "   async def get_active_goals(self, user_id: str, domain: Optional[str] = None) -> Dict[str, Any]:\n",
        "       \"\"\" API Method: Fetches active goals via AGG Tool API proxy. \"\"\"\n",
        "       self.logger.info(f\"API: get_active_goals async for user '{user_id}', domain '{domain}'\")\n",
        "       start_time = time.monotonic()\n",
        "       if not await self._check_permissions(user_id, \"view_goals\"):\n",
        "           return self._create_miz_oki_response(None, \"permission_denied\", errors=[{\"message\": \"Permission denied\"}])\n",
        "       if not self.agg_tool or not hasattr(self.agg_tool, 'get_active_goals'):\n",
        "           return self._create_miz_oki_response(None, \"service_unavailable\", errors=[{\"message\": \"Goal generator (AGG Tool proxy) unavailable.\"}])\n",
        "\n",
        "       status = \"pending\"; response_payload = None; errors = []\n",
        "       try:\n",
        "            # Prepare MIZ OKI request for AGG Tool\n",
        "            agg_request = {\"payload\": {\"domain_filter\": domain}, \"request_id\": f\"agg_get_goals_{user_id}\"}\n",
        "            agg_response = await self.agg_tool.get_active_goals(request=agg_request) # Call API proxy\n",
        "\n",
        "            if agg_response.get(\"status\") == \"success\":\n",
        "                status = \"success\"; response_payload = {\"active_goals\": agg_response.get(\"payload\", {}).get(\"goals\", [])}\n",
        "            else:\n",
        "                status = \"failed\"; errors.append({\"code\": \"AGG_FETCH_ERROR\", \"message\": f\"Error fetching goals from AGG Tool API: {agg_response.get('error_details')}\"})\n",
        "       except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": f\"Error fetching goals: {e}\"})\n",
        "            self.logger.error(f\"Error calling AGG Tool API proxy get_active_goals: {e}\", exc_info=True)\n",
        "\n",
        "       response = self._create_miz_oki_response(None, status, response_payload, errors if errors else None)\n",
        "       response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "       return response\n",
        "\n",
        "   async def add_manual_goal(self, user_id: str, description: str, kpis: List[str], owner_agent: str=\"human\", priority: float = 0.5, target_values: Optional[Dict] = None) -> Dict[str, Any]:\n",
        "       \"\"\" API Method: Adds a manual goal via AGG Tool API proxy (which triggers planning). \"\"\"\n",
        "       self.logger.info(f\"API: add_manual_goal async by '{user_id}': {description}\")\n",
        "       start_time = time.monotonic()\n",
        "       if not await self._check_permissions(user_id, \"add_goal\"):\n",
        "           return self._create_miz_oki_response(None, \"permission_denied\", errors=[{\"message\": \"Permission denied\"}])\n",
        "       if not self.agg_tool or not hasattr(self.agg_tool, 'add_goal'):\n",
        "           return self._create_miz_oki_response(None, \"service_unavailable\", errors=[{\"message\": \"Goal generator (AGG Tool proxy) unavailable.\"}])\n",
        "       if not description or not kpis:\n",
        "            return self._create_miz_oki_response(None, \"bad_request\", errors=[{\"message\": \"Description and KPIs are required.\"}])\n",
        "\n",
        "       status = \"pending\"; response_payload = None; errors = []\n",
        "       try:\n",
        "            # Prepare MIZ OKI request for AGG Tool API proxy\n",
        "            agg_request = {\n",
        "                \"payload\": {\n",
        "                    \"description\": description, \"kpis\": kpis, \"owner_agent\": owner_agent,\n",
        "                    \"priority\": priority, \"target_values\": target_values,\n",
        "                    \"source\": f\"human:{user_id}\"\n",
        "                },\n",
        "                \"request_id\": f\"agg_add_goal_{user_id}\"\n",
        "            }\n",
        "            agg_response = await self.agg_tool.add_goal(request=agg_request) # Call API proxy method\n",
        "\n",
        "            if agg_response.get(\"status\") == \"success\":\n",
        "                status = \"created\"; response_payload = {\"goal_id\": agg_response.get(\"payload\", {}).get(\"goal_id\")}\n",
        "            else:\n",
        "                status = \"failed\"; errors.append({\"code\": \"AGG_ADD_ERROR\", \"message\": f\"Failed to add goal via AGG Tool API: {agg_response.get('error_details')}\"})\n",
        "       except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": f\"Internal error adding goal: {e}\"})\n",
        "            self.logger.error(f\"Error calling AGG Tool API proxy add_goal: {e}\", exc_info=True)\n",
        "\n",
        "       response = self._create_miz_oki_response(None, status, response_payload, errors if errors else None)\n",
        "       response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "       return response\n",
        "\n",
        "   # --- Explainability Access (Uses Real XAI Tool Proxy) ---\n",
        "   async def get_decision_explanation(self, user_id: str, decision_or_execution_id: str, method: str = \"chain_of_thought\", role: Optional[str] = None) -> Dict[str, Any]:\n",
        "       \"\"\" API Method: Gets explanation via XAI Tool API proxy. \"\"\"\n",
        "       self.logger.info(f\"API: get_decision_explanation async for '{decision_or_execution_id}' by '{user_id}', method '{method}', role '{role}'\")\n",
        "       start_time = time.monotonic()\n",
        "       # --- TODO: Resolve execution_id to decision_id if needed ---\n",
        "       # This might involve querying the KG Tool API or checking workflow output mapping.\n",
        "       decision_id = decision_or_execution_id\n",
        "       # --- End TODO ---\n",
        "       if not await self._check_permissions(user_id, \"view_explanation\", decision_id):\n",
        "           return self._create_miz_oki_response(None, \"permission_denied\", errors=[{\"message\": \"Permission denied\"}])\n",
        "       if not self.xai_tool:\n",
        "           return self._create_miz_oki_response(None, \"service_unavailable\", errors=[{\"message\": \"Explainability system (XAI Tool proxy) unavailable.\"}])\n",
        "\n",
        "       status = \"pending\"; response_payload = None; errors = []\n",
        "       try:\n",
        "           # Prepare MIZ OKI request for XAI Tool API proxy\n",
        "           xai_request = {\n",
        "               \"payload\": {\"decision_id\": decision_id, \"method\": method, \"role\": role},\n",
        "               \"request_id\": f\"xai_explain_{decision_id}\"\n",
        "           }\n",
        "           xai_response = None\n",
        "           # Call appropriate XAI Tool API proxy method\n",
        "           if role and hasattr(self.xai_tool, 'provide_role_based_explanation'):\n",
        "               xai_response = await self.xai_tool.provide_role_based_explanation(input_data=xai_request)\n",
        "           elif hasattr(self.xai_tool, 'explain_decision'):\n",
        "               xai_response = await self.xai_tool.explain_decision(input_data=xai_request)\n",
        "           else:\n",
        "               errors.append({\"code\": \"XAI_METHOD_UNAVAILABLE\", \"message\": \"Required XAI explanation method unavailable via proxy.\"})\n",
        "               status = \"config_error\"\n",
        "\n",
        "           if xai_response: # If a method was called\n",
        "               if xai_response.get(\"status\") == \"success\":\n",
        "                   status = \"success\"; response_payload = {\"explanation\": xai_response.get(\"payload\", {}).get(\"explanation\")}\n",
        "               else:\n",
        "                   status = xai_response.get(\"status\", \"failed\")\n",
        "                   errors = xai_response.get(\"error_details\", [{\"code\": \"XAI_ERROR\", \"message\": \"Explanation failed/not found.\"}])\n",
        "       except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": f\"Internal error getting explanation: {e}\"})\n",
        "            self.logger.error(f\"Error calling XAI Tool API proxy: {e}\", exc_info=True)\n",
        "\n",
        "       response = self._create_miz_oki_response(None, status, response_payload, errors if errors else None)\n",
        "       response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "       return response\n",
        "\n",
        "   # --- Task Assignment (Uses REAL Vertex Workflow Client Proxy) ---\n",
        "   async def assign_task_to_workflow(self, user_id: str, workflow_id: str, task_data: Dict, initial_context: Optional[Dict] = None) -> Dict[str, Any]:\n",
        "       \"\"\" API Method: Triggers a specific Vertex AI Workflow asynchronously using the REAL client proxy. \"\"\"\n",
        "       self.logger.info(f\"API: assign_task_to_workflow async '{workflow_id}' by '{user_id}'\")\n",
        "       start_time = time.monotonic()\n",
        "       if not await self._check_permissions(user_id, \"trigger_workflow\", workflow_id):\n",
        "           return self._create_miz_oki_response(None, \"permission_denied\", errors=[{\"message\": \"Permission denied\"}])\n",
        "       if not self.exec_client or not VERTEX_WORKFLOWS_SDK_AVAILABLE:\n",
        "           return self._create_miz_oki_response(None, \"service_unavailable\", errors=[{\"message\": \"Workflow execution system (Vertex client proxy/SDK) unavailable.\"}])\n",
        "       if not isinstance(task_data, dict):\n",
        "            return self._create_miz_oki_response(None, \"bad_request\", errors=[{\"message\": \"task_data must be a dictionary.\"}])\n",
        "\n",
        "       status = \"pending\"; response_payload = None; errors = []\n",
        "       try:\n",
        "           parent = await self._get_parent_path()\n",
        "           workflow_name = f\"{parent}/workflows/{workflow_id}\"\n",
        "\n",
        "           # Prepare MIZ OKI payload for the workflow argument\n",
        "           workflow_input_payload = {**(initial_context or {}), **task_data}\n",
        "           miz_oki_input = {\n",
        "               \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "               \"request_id\": f\"req_wf_{uuid.uuid4().hex[:8]}\",\n",
        "               \"trace_id\": f\"trace_api_{uuid.uuid4().hex[:8]}\", # Generate trace ID if not provided\n",
        "               \"source_component\": f\"HumanAPI:{user_id}\",\n",
        "               \"target_component\": workflow_id,\n",
        "               \"payload\": workflow_input_payload\n",
        "           }\n",
        "           execution_args = json.dumps(miz_oki_input)\n",
        "           execution_proto = Execution(argument=execution_args) # Use real proto type\n",
        "           request = CreateExecutionRequest(parent=workflow_name, execution=execution_proto) # Use real request type\n",
        "\n",
        "           # Call the REAL client proxy method\n",
        "           exec_response = await self.exec_client.create_execution(request=request)\n",
        "           execution_name = exec_response.name # Full execution name\n",
        "\n",
        "           status = \"submitted\"; response_payload = {\"execution_name\": execution_name}\n",
        "           self.logger.info(f\"Workflow '{workflow_id}' execution '{execution_name}' created by {user_id} via client proxy.\")\n",
        "\n",
        "       except google_api_exceptions.NotFound as nf_e:\n",
        "            status = \"not_found\"; errors.append({\"code\": \"WORKFLOW_NOT_FOUND\", \"message\": f\"Workflow '{workflow_id}' not found.\"}); logger.error(f\"Workflow '{workflow_id}' not found: {nf_e}\")\n",
        "       except google_api_exceptions.GoogleAPIError as api_e:\n",
        "            status = \"api_error\"; errors.append({\"code\": \"VERTEX_API_ERROR\", \"message\": str(api_e)}); logger.error(f\"API Error triggering workflow '{workflow_id}' via client proxy: {api_e}\")\n",
        "       except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": f\"Internal error: {e}\"}); logger.error(f\"Error triggering workflow async '{workflow_id}' via client proxy: {e}\", exc_info=True)\n",
        "\n",
        "       response = self._create_miz_oki_response(None, status, response_payload, errors if errors else None)\n",
        "       response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "       return response\n",
        "\n",
        "   async def get_workflow_execution_details(self, user_id: str, execution_name: str) -> Dict[str, Any]:\n",
        "       \"\"\" API Method: Retrieves workflow execution details async using the REAL client proxy. \"\"\"\n",
        "       self.logger.info(f\"API: get_workflow_execution_details async for '{execution_name}' by '{user_id}'\")\n",
        "       start_time = time.monotonic()\n",
        "       # Basic validation of execution_name format\n",
        "       if not execution_name or not execution_name.startswith(\"projects/\"):\n",
        "            return self._create_miz_oki_response(None, \"bad_request\", errors=[{\"message\": \"Invalid execution_name format.\"}])\n",
        "       if not await self._check_permissions(user_id, \"view_task\", execution_name):\n",
        "           return self._create_miz_oki_response(None, \"permission_denied\", errors=[{\"message\": \"Permission denied\"}])\n",
        "       if not self.exec_client or not VERTEX_WORKFLOWS_SDK_AVAILABLE:\n",
        "           return self._create_miz_oki_response(None, \"service_unavailable\", errors=[{\"message\": \"Workflow status system (Vertex client proxy/SDK) unavailable.\"}])\n",
        "\n",
        "       status = \"pending\"; response_payload = None; errors = []\n",
        "       try:\n",
        "           request = GetExecutionRequest(name=execution_name) # Use real request type\n",
        "           execution_details_proto = await self.exec_client.get_execution(request=request) # Call proxy\n",
        "\n",
        "           # Convert proto to dict for JSON response (handle potential errors)\n",
        "           try:\n",
        "               execution_details_dict = json_format.MessageToDict(execution_details_proto._pb, preserving_proto_field_name=True)\n",
        "               # Convert state enum number to name string if possible\n",
        "               if 'state' in execution_details_dict:\n",
        "                    try: execution_details_dict['state'] = ExecutionState(execution_details_proto.state).name\n",
        "                    except ValueError: execution_details_dict['state'] = f\"UNKNOWN_STATE_{execution_details_proto.state}\"\n",
        "           except Exception as format_e:\n",
        "                logger.error(f\"Failed to format execution details proto to dict: {format_e}\")\n",
        "                # Fallback: return basic info\n",
        "                execution_details_dict = {\"name\": execution_details_proto.name, \"state\": ExecutionState(execution_details_proto.state).name, \"error\": \"Details formatting error\"}\n",
        "\n",
        "           status = \"success\"; response_payload = {\"execution\": execution_details_dict}\n",
        "\n",
        "       except google_api_exceptions.NotFound:\n",
        "            status = \"not_found\"; errors.append({\"code\": \"EXECUTION_NOT_FOUND\", \"message\": \"Workflow execution not found.\"}); logger.warning(f\"Workflow execution '{execution_name}' not found.\")\n",
        "       except google_api_exceptions.GoogleAPIError as api_e:\n",
        "            status = \"api_error\"; errors.append({\"code\": \"VERTEX_API_ERROR\", \"message\": str(api_e)}); logger.error(f\"API Error getting execution '{execution_name}' via client proxy: {api_e}\")\n",
        "       except Exception as e:\n",
        "            status = \"internal_error\"; errors.append({\"code\": \"INTERNAL_ERROR\", \"message\": f\"Internal error: {e}\"}); logger.error(f\"Error getting execution details async for '{execution_name}' via client proxy: {e}\", exc_info=True)\n",
        "\n",
        "       response = self._create_miz_oki_response(None, status, response_payload, errors if errors else None)\n",
        "       response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "       return response\n",
        "\n",
        "# --- Initialization (Conceptual - within API framework like FastAPI) ---\n",
        "# human_agent_interface_api: Optional[HumanAgentInterfaceAPI] = None\n",
        "\n",
        "# async def initialize_human_api():\n",
        "#     global human_agent_interface_api\n",
        "#     if not _config_obj or not _real_backend_tools: # Check if config and tool proxies are ready\n",
        "#         logger.critical(\"Cannot initialize Human API: Config or backend tool proxies missing.\")\n",
        "#         return\n",
        "#     try:\n",
        "#         human_agent_interface_api = HumanAgentInterfaceAPI(\n",
        "#             decision_engine_proxy=_hde_tool_proxy,\n",
        "#             goal_generator_proxy=_agg_tool_proxy,\n",
        "#             validator_proxy=_validator_tool_proxy,\n",
        "#             xai_proxy=_xai_tool_proxy,\n",
        "#             executions_client_proxy=_exec_client_proxy, # Pass REAL client proxy instance\n",
        "#             pubsub_client_proxy=_pubsub_client_proxy, # Pass REAL client proxy instance\n",
        "#             config=_config_obj\n",
        "#         )\n",
        "#         logger.info(\"HumanAgentInterfaceAPI initialized.\")\n",
        "#     except Exception as e:\n",
        "#         logger.critical(f\"HumanAgentInterfaceAPI initialization failed: {e}\", exc_info=True)\n",
        "#         human_agent_interface_api = None\n",
        "\n",
        "# --- Example FastAPI Endpoint Definition (Conceptual) ---\n",
        "# if _real_backend_tools and human_agent_interface_api: # Only define if dependencies are real\n",
        "#     @app.get(\"/reviews/pending\", response_model=MizOkiResponse)\n",
        "#     async def get_reviews(user_id: str = Query(...), limit: int = Query(20)):\n",
        "#         # Assumes user_id is obtained from auth middleware\n",
        "#         result = await human_agent_interface_api.get_pending_reviews(user_id, limit)\n",
        "#         # FastAPI handles converting dict to JSON response\n",
        "#         return result\n",
        "#\n",
        "#     @app.post(\"/reviews/{review_id}/approve\", response_model=MizOkiResponse)\n",
        "#     async def approve(user_id: str = Query(...), review_id: str = Path(...), payload: Optional[Dict] = Body(None)):\n",
        "#         result = await human_agent_interface_api.approve_action(user_id, review_id, payload.get(\"approval_data\"), payload.get(\"comments\"))\n",
        "#         return result\n",
        "#     # ... other endpoints ...\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 Human-Agent Interface API Logic (Cell 16 - Reworked) ---\")\n",
        "print(\"Uses REAL Vertex AI Executions client proxy (if available) for workflow interactions.\")\n",
        "print(\"Calls backend tools via injected proxies (conceptual MIZ OKI APIs).\")\n",
        "print(\"Requires deployment within an API framework (e.g., FastAPI).\")\n",
        "print(\"--------------------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Ao6Kfb3A4CjX",
        "outputId": "851d4ebd-d295-4bb0-ee8b-be6964b46b8d"
      },
      "id": "Ao6Kfb3A4CjX",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-18-5ce5a48db9ae>, line 34)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-5ce5a48db9ae>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    class executions_v1: ExecutionState = ExecutionState; class ExecutionsAsyncClient: pass; class Execution(DummyProto): pass; class CreateExecutionRequest: pass; class GetExecutionRequest: pass; class CancelExecutionRequest: pass; class ListExecutionsRequest: pass; class ListExecutionsResponse: pass; ExecutionView = type('Enum', (), {'BASIC': 1, 'FULL': 2})()\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: MLOps & Training Pipelines (Vertex AI Integration) (Reworked)\n",
        "# Status: Uses google-auth for secure MoE registry API call. KFP v1 structure maintained.\n",
        "#         Component logic remains placeholder. LLM pipeline needs separate definition.\n",
        "#         Added conceptual orchestrator class for triggering/monitoring.\n",
        "\n",
        "import kfp\n",
        "from kfp import dsl\n",
        "from kfp import compiler as v1_compiler\n",
        "# Check if components are available, otherwise define dummies for compilation\n",
        "try:\n",
        "    from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
        "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
        "    GCPC_V1_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GCPC_V1_AVAILABLE = False\n",
        "    logging.warning(\"google-cloud-pipeline-components v1 not found. Using dummy Ops for pipeline definition.\")\n",
        "    # Define dummy Ops for compilation if SDK is missing\n",
        "    @dsl.component\n",
        "    def EndpointCreateOp(project: str, location: str, display_name: str, labels: dict) -> dsl.OutputPath(dsl.Artifact): return dsl.OutputPath(dsl.Artifact)\n",
        "    @dsl.component\n",
        "    def ModelUploadOp(project: str, location: str, display_name: str, artifact_uri: str, serving_container_image_uri: str, labels: dict) -> dsl.OutputPath(dsl.Artifact): return dsl.OutputPath(dsl.Artifact)\n",
        "    @dsl.component\n",
        "    def ModelDeployOp(project: str, endpoint: dsl.Input[dsl.Artifact], model: dsl.Input[dsl.Artifact], deployed_model_display_name: str, machine_type: str, traffic_split: dict) -> dsl.OutputPath(dsl.Artifact): return dsl.OutputPath(dsl.Artifact)\n",
        "\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import random # Added for component simulation\n",
        "\n",
        "# --- Ensure Logger and Config Vars are Available ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('MIZ-OKI.MLOps')\n",
        "\n",
        "# Load config using CONFIG_OBJ from Cell 1 (reworked)\n",
        "try:\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise ValueError(\"CONFIG_OBJ not found or is None.\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    PROJECT_ID = CONFIG_OBJ.gcp.project_id\n",
        "    REGION = CONFIG_OBJ.gcp.region\n",
        "    BUCKET_NAME = CONFIG_OBJ.gcp.gcs_bucket_name\n",
        "    PIPELINE_ROOT = CONFIG_OBJ.mlops_pipeline_root\n",
        "    # Use service_endpoints config for MoE registry URL\n",
        "    MOE_REGISTRY_ENDPOINT = CONFIG_OBJ.service_endpoints.moe_registry_api_endpoint\n",
        "    MLOPS_TRIGGER_TOPIC = CONFIG_OBJ.mlops_trigger_topic\n",
        "    MLOPS_SERVING_IMAGE = CONFIG_OBJ.mlops_serving_image\n",
        "\n",
        "    if not all([PROJECT_ID, REGION, BUCKET_NAME, PIPELINE_ROOT, MLOPS_SERVING_IMAGE]):\n",
        "        raise ValueError(\"Essential GCP config (Project, Region, Bucket, Pipeline Root, Serving Image) missing for MLOps.\")\n",
        "    if not MOE_REGISTRY_ENDPOINT:\n",
        "         logger.warning(\"MOE_REGISTRY_API_ENDPOINT not configured. Pipeline registration step will fail if included.\")\n",
        "\n",
        "    TIMESTAMP = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n",
        "    logger.info(f\"MLOps Config: Project={PROJECT_ID}, Region={REGION}, PipelineRoot={PIPELINE_ROOT}\")\n",
        "    MLOPS_CONFIG_LOADED = True\n",
        "except Exception as config_err:\n",
        "   logger.critical(f\"MLOps Configuration Error: {config_err}. Cannot define pipeline.\", exc_info=True)\n",
        "   # Define dummies to allow script execution but pipeline will be invalid\n",
        "   PROJECT_ID, REGION, BUCKET_NAME, PIPELINE_ROOT, MOE_REGISTRY_ENDPOINT, MLOPS_TRIGGER_TOPIC, MLOPS_SERVING_IMAGE = \"dummy\", \"dummy\", \"dummy\", \"gs://dummy/pipelines\", None, \"dummy\", \"dummy-image\"\n",
        "   TIMESTAMP = \"dummy-ts\"\n",
        "   MLOPS_CONFIG_LOADED = False\n",
        "\n",
        "# --- Pipeline Components (Placeholders - Need Real Implementation) ---\n",
        "\n",
        "# @kfp.dsl.component(...) # Keep decorator\n",
        "# def prepare_data_op(...): ... # Keep signature and placeholder logic from previous rework\n",
        "\n",
        "# @kfp.dsl.component(...) # Keep decorator\n",
        "# def train_expert_model_op(...): ... # Keep signature and placeholder logic from previous rework\n",
        "\n",
        "# @kfp.dsl.component(...) # Keep decorator\n",
        "# def evaluate_model_op(...): ... # Keep signature and placeholder logic from previous rework\n",
        "\n",
        "# --- [Refined update_moe_manager_op definition - Uses google-auth] ---\n",
        "@kfp.dsl.component(\n",
        "    base_image=\"python:3.10\", # Use a standard Python image\n",
        "    packages_to_install=[\"google-cloud-aiplatform\", \"requests\", \"google-auth\"] # Add necessary packages\n",
        ")\n",
        "def update_moe_manager_op(\n",
        "    expert_id: str, # Unique ID for the expert (e.g., model display name)\n",
        "    model_resource_name: str, # Full Vertex AI Model resource name (projects/.../models/...)\n",
        "    endpoint_resource_name: str, # Full Vertex AI Endpoint resource name (projects/.../endpoints/...)\n",
        "    task_type: str, # e.g., 'classification', 'forecasting', 'recommendation'\n",
        "    domain: str, # e.g., 'roas', 'churn', 'product_similarity'\n",
        "    metrics_json: dsl.Input[dsl.Artifact], # Input artifact containing evaluation metrics JSON file\n",
        "    moe_registry_api_endpoint: str, # URL of the deployed MoE Registry API service\n",
        "    project: str, # GCP Project ID (for logging/context)\n",
        "    location: str # GCP Region (for logging/context)\n",
        "):\n",
        "   \"\"\"\n",
        "   Calls the MoE Registry API (deployed as a separate service, e.g., Cloud Run)\n",
        "   to register or update an expert model using google-auth for secure invocation.\n",
        "   \"\"\"\n",
        "   import logging\n",
        "   import json\n",
        "   import os\n",
        "   import requests\n",
        "   import google.auth.transport.requests\n",
        "   import google.oauth2.id_token\n",
        "\n",
        "   logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s: %(message)s')\n",
        "   logger = logging.getLogger('UpdateMoEManagerOp')\n",
        "\n",
        "   if not moe_registry_api_endpoint:\n",
        "       logger.critical(\"MoE Registry API endpoint missing! Cannot register expert.\")\n",
        "       raise ValueError(\"moe_registry_api_endpoint parameter is required.\")\n",
        "\n",
        "   logger.info(f\"Updating MoE Registry for Expert: {expert_id} via API: {moe_registry_api_endpoint}\")\n",
        "\n",
        "   # Load evaluation metrics from the input artifact\n",
        "   eval_metrics = {}\n",
        "   try:\n",
        "       with open(metrics_json.path, 'r') as f:\n",
        "           eval_metrics = json.load(f)\n",
        "       logger.info(f\"Loaded metrics: {eval_metrics}\")\n",
        "   except Exception as metrics_e:\n",
        "       logger.warning(f\"Could not load metrics from {metrics_json.path}: {metrics_e}\")\n",
        "\n",
        "   # Construct the payload for the MoE Registry API\n",
        "   registry_payload = {\n",
        "       \"expert_id\": expert_id,\n",
        "       \"vertex_model_name\": model_resource_name,\n",
        "       \"vertex_endpoint_name\": endpoint_resource_name,\n",
        "       \"task_type\": task_type,\n",
        "       \"domain\": domain,\n",
        "       \"status\": \"active\", # Mark as active upon successful deployment\n",
        "       \"evaluation_metrics\": eval_metrics,\n",
        "       \"pipeline_job_name\": os.environ.get('KFP_RUN_ID', 'unknown_kfp_run_id'), # Get KFP run ID if available\n",
        "       \"last_updated\": datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
        "   }\n",
        "\n",
        "   auth_token = None\n",
        "   try:\n",
        "       # Obtain an OIDC ID token to authenticate the call to the MoE Registry API\n",
        "       # Assumes the MoE Registry API is deployed on Cloud Run/Functions and allows authenticated invocations\n",
        "       # The pipeline's service account needs the 'roles/run.invoker' role on the MoE Registry service.\n",
        "       auth_req = google.auth.transport.requests.Request()\n",
        "       id_token = google.oauth2.id_token.fetch_id_token(auth_req, moe_registry_api_endpoint)\n",
        "       auth_token = f\"Bearer {id_token}\"\n",
        "       logger.info(f\"Fetched Google ID token for audience: {moe_registry_api_endpoint}\")\n",
        "   except Exception as auth_e:\n",
        "       logger.error(f\"Failed to get Google ID token: {auth_e}. Check pipeline SA permissions.\", exc_info=True)\n",
        "       # Decide whether to fail the pipeline or proceed without registration\n",
        "       raise RuntimeError(f\"Auth failed for MoE API: {auth_e}\") from auth_e\n",
        "\n",
        "   headers = {\"Authorization\": auth_token, \"Content-Type\": \"application/json\"}\n",
        "   # Assuming the MoE Registry API uses PUT for create/update on /experts/{expert_id}\n",
        "   expert_api_url = f\"{moe_registry_api_endpoint.rstrip('/')}/experts/{expert_id}\"\n",
        "\n",
        "   logger.info(f\"Calling MoE Registry API via PUT: {expert_api_url}\")\n",
        "   try:\n",
        "       # Make the authenticated API call\n",
        "       response = requests.put(expert_api_url, headers=headers, json=registry_payload, timeout=90) # 90s timeout\n",
        "       logger.info(f\"MoE Registry API Response Status: {response.status_code}\")\n",
        "       response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "       logger.info(f\"Successfully registered/updated expert '{expert_id}' in MoE registry.\")\n",
        "   except requests.exceptions.RequestException as api_e:\n",
        "       logger.error(f\"MoE Registry API call failed: {api_e}\")\n",
        "       # Log response body if available for debugging\n",
        "       try: logger.error(f\"API Response Body: {api_e.response.text}\")\n",
        "       except: pass\n",
        "       raise RuntimeError(f\"Failed to update MoE registry via API: {api_e}\") from api_e\n",
        "   except Exception as e:\n",
        "       logger.error(f\"Update MoE Manager op failed unexpectedly: {e}\", exc_info=True)\n",
        "       raise\n",
        "\n",
        "# --- Define the Training Pipeline (Structure remains same, uses refined component) ---\n",
        "@kfp.dsl.pipeline(\n",
        "    name=\"miz3-expert-training-pipeline-v1deploy-apireg-reworked\",\n",
        "    description=\"Trains, evaluates, deploys MIZ 3.0 expert models, registers via API.\",\n",
        "    pipeline_root=PIPELINE_ROOT\n",
        ")\n",
        "def expert_training_pipeline_v1deploy_apireg(\n",
        "   project: str = PROJECT_ID,\n",
        "   location: str = REGION,\n",
        "   source_uri_or_query: str, # GCS path (e.g., gs://...) or BQ table (project.dataset.table)\n",
        "   target_column: str, # Name of the target variable column\n",
        "   model_display_name_prefix: str, # Prefix for the deployed Vertex AI Model name\n",
        "   task_type: str, # e.g., 'classification', 'forecasting', 'recommendation'\n",
        "   expert_domain: str, # e.g., 'roas', 'churn', 'product_similarity'\n",
        "   # Ensure MOE_REGISTRY_ENDPOINT is passed or available globally/via config\n",
        "   moe_registry_api_endpoint: str = MOE_REGISTRY_ENDPOINT,\n",
        "   data_source_type: str = 'gcs', # 'gcs' or 'bq'\n",
        "   output_shape_json: str = '[1]', # JSON string representing output shape, e.g., '[1]' for regression, '[num_classes]' for classification\n",
        "   hyperparameters_json: str = '{}', # JSON string of hyperparameters for training component\n",
        "   epochs: int = 10, # Example hyperparameter\n",
        "   batch_size: int = 32, # Example hyperparameter\n",
        "   serving_image: str = MLOPS_SERVING_IMAGE, # Serving container image URI\n",
        "   deployment_threshold_metric: str = \"accuracy\", # Metric used for deployment condition\n",
        "   deployment_threshold_value: float = 0.75, # Threshold value for deployment\n",
        "   endpoint_display_name_prefix: str = \"miz3-shared-expert-endpoint\", # Prefix for Vertex AI Endpoint\n",
        "   deploy_machine_type: str = \"n1-standard-4\", # Machine type for deployment\n",
        "   deploy_traffic_split_json: str = '{\"0\": 100}', # Deploy with 100% traffic initially\n",
        "):\n",
        "    # --- Check if GCPC SDK is available before using its Ops ---\n",
        "    if not GCPC_V1_AVAILABLE:\n",
        "        raise RuntimeError(\"google-cloud-pipeline-components v1 SDK not found. Cannot define pipeline using GCPC Ops.\")\n",
        "\n",
        "    # Generate unique names for model and endpoint using pipeline job ID\n",
        "    run_timestamp = dsl.PIPELINE_JOB_ID_PLACEHOLDER # KFP v1 placeholder\n",
        "    model_display_name = f\"{model_display_name_prefix}-{expert_domain}-{run_timestamp}\"\n",
        "    endpoint_display_name = f\"{endpoint_display_name_prefix}-{expert_domain}\" # Shared endpoint per domain\n",
        "\n",
        "    # 1. Prepare Data\n",
        "    prepare_data_task = prepare_data_op(\n",
        "        project_id=project,\n",
        "        bucket_name=BUCKET_NAME, # Assuming BUCKET_NAME is globally available from config\n",
        "        data_source_type=data_source_type,\n",
        "        source_uri_or_query=source_uri_or_query,\n",
        "        target_column=target_column\n",
        "    ).set_display_name(\"Prepare Data\")\n",
        "\n",
        "    # 2. Train Model\n",
        "    train_model_task = train_expert_model_op(\n",
        "        train_data=prepare_data_task.outputs[\"output_train_uri\"],\n",
        "        input_scaler_uri=prepare_data_task.outputs[\"output_scaler_uri\"],\n",
        "        target_column=target_column,\n",
        "        model_id_prefix=model_display_name_prefix, # Pass prefix\n",
        "        model_version=run_timestamp, # Use run ID as version\n",
        "        task_type=task_type,\n",
        "        output_shape_json=output_shape_json,\n",
        "        hyperparameters_json=hyperparameters_json,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size\n",
        "    ).set_display_name(f\"Train Expert ({task_type}/{expert_domain})\")\n",
        "\n",
        "    # 3. Evaluate Model\n",
        "    evaluate_model_task = evaluate_model_op(\n",
        "        test_data=prepare_data_task.outputs[\"output_test_uri\"],\n",
        "        model=train_model_task.outputs[\"model_dir\"],\n",
        "        input_scaler_uri=prepare_data_task.outputs[\"output_scaler_uri\"],\n",
        "        target_column=target_column\n",
        "    ).set_display_name(\"Evaluate Model\")\n",
        "\n",
        "    # 4. Conditional Deployment & Registration\n",
        "    with dsl.Condition(\n",
        "        evaluate_model_task.outputs[\"kfp_metrics\"].outputs[deployment_threshold_metric] >= deployment_threshold_value,\n",
        "        name=\"deploy-condition\"\n",
        "    ):\n",
        "        # 4a. Upload Model to Vertex AI Model Registry\n",
        "        model_upload_op = ModelUploadOp(\n",
        "            project=project,\n",
        "            location=location,\n",
        "            display_name=model_display_name,\n",
        "            artifact_uri=train_model_task.outputs[\"model_dir\"].uri, # Use .uri for artifact path\n",
        "            serving_container_image_uri=serving_image,\n",
        "            labels={\"miz_pipeline_run_id\": run_timestamp, \"miz_expert_domain\": expert_domain, \"miz_task_type\": task_type}\n",
        "        ).set_display_name(\"Upload Model\")\n",
        "\n",
        "        # 4b. Create or Get Endpoint (shared endpoint per domain)\n",
        "        endpoint_create_op = EndpointCreateOp(\n",
        "            project=project,\n",
        "            location=location,\n",
        "            display_name=endpoint_display_name, # Use consistent name for shared endpoint\n",
        "            labels={\"miz_app\": \"bgi_platform\", \"miz_domain\": expert_domain}\n",
        "        ).set_display_name(f\"Create/Get Endpoint ({expert_domain})\")\n",
        "\n",
        "        # 4c. Deploy Model to Endpoint\n",
        "        model_deploy_op = ModelDeployOp(\n",
        "            project=project,\n",
        "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
        "            model=model_upload_op.outputs[\"model\"],\n",
        "            deployed_model_display_name=model_display_name, # Unique name for this deployment\n",
        "            machine_type=deploy_machine_type,\n",
        "            traffic_split=json.loads(deploy_traffic_split_json) # Deploy with specified traffic\n",
        "        ).set_display_name(f\"Deploy Model ({deploy_traffic_split_json} Traffic)\")\n",
        "\n",
        "        # 4d. Update MoE Registry via API Call (using the refined component)\n",
        "        # Ensure the endpoint parameter is passed correctly\n",
        "        update_moe_task = update_moe_manager_op(\n",
        "            project=project,\n",
        "            location=location,\n",
        "            expert_id=model_display_name, # Use the unique model display name as expert ID\n",
        "            model_resource_name=model_upload_op.outputs[\"model\"].resource_name, # Pass full resource name\n",
        "            endpoint_resource_name=endpoint_create_op.outputs[\"endpoint\"].resource_name, # Pass full resource name\n",
        "            task_type=task_type,\n",
        "            domain=expert_domain,\n",
        "            metrics_json=evaluate_model_task.outputs[\"metrics_output_path\"],\n",
        "            moe_registry_api_endpoint=moe_registry_api_endpoint # Pass the API endpoint URL\n",
        "        ).after(model_deploy_op).set_display_name(\"Update MoE Registry API\") # Run after deployment\n",
        "\n",
        "# --- Compile Pipeline ---\n",
        "pipeline_filename = None\n",
        "if MLOPS_CONFIG_LOADED:\n",
        "    pipeline_filename = f\"miz3_expert_training_pipeline_{TIMESTAMP}.json\"\n",
        "    try:\n",
        "       if MOE_REGISTRY_ENDPOINT is None:\n",
        "           logger.warning(\"MOE_REGISTRY_API_ENDPOINT is not set. The 'update_moe_manager_op' step will fail if included in execution.\")\n",
        "           # Optionally remove the MoE update step if endpoint is missing, or let it fail during execution\n",
        "           # For compilation, we might need to pass a dummy value if the parameter is mandatory\n",
        "           # However, the component logic handles the check, so compilation might proceed.\n",
        "\n",
        "       # Use KFP v1 compiler explicitly\n",
        "       v1_compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(\n",
        "           pipeline_func=expert_training_pipeline_v1deploy_apireg,\n",
        "           package_path=pipeline_filename\n",
        "       )\n",
        "       logger.info(f\"Pipeline compiled successfully to {pipeline_filename}\")\n",
        "    except Exception as compile_e:\n",
        "       logger.critical(\"Pipeline compilation failed!\", exc_info=True)\n",
        "       pipeline_filename = None\n",
        "else:\n",
        "    logger.critical(\"MLOps configuration failed to load. Skipping pipeline compilation.\")\n",
        "\n",
        "\n",
        "# --- Conceptual Class for Triggering/Monitoring Pipelines via MIZ OKI Events ---\n",
        "class TrainingPipelineOrchestrator:\n",
        "    \"\"\"\n",
        "    Conceptual service that listens to Pub/Sub triggers (MIZ OKI format)\n",
        "    and launches/monitors Vertex AI Pipeline Jobs.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, pipeline_client: Optional[Any] = None, pubsub_client: Optional[Any] = None):\n",
        "        # In a real service, pipeline_client would be Vertex AI Pipelines client\n",
        "        # pubsub_client would be Pub/Sub client\n",
        "        self.config = config\n",
        "        self.pipeline_client = pipeline_client # e.g., aiplatform.PipelineJob\n",
        "        self.pubsub_client = pubsub_client # e.g., pubsub_v1.PublisherClient\n",
        "        self.logger = logging.getLogger(\"MIZ-OKI.TrainingPipelineOrchestrator\")\n",
        "        if not self.pipeline_client: self.logger.warning(\"Vertex AI Pipeline client not provided.\")\n",
        "        if not self.pubsub_client: self.logger.warning(\"Pub/Sub client not provided.\")\n",
        "\n",
        "    async def handle_trigger_event(self, miz_oki_event: Dict) -> Dict:\n",
        "        \"\"\"Handles a MIZ OKI event requesting a pipeline launch.\"\"\"\n",
        "        trace_id = miz_oki_event.get(\"trace_id\", f\"mlops_trigger_{uuid.uuid4().hex[:8]}\")\n",
        "        response_status = \"failed\"; response_payload = None; errors = []\n",
        "        try:\n",
        "            if not self.pipeline_client: raise RuntimeError(\"Pipeline client unavailable.\")\n",
        "            event_payload = miz_oki_event.get(\"payload\", {})\n",
        "            pipeline_name = event_payload.get(\"pipeline_name\")\n",
        "            pipeline_params = event_payload.get(\"parameters\", {})\n",
        "            if not pipeline_name or not pipeline_params:\n",
        "                raise ValueError(\"Missing 'pipeline_name' or 'parameters' in trigger event payload.\")\n",
        "\n",
        "            # --- TODO: Map pipeline_name to the compiled template path (e.g., from GCS) ---\n",
        "            template_path = f\"{PIPELINE_ROOT}/{pipeline_name}.json\" # Example mapping\n",
        "            if not template_path: raise ValueError(f\"Cannot find template for pipeline: {pipeline_name}\")\n",
        "            # --- End TODO ---\n",
        "\n",
        "            job_id = f\"{pipeline_name}-run-{datetime.datetime.now(datetime.timezone.utc).strftime('%Y%m%d%H%M%S')}\"\n",
        "            logger.info(f\"Submitting Vertex AI Pipeline job '{job_id}' from template '{template_path}' with params: {pipeline_params}\")\n",
        "\n",
        "            # Use Vertex AI SDK to submit the job (this part is synchronous in current SDK)\n",
        "            # Needs to be run in a thread for async context\n",
        "            def _submit_job():\n",
        "                job = aiplatform.PipelineJob(\n",
        "                    display_name=job_id,\n",
        "                    template_path=template_path,\n",
        "                    pipeline_root=PIPELINE_ROOT,\n",
        "                    parameter_values=pipeline_params,\n",
        "                    project=self.config.gcp.project_id,\n",
        "                    location=self.config.gcp.region,\n",
        "                    # enable_caching=True, # Optional\n",
        "                )\n",
        "                job.submit() # Submits the job\n",
        "                return job.resource_name # Return the job resource name\n",
        "\n",
        "            job_resource_name = await asyncio.to_thread(_submit_job)\n",
        "            logger.info(f\"Successfully submitted Vertex AI Pipeline job: {job_resource_name}\")\n",
        "            response_status = \"submitted\"; response_payload = {\"job_resource_name\": job_resource_name, \"job_id_prefix\": job_id}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Pipeline launch error from trigger event: {e}\", exc_info=True)\n",
        "            errors.append({\"code\": \"PIPELINE_SUBMIT_ERROR\", \"message\": str(e)})\n",
        "            response_status = \"error\"\n",
        "\n",
        "        # Return a MIZ OKI response (optional, depends if caller needs confirmation)\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": miz_oki_event.get(\"request_id\"), \"trace_id\": trace_id,\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"TrainingPipelineOrchestrator\",\n",
        "            \"status\": response_status, \"payload\": response_payload, \"error_details\": errors\n",
        "        }\n",
        "\n",
        "    async def handle_completion_event(self, miz_oki_event: Dict) -> Dict:\n",
        "        \"\"\"Handles Pub/Sub event for completed training, triggers post-processing/evaluation.\"\"\"\n",
        "        # This would typically be triggered by the pipeline itself publishing to a topic upon completion.\n",
        "        # The event payload should contain job details (resource name, status, outputs).\n",
        "        trace_id = miz_oki_event.get(\"trace_id\", f\"mlops_complete_{uuid.uuid4().hex[:8]}\")\n",
        "        response_status = \"failed\"; response_payload = None; errors = []\n",
        "        try:\n",
        "            job_details = miz_oki_event.get(\"payload\", {})\n",
        "            job_resource_name = job_details.get(\"job_resource_name\")\n",
        "            job_status = job_details.get(\"status\") # e.g., 'PIPELINE_STATE_SUCCEEDED', 'PIPELINE_STATE_FAILED'\n",
        "            if not job_resource_name or not job_status:\n",
        "                raise ValueError(\"Missing job details in completion event.\")\n",
        "\n",
        "            logger.info(f\"Handling pipeline completion event for job: {job_resource_name}, Status: {job_status}\")\n",
        "\n",
        "            if job_status == 'PIPELINE_STATE_SUCCEEDED':\n",
        "                # --- TODO: Trigger next steps ---\n",
        "                # - Call MoE Registry update (if not done in pipeline)\n",
        "                # - Trigger evaluation workflows\n",
        "                # - Update KG with model metadata\n",
        "                # Example: Publish event for LI Tool\n",
        "                # if self.pubsub_client:\n",
        "                #     li_event = {...}\n",
        "                #     await self.pubsub_client.publish(...)\n",
        "                # --- End TODO ---\n",
        "                response_status = \"success\"; response_payload = {\"job_resource_name\": job_resource_name, \"action\": \"Post-processing triggered (placeholder)\"}\n",
        "            else:\n",
        "                # Handle failed pipeline\n",
        "                logger.error(f\"Pipeline job {job_resource_name} failed. Status: {job_status}. Details: {job_details.get('error')}\")\n",
        "                response_status = \"failed\"; errors.append({\"code\": \"PIPELINE_FAILED\", \"message\": f\"Job {job_resource_name} failed.\", \"details\": job_details.get('error')})\n",
        "                # --- TODO: Trigger alerting or remediation ---\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Pipeline completion handler error: {e}\", exc_info=True)\n",
        "            errors.append({\"code\": \"HANDLER_ERROR\", \"message\": str(e)})\n",
        "            response_status = \"error\"\n",
        "\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": miz_oki_event.get(\"request_id\"), \"trace_id\": trace_id,\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"TrainingPipelineOrchestrator\",\n",
        "            \"status\": response_status, \"payload\": response_payload, \"error_details\": errors\n",
        "        }\n",
        "\n",
        "\n",
        "# --- Notes on Execution ---\n",
        "logger.info(\"--- MLOps Pipeline Execution Notes (Reworked) ---\")\n",
        "if MLOPS_CONFIG_LOADED and pipeline_filename:\n",
        "    logger.info(f\"1. Upload compiled pipeline: '{pipeline_filename}' to GCS bucket '{BUCKET_NAME}' (e.g., under {PIPELINE_ROOT}).\")\n",
        "    logger.info(f\"2. Setup Cloud Function/Run service triggered by Pub/Sub topic: '{MLOPS_TRIGGER_TOPIC}'.\")\n",
        "    logger.info(\"3. The trigger service should instantiate TrainingPipelineOrchestrator and call handle_trigger_event.\")\n",
        "    logger.info(f\"4. Ensure MoE Registry API ('{MOE_REGISTRY_ENDPOINT or 'endpoint_not_set'}') is deployed & pipeline SA has 'roles/run.invoker'.\")\n",
        "    logger.info(\"5. Implement REAL logic in pipeline components (prepare_data_op, train_expert_model_op, evaluate_model_op).\")\n",
        "    logger.info(\"6. Define SEPARATE KFP pipelines for LLM Fine-Tuning/Distillation (likely using CustomJobOp or specific GCPC components).\")\n",
        "else:\n",
        "     logger.error(\"MLOps pipeline configuration incomplete or compilation failed. Deployment steps cannot be determined.\")\n",
        "\n",
        "print(f\"\\n--- MIZ 3.0 MLOps Pipeline Definition Compiled (Cell 17 - Reworked) ---\")\n",
        "if pipeline_filename: print(f\"Pipeline definition saved to: {pipeline_filename}\")\n",
        "else: print(\"Pipeline compilation FAILED. Check logs.\")\n",
        "print(\"-----------------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "eMhNczJ84LP9",
        "outputId": "056a4951-57bb-4a0a-afe1-0ac712f11f64"
      },
      "id": "eMhNczJ84LP9",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "non-default argument follows default argument (<ipython-input-19-287087a799ae>, line 178)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-287087a799ae>\"\u001b[0;36m, line \u001b[0;32m178\u001b[0m\n\u001b[0;31m    source_uri_or_query: str, # GCS path (e.g., gs://...) or BQ table (project.dataset.table)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: MLOps & Training Pipelines (Vertex AI Integration) (Reworked)\n",
        "# Status: Uses google-auth for secure MoE registry API call. KFP v1 structure maintained.\n",
        "#         Component logic remains placeholder. LLM pipeline needs separate definition.\n",
        "#         Added conceptual orchestrator class for triggering/monitoring.\n",
        "\n",
        "import kfp\n",
        "from kfp import dsl\n",
        "from kfp import compiler as v1_compiler\n",
        "# Check if components are available, otherwise define dummies for compilation\n",
        "try:\n",
        "    from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
        "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
        "    GCPC_V1_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GCPC_V1_AVAILABLE = False\n",
        "    logging.warning(\"google-cloud-pipeline-components v1 not found. Using dummy Ops for pipeline definition.\")\n",
        "    # Define dummy Ops for compilation if SDK is missing\n",
        "    @dsl.component\n",
        "    def EndpointCreateOp(project: str, location: str, display_name: str, labels: dict) -> dsl.OutputPath(dsl.Artifact): return dsl.OutputPath(dsl.Artifact)\n",
        "    @dsl.component\n",
        "    def ModelUploadOp(project: str, location: str, display_name: str, artifact_uri: str, serving_container_image_uri: str, labels: dict) -> dsl.OutputPath(dsl.Artifact): return dsl.OutputPath(dsl.Artifact)\n",
        "    @dsl.component\n",
        "    def ModelDeployOp(project: str, endpoint: dsl.Input[dsl.Artifact], model: dsl.Input[dsl.Artifact], deployed_model_display_name: str, machine_type: str, traffic_split: dict) -> dsl.OutputPath(dsl.Artifact): return dsl.OutputPath(dsl.Artifact)\n",
        "\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import random # Added for component simulation\n",
        "\n",
        "# --- Ensure Logger and Config Vars are Available ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('MIZ-OKI.MLOps')\n",
        "\n",
        "# Load config using CONFIG_OBJ from Cell 1 (reworked)\n",
        "try:\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise ValueError(\"CONFIG_OBJ not found or is None.\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "\n",
        "    PROJECT_ID = CONFIG_OBJ.gcp.project_id\n",
        "    REGION = CONFIG_OBJ.gcp.region\n",
        "    BUCKET_NAME = CONFIG_OBJ.gcp.gcs_bucket_name\n",
        "    PIPELINE_ROOT = CONFIG_OBJ.mlops_pipeline_root\n",
        "    # Use service_endpoints config for MoE registry URL\n",
        "    MOE_REGISTRY_ENDPOINT = CONFIG_OBJ.service_endpoints.moe_registry_api_endpoint\n",
        "    MLOPS_TRIGGER_TOPIC = CONFIG_OBJ.mlops_trigger_topic\n",
        "    MLOPS_SERVING_IMAGE = CONFIG_OBJ.mlops_serving_image\n",
        "\n",
        "    if not all([PROJECT_ID, REGION, BUCKET_NAME, PIPELINE_ROOT, MLOPS_SERVING_IMAGE]):\n",
        "        raise ValueError(\"Essential GCP config (Project, Region, Bucket, Pipeline Root, Serving Image) missing for MLOps.\")\n",
        "    if not MOE_REGISTRY_ENDPOINT:\n",
        "         logger.warning(\"MOE_REGISTRY_API_ENDPOINT not configured. Pipeline registration step will fail if included.\")\n",
        "\n",
        "    TIMESTAMP = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n",
        "    logger.info(f\"MLOps Config: Project={PROJECT_ID}, Region={REGION}, PipelineRoot={PIPELINE_ROOT}\")\n",
        "    MLOPS_CONFIG_LOADED = True\n",
        "except Exception as config_err:\n",
        "   logger.critical(f\"MLOps Configuration Error: {config_err}. Cannot define pipeline.\", exc_info=True)\n",
        "   # Define dummies to allow script execution but pipeline will be invalid\n",
        "   PROJECT_ID, REGION, BUCKET_NAME, PIPELINE_ROOT, MOE_REGISTRY_ENDPOINT, MLOPS_TRIGGER_TOPIC, MLOPS_SERVING_IMAGE = \"dummy\", \"dummy\", \"dummy\", \"gs://dummy/pipelines\", None, \"dummy\", \"dummy-image\"\n",
        "   TIMESTAMP = \"dummy-ts\"\n",
        "   MLOPS_CONFIG_LOADED = False\n",
        "\n",
        "# --- Pipeline Components (Placeholders - Need Real Implementation) ---\n",
        "\n",
        "# @kfp.dsl.component(...) # Keep decorator\n",
        "# def prepare_data_op(...): ... # Keep signature and placeholder logic from previous rework\n",
        "\n",
        "# @kfp.dsl.component(...) # Keep decorator\n",
        "# def train_expert_model_op(...): ... # Keep signature and placeholder logic from previous rework\n",
        "\n",
        "# @kfp.dsl.component(...) # Keep decorator\n",
        "# def evaluate_model_op(...): ... # Keep signature and placeholder logic from previous rework\n",
        "\n",
        "# --- [Refined update_moe_manager_op definition - Uses google-auth] ---\n",
        "@kfp.dsl.component(\n",
        "    base_image=\"python:3.10\", # Use a standard Python image\n",
        "    packages_to_install=[\"google-cloud-aiplatform\", \"requests\", \"google-auth\"] # Add necessary packages\n",
        ")\n",
        "def update_moe_manager_op(\n",
        "    expert_id: str, # Unique ID for the expert (e.g., model display name)\n",
        "    model_resource_name: str, # Full Vertex AI Model resource name (projects/.../models/...)\n",
        "    endpoint_resource_name: str, # Full Vertex AI Endpoint resource name (projects/.../endpoints/...)\n",
        "    task_type: str, # e.g., 'classification', 'forecasting', 'recommendation'\n",
        "    domain: str, # e.g., 'roas', 'churn', 'product_similarity'\n",
        "    metrics_json: dsl.Input[dsl.Artifact], # Input artifact containing evaluation metrics JSON file\n",
        "    moe_registry_api_endpoint: str, # URL of the deployed MoE Registry API service\n",
        "    project: str, # GCP Project ID (for logging/context)\n",
        "    location: str # GCP Region (for logging/context)\n",
        "):\n",
        "   \"\"\"\n",
        "   Calls the MoE Registry API (deployed as a separate service, e.g., Cloud Run)\n",
        "   to register or update an expert model using google-auth for secure invocation.\n",
        "   \"\"\"\n",
        "   import logging\n",
        "   import json\n",
        "   import os\n",
        "   import requests\n",
        "   import google.auth.transport.requests\n",
        "   import google.oauth2.id_token\n",
        "\n",
        "   logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s: %(message)s')\n",
        "   logger = logging.getLogger('UpdateMoEManagerOp')\n",
        "\n",
        "   if not moe_registry_api_endpoint:\n",
        "       logger.critical(\"MoE Registry API endpoint missing! Cannot register expert.\")\n",
        "       raise ValueError(\"moe_registry_api_endpoint parameter is required.\")\n",
        "\n",
        "   logger.info(f\"Updating MoE Registry for Expert: {expert_id} via API: {moe_registry_api_endpoint}\")\n",
        "\n",
        "   # Load evaluation metrics from the input artifact\n",
        "   eval_metrics = {}\n",
        "   try:\n",
        "       with open(metrics_json.path, 'r') as f:\n",
        "           eval_metrics = json.load(f)\n",
        "       logger.info(f\"Loaded metrics: {eval_metrics}\")\n",
        "   except Exception as metrics_e:\n",
        "       logger.warning(f\"Could not load metrics from {metrics_json.path}: {metrics_e}\")\n",
        "\n",
        "   # Construct the payload for the MoE Registry API\n",
        "   registry_payload = {\n",
        "       \"expert_id\": expert_id,\n",
        "       \"vertex_model_name\": model_resource_name,\n",
        "       \"vertex_endpoint_name\": endpoint_resource_name,\n",
        "       \"task_type\": task_type,\n",
        "       \"domain\": domain,\n",
        "       \"status\": \"active\", # Mark as active upon successful deployment\n",
        "       \"evaluation_metrics\": eval_metrics,\n",
        "       \"pipeline_job_name\": os.environ.get('KFP_RUN_ID', 'unknown_kfp_run_id'), # Get KFP run ID if available\n",
        "       \"last_updated\": datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
        "   }\n",
        "\n",
        "   auth_token = None\n",
        "   try:\n",
        "       # Obtain an OIDC ID token to authenticate the call to the MoE Registry API\n",
        "       # Assumes the MoE Registry API is deployed on Cloud Run/Functions and allows authenticated invocations\n",
        "       # The pipeline's service account needs the 'roles/run.invoker' role on the MoE Registry service.\n",
        "       auth_req = google.auth.transport.requests.Request()\n",
        "       id_token = google.oauth2.id_token.fetch_id_token(auth_req, moe_registry_api_endpoint)\n",
        "       auth_token = f\"Bearer {id_token}\"\n",
        "       logger.info(f\"Fetched Google ID token for audience: {moe_registry_api_endpoint}\")\n",
        "   except Exception as auth_e:\n",
        "       logger.error(f\"Failed to get Google ID token: {auth_e}. Check pipeline SA permissions.\", exc_info=True)\n",
        "       # Decide whether to fail the pipeline or proceed without registration\n",
        "       raise RuntimeError(f\"Auth failed for MoE API: {auth_e}\") from auth_e\n",
        "\n",
        "   headers = {\"Authorization\": auth_token, \"Content-Type\": \"application/json\"}\n",
        "   # Assuming the MoE Registry API uses PUT for create/update on /experts/{expert_id}\n",
        "   expert_api_url = f\"{moe_registry_api_endpoint.rstrip('/')}/experts/{expert_id}\"\n",
        "\n",
        "   logger.info(f\"Calling MoE Registry API via PUT: {expert_api_url}\")\n",
        "   try:\n",
        "       # Make the authenticated API call\n",
        "       response = requests.put(expert_api_url, headers=headers, json=registry_payload, timeout=90) # 90s timeout\n",
        "       logger.info(f\"MoE Registry API Response Status: {response.status_code}\")\n",
        "       response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "       logger.info(f\"Successfully registered/updated expert '{expert_id}' in MoE registry.\")\n",
        "   except requests.exceptions.RequestException as api_e:\n",
        "       logger.error(f\"MoE Registry API call failed: {api_e}\")\n",
        "       # Log response body if available for debugging\n",
        "       try: logger.error(f\"API Response Body: {api_e.response.text}\")\n",
        "       except: pass\n",
        "       raise RuntimeError(f\"Failed to update MoE registry via API: {api_e}\") from api_e\n",
        "   except Exception as e:\n",
        "       logger.error(f\"Update MoE Manager op failed unexpectedly: {e}\", exc_info=True)\n",
        "       raise\n",
        "\n",
        "# --- Define the Training Pipeline (Structure remains same, uses refined component) ---\n",
        "@kfp.dsl.pipeline(\n",
        "    name=\"miz3-expert-training-pipeline-v1deploy-apireg-reworked\",\n",
        "    description=\"Trains, evaluates, deploys MIZ 3.0 expert models, registers via API.\",\n",
        "    pipeline_root=PIPELINE_ROOT\n",
        ")\n",
        "def expert_training_pipeline_v1deploy_apireg(\n",
        "   project: str = PROJECT_ID,\n",
        "   location: str = REGION,\n",
        "   source_uri_or_query: str, # GCS path (e.g., gs://...) or BQ table (project.dataset.table)\n",
        "   target_column: str, # Name of the target variable column\n",
        "   model_display_name_prefix: str, # Prefix for the deployed Vertex AI Model name\n",
        "   task_type: str, # e.g., 'classification', 'forecasting', 'recommendation'\n",
        "   expert_domain: str, # e.g., 'roas', 'churn', 'product_similarity'\n",
        "   # Ensure MOE_REGISTRY_ENDPOINT is passed or available globally/via config\n",
        "   moe_registry_api_endpoint: str = MOE_REGISTRY_ENDPOINT,\n",
        "   data_source_type: str = 'gcs', # 'gcs' or 'bq'\n",
        "   output_shape_json: str = '[1]', # JSON string representing output shape, e.g., '[1]' for regression, '[num_classes]' for classification\n",
        "   hyperparameters_json: str = '{}', # JSON string of hyperparameters for training component\n",
        "   epochs: int = 10, # Example hyperparameter\n",
        "   batch_size: int = 32, # Example hyperparameter\n",
        "   serving_image: str = MLOPS_SERVING_IMAGE, # Serving container image URI\n",
        "   deployment_threshold_metric: str = \"accuracy\", # Metric used for deployment condition\n",
        "   deployment_threshold_value: float = 0.75, # Threshold value for deployment\n",
        "   endpoint_display_name_prefix: str = \"miz3-shared-expert-endpoint\", # Prefix for Vertex AI Endpoint\n",
        "   deploy_machine_type: str = \"n1-standard-4\", # Machine type for deployment\n",
        "   deploy_traffic_split_json: str = '{\"0\": 100}', # Deploy with 100% traffic initially\n",
        "):\n",
        "    # --- Check if GCPC SDK is available before using its Ops ---\n",
        "    if not GCPC_V1_AVAILABLE:\n",
        "        raise RuntimeError(\"google-cloud-pipeline-components v1 SDK not found. Cannot define pipeline using GCPC Ops.\")\n",
        "\n",
        "    # Generate unique names for model and endpoint using pipeline job ID\n",
        "    run_timestamp = dsl.PIPELINE_JOB_ID_PLACEHOLDER # KFP v1 placeholder\n",
        "    model_display_name = f\"{model_display_name_prefix}-{expert_domain}-{run_timestamp}\"\n",
        "    endpoint_display_name = f\"{endpoint_display_name_prefix}-{expert_domain}\" # Shared endpoint per domain\n",
        "\n",
        "    # 1. Prepare Data\n",
        "    prepare_data_task = prepare_data_op(\n",
        "        project_id=project,\n",
        "        bucket_name=BUCKET_NAME, # Assuming BUCKET_NAME is globally available from config\n",
        "        data_source_type=data_source_type,\n",
        "        source_uri_or_query=source_uri_or_query,\n",
        "        target_column=target_column\n",
        "    ).set_display_name(\"Prepare Data\")\n",
        "\n",
        "    # 2. Train Model\n",
        "    train_model_task = train_expert_model_op(\n",
        "        train_data=prepare_data_task.outputs[\"output_train_uri\"],\n",
        "        input_scaler_uri=prepare_data_task.outputs[\"output_scaler_uri\"],\n",
        "        target_column=target_column,\n",
        "        model_id_prefix=model_display_name_prefix, # Pass prefix\n",
        "        model_version=run_timestamp, # Use run ID as version\n",
        "        task_type=task_type,\n",
        "        output_shape_json=output_shape_json,\n",
        "        hyperparameters_json=hyperparameters_json,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size\n",
        "    ).set_display_name(f\"Train Expert ({task_type}/{expert_domain})\")\n",
        "\n",
        "    # 3. Evaluate Model\n",
        "    evaluate_model_task = evaluate_model_op(\n",
        "        test_data=prepare_data_task.outputs[\"output_test_uri\"],\n",
        "        model=train_model_task.outputs[\"model_dir\"],\n",
        "        input_scaler_uri=prepare_data_task.outputs[\"output_scaler_uri\"],\n",
        "        target_column=target_column\n",
        "    ).set_display_name(\"Evaluate Model\")\n",
        "\n",
        "    # 4. Conditional Deployment & Registration\n",
        "    with dsl.Condition(\n",
        "        evaluate_model_task.outputs[\"kfp_metrics\"].outputs[deployment_threshold_metric] >= deployment_threshold_value,\n",
        "        name=\"deploy-condition\"\n",
        "    ):\n",
        "        # 4a. Upload Model to Vertex AI Model Registry\n",
        "        model_upload_op = ModelUploadOp(\n",
        "            project=project,\n",
        "            location=location,\n",
        "            display_name=model_display_name,\n",
        "            artifact_uri=train_model_task.outputs[\"model_dir\"].uri, # Use .uri for artifact path\n",
        "            serving_container_image_uri=serving_image,\n",
        "            labels={\"miz_pipeline_run_id\": run_timestamp, \"miz_expert_domain\": expert_domain, \"miz_task_type\": task_type}\n",
        "        ).set_display_name(\"Upload Model\")\n",
        "\n",
        "        # 4b. Create or Get Endpoint (shared endpoint per domain)\n",
        "        endpoint_create_op = EndpointCreateOp(\n",
        "            project=project,\n",
        "            location=location,\n",
        "            display_name=endpoint_display_name, # Use consistent name for shared endpoint\n",
        "            labels={\"miz_app\": \"bgi_platform\", \"miz_domain\": expert_domain}\n",
        "        ).set_display_name(f\"Create/Get Endpoint ({expert_domain})\")\n",
        "\n",
        "        # 4c. Deploy Model to Endpoint\n",
        "        model_deploy_op = ModelDeployOp(\n",
        "            project=project,\n",
        "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
        "            model=model_upload_op.outputs[\"model\"],\n",
        "            deployed_model_display_name=model_display_name, # Unique name for this deployment\n",
        "            machine_type=deploy_machine_type,\n",
        "            traffic_split=json.loads(deploy_traffic_split_json) # Deploy with specified traffic\n",
        "        ).set_display_name(f\"Deploy Model ({deploy_traffic_split_json} Traffic)\")\n",
        "\n",
        "        # 4d. Update MoE Registry via API Call (using the refined component)\n",
        "        # Ensure the endpoint parameter is passed correctly\n",
        "        update_moe_task = update_moe_manager_op(\n",
        "            project=project,\n",
        "            location=location,\n",
        "            expert_id=model_display_name, # Use the unique model display name as expert ID\n",
        "            model_resource_name=model_upload_op.outputs[\"model\"].resource_name, # Pass full resource name\n",
        "            endpoint_resource_name=endpoint_create_op.outputs[\"endpoint\"].resource_name, # Pass full resource name\n",
        "            task_type=task_type,\n",
        "            domain=expert_domain,\n",
        "            metrics_json=evaluate_model_task.outputs[\"metrics_output_path\"],\n",
        "            moe_registry_api_endpoint=moe_registry_api_endpoint # Pass the API endpoint URL\n",
        "        ).after(model_deploy_op).set_display_name(\"Update MoE Registry API\") # Run after deployment\n",
        "\n",
        "# --- Compile Pipeline ---\n",
        "pipeline_filename = None\n",
        "if MLOPS_CONFIG_LOADED:\n",
        "    pipeline_filename = f\"miz3_expert_training_pipeline_{TIMESTAMP}.json\"\n",
        "    try:\n",
        "       if MOE_REGISTRY_ENDPOINT is None:\n",
        "           logger.warning(\"MOE_REGISTRY_API_ENDPOINT is not set. The 'update_moe_manager_op' step will fail if included in execution.\")\n",
        "           # Optionally remove the MoE update step if endpoint is missing, or let it fail during execution\n",
        "           # For compilation, we might need to pass a dummy value if the parameter is mandatory\n",
        "           # However, the component logic handles the check, so compilation might proceed.\n",
        "\n",
        "       # Use KFP v1 compiler explicitly\n",
        "       v1_compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(\n",
        "           pipeline_func=expert_training_pipeline_v1deploy_apireg,\n",
        "           package_path=pipeline_filename\n",
        "       )\n",
        "       logger.info(f\"Pipeline compiled successfully to {pipeline_filename}\")\n",
        "    except Exception as compile_e:\n",
        "       logger.critical(\"Pipeline compilation failed!\", exc_info=True)\n",
        "       pipeline_filename = None\n",
        "else:\n",
        "    logger.critical(\"MLOps configuration failed to load. Skipping pipeline compilation.\")\n",
        "\n",
        "\n",
        "# --- Conceptual Class for Triggering/Monitoring Pipelines via MIZ OKI Events ---\n",
        "class TrainingPipelineOrchestrator:\n",
        "    \"\"\"\n",
        "    Conceptual service that listens to Pub/Sub triggers (MIZ OKI format)\n",
        "    and launches/monitors Vertex AI Pipeline Jobs.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: EnhancedConfig, pipeline_client: Optional[Any] = None, pubsub_client: Optional[Any] = None):\n",
        "        # In a real service, pipeline_client would be Vertex AI Pipelines client\n",
        "        # pubsub_client would be Pub/Sub client\n",
        "        self.config = config\n",
        "        self.pipeline_client = pipeline_client # e.g., aiplatform.PipelineJob\n",
        "        self.pubsub_client = pubsub_client # e.g., pubsub_v1.PublisherClient\n",
        "        self.logger = logging.getLogger(\"MIZ-OKI.TrainingPipelineOrchestrator\")\n",
        "        if not self.pipeline_client: self.logger.warning(\"Vertex AI Pipeline client not provided.\")\n",
        "        if not self.pubsub_client: self.logger.warning(\"Pub/Sub client not provided.\")\n",
        "\n",
        "    async def handle_trigger_event(self, miz_oki_event: Dict) -> Dict:\n",
        "        \"\"\"Handles a MIZ OKI event requesting a pipeline launch.\"\"\"\n",
        "        trace_id = miz_oki_event.get(\"trace_id\", f\"mlops_trigger_{uuid.uuid4().hex[:8]}\")\n",
        "        response_status = \"failed\"; response_payload = None; errors = []\n",
        "        try:\n",
        "            if not self.pipeline_client: raise RuntimeError(\"Pipeline client unavailable.\")\n",
        "            event_payload = miz_oki_event.get(\"payload\", {})\n",
        "            pipeline_name = event_payload.get(\"pipeline_name\")\n",
        "            pipeline_params = event_payload.get(\"parameters\", {})\n",
        "            if not pipeline_name or not pipeline_params:\n",
        "                raise ValueError(\"Missing 'pipeline_name' or 'parameters' in trigger event payload.\")\n",
        "\n",
        "            # --- TODO: Map pipeline_name to the compiled template path (e.g., from GCS) ---\n",
        "            template_path = f\"{PIPELINE_ROOT}/{pipeline_name}.json\" # Example mapping\n",
        "            if not template_path: raise ValueError(f\"Cannot find template for pipeline: {pipeline_name}\")\n",
        "            # --- End TODO ---\n",
        "\n",
        "            job_id = f\"{pipeline_name}-run-{datetime.datetime.now(datetime.timezone.utc).strftime('%Y%m%d%H%M%S')}\"\n",
        "            logger.info(f\"Submitting Vertex AI Pipeline job '{job_id}' from template '{template_path}' with params: {pipeline_params}\")\n",
        "\n",
        "            # Use Vertex AI SDK to submit the job (this part is synchronous in current SDK)\n",
        "            # Needs to be run in a thread for async context\n",
        "            def _submit_job():\n",
        "                job = aiplatform.PipelineJob(\n",
        "                    display_name=job_id,\n",
        "                    template_path=template_path,\n",
        "                    pipeline_root=PIPELINE_ROOT,\n",
        "                    parameter_values=pipeline_params,\n",
        "                    project=self.config.gcp.project_id,\n",
        "                    location=self.config.gcp.region,\n",
        "                    # enable_caching=True, # Optional\n",
        "                )\n",
        "                job.submit() # Submits the job\n",
        "                return job.resource_name # Return the job resource name\n",
        "\n",
        "            job_resource_name = await asyncio.to_thread(_submit_job)\n",
        "            logger.info(f\"Successfully submitted Vertex AI Pipeline job: {job_resource_name}\")\n",
        "            response_status = \"submitted\"; response_payload = {\"job_resource_name\": job_resource_name, \"job_id_prefix\": job_id}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Pipeline launch error from trigger event: {e}\", exc_info=True)\n",
        "            errors.append({\"code\": \"PIPELINE_SUBMIT_ERROR\", \"message\": str(e)})\n",
        "            response_status = \"error\"\n",
        "\n",
        "        # Return a MIZ OKI response (optional, depends if caller needs confirmation)\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": miz_oki_event.get(\"request_id\"), \"trace_id\": trace_id,\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"TrainingPipelineOrchestrator\",\n",
        "            \"status\": response_status, \"payload\": response_payload, \"error_details\": errors\n",
        "        }\n",
        "\n",
        "    async def handle_completion_event(self, miz_oki_event: Dict) -> Dict:\n",
        "        \"\"\"Handles Pub/Sub event for completed training, triggers post-processing/evaluation.\"\"\"\n",
        "        # This would typically be triggered by the pipeline itself publishing to a topic upon completion.\n",
        "        # The event payload should contain job details (resource name, status, outputs).\n",
        "        trace_id = miz_oki_event.get(\"trace_id\", f\"mlops_complete_{uuid.uuid4().hex[:8]}\")\n",
        "        response_status = \"failed\"; response_payload = None; errors = []\n",
        "        try:\n",
        "            job_details = miz_oki_event.get(\"payload\", {})\n",
        "            job_resource_name = job_details.get(\"job_resource_name\")\n",
        "            job_status = job_details.get(\"status\") # e.g., 'PIPELINE_STATE_SUCCEEDED', 'PIPELINE_STATE_FAILED'\n",
        "            if not job_resource_name or not job_status:\n",
        "                raise ValueError(\"Missing job details in completion event.\")\n",
        "\n",
        "            logger.info(f\"Handling pipeline completion event for job: {job_resource_name}, Status: {job_status}\")\n",
        "\n",
        "            if job_status == 'PIPELINE_STATE_SUCCEEDED':\n",
        "                # --- TODO: Trigger next steps ---\n",
        "                # - Call MoE Registry update (if not done in pipeline)\n",
        "                # - Trigger evaluation workflows\n",
        "                # - Update KG with model metadata\n",
        "                # Example: Publish event for LI Tool\n",
        "                # if self.pubsub_client:\n",
        "                #     li_event = {...}\n",
        "                #     await self.pubsub_client.publish(...)\n",
        "                # --- End TODO ---\n",
        "                response_status = \"success\"; response_payload = {\"job_resource_name\": job_resource_name, \"action\": \"Post-processing triggered (placeholder)\"}\n",
        "            else:\n",
        "                # Handle failed pipeline\n",
        "                logger.error(f\"Pipeline job {job_resource_name} failed. Status: {job_status}. Details: {job_details.get('error')}\")\n",
        "                response_status = \"failed\"; errors.append({\"code\": \"PIPELINE_FAILED\", \"message\": f\"Job {job_resource_name} failed.\", \"details\": job_details.get('error')})\n",
        "                # --- TODO: Trigger alerting or remediation ---\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Pipeline completion handler error: {e}\", exc_info=True)\n",
        "            errors.append({\"code\": \"HANDLER_ERROR\", \"message\": str(e)})\n",
        "            response_status = \"error\"\n",
        "\n",
        "        return {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": miz_oki_event.get(\"request_id\"), \"trace_id\": trace_id,\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"TrainingPipelineOrchestrator\",\n",
        "            \"status\": response_status, \"payload\": response_payload, \"error_details\": errors\n",
        "        }\n",
        "\n",
        "\n",
        "# --- Notes on Execution ---\n",
        "logger.info(\"--- MLOps Pipeline Execution Notes (Reworked) ---\")\n",
        "if MLOPS_CONFIG_LOADED and pipeline_filename:\n",
        "    logger.info(f\"1. Upload compiled pipeline: '{pipeline_filename}' to GCS bucket '{BUCKET_NAME}' (e.g., under {PIPELINE_ROOT}).\")\n",
        "    logger.info(f\"2. Setup Cloud Function/Run service triggered by Pub/Sub topic: '{MLOPS_TRIGGER_TOPIC}'.\")\n",
        "    logger.info(\"3. The trigger service should instantiate TrainingPipelineOrchestrator and call handle_trigger_event.\")\n",
        "    logger.info(f\"4. Ensure MoE Registry API ('{MOE_REGISTRY_ENDPOINT or 'endpoint_not_set'}') is deployed & pipeline SA has 'roles/run.invoker'.\")\n",
        "    logger.info(\"5. Implement REAL logic in pipeline components (prepare_data_op, train_expert_model_op, evaluate_model_op).\")\n",
        "    logger.info(\"6. Define SEPARATE KFP pipelines for LLM Fine-Tuning/Distillation (likely using CustomJobOp or specific GCPC components).\")\n",
        "else:\n",
        "     logger.error(\"MLOps pipeline configuration incomplete or compilation failed. Deployment steps cannot be determined.\")\n",
        "\n",
        "print(f\"\\n--- MIZ 3.0 MLOps Pipeline Definition Compiled (Cell 17 - Reworked) ---\")\n",
        "if pipeline_filename: print(f\"Pipeline definition saved to: {pipeline_filename}\")\n",
        "else: print(\"Pipeline compilation FAILED. Check logs.\")\n",
        "print(\"-----------------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ucUBGTbC4Zoq",
        "outputId": "264ae39e-3cd5-4d0b-af89-a1ca370d6d30"
      },
      "id": "ucUBGTbC4Zoq",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "non-default argument follows default argument (<ipython-input-20-287087a799ae>, line 178)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-287087a799ae>\"\u001b[0;36m, line \u001b[0;32m178\u001b[0m\n\u001b[0;31m    source_uri_or_query: str, # GCS path (e.g., gs://...) or BQ table (project.dataset.table)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: Foundation Model Client (New Implementation)\n",
        "# Status: Implements the unified client logic as a deployable Tool/Service.\n",
        "#         Handles multiple providers (Vertex, OpenAI, Anthropic), async calls,\n",
        "#         rate limiting, retries, caching, cost estimation, MIZ OKI I/O.\n",
        "\n",
        "import logging\n",
        "import asyncio\n",
        "import json\n",
        "import time\n",
        "import uuid\n",
        "import os\n",
        "from typing import Dict, Any, Optional, List, Union, Tuple\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass, field, asdict\n",
        "import aiohttp # For async HTTP calls (e.g., Anthropic REST)\n",
        "from cachetools import TTLCache # For response caching\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError # For robust API calls\n",
        "\n",
        "# --- Assume Real Config is Available ---\n",
        "try:\n",
        "    # Use CONFIG_OBJ loaded in Cell 1\n",
        "    if 'CONFIG_OBJ' not in globals() or not CONFIG_OBJ: raise NameError(\"CONFIG_OBJ not found or is None\")\n",
        "    if not isinstance(CONFIG_OBJ, EnhancedConfig): raise NameError(\"CONFIG_OBJ is not an EnhancedConfig instance\")\n",
        "    _config_obj = CONFIG_OBJ\n",
        "    _real_dependencies = True\n",
        "    logger.debug(\"Using real CONFIG_OBJ in Cell 18 (New Implementation).\")\n",
        "\n",
        "    # --- Vertex AI SDK ---\n",
        "    try:\n",
        "        import vertexai\n",
        "        from vertexai.generative_models import GenerativeModel, Part, FinishReason, Candidate, GenerationResponse\n",
        "        from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput # Use specific embedding model class\n",
        "        # Ensure initialized (from Cell 1)\n",
        "        if not _config_obj.gcp.project_id or not _config_obj.gcp.region: raise Exception(\"GCP Project/Region missing\")\n",
        "        # Check if SDK is initialized (simple check)\n",
        "        if not getattr(vertexai.preview.initializer.global_config, 'project', None):\n",
        "             vertexai.init(project=_config_obj.gcp.project_id, location=_config_obj.gcp.region)\n",
        "             logger.info(\"Vertex AI SDK initialized in FM Client.\")\n",
        "        VERTEX_SDK_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        logger.warning(\"Vertex AI SDK not found. Vertex models unavailable.\")\n",
        "        VERTEX_SDK_AVAILABLE = False\n",
        "        # Define dummy classes for type hinting if SDK missing\n",
        "        class GenerativeModel: pass; class TextEmbeddingModel: pass; class Part: pass; class FinishReason: pass; class Candidate: pass; class GenerationResponse: pass; class TextEmbeddingInput: pass\n",
        "\n",
        "    # --- OpenAI SDK ---\n",
        "    try:\n",
        "        from openai import AsyncOpenAI, RateLimitError as OpenAIRateLimitError, APIError as OpenAIAPIError\n",
        "        OPENAI_SDK_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        logger.warning(\"OpenAI SDK not found.\")\n",
        "        OPENAI_SDK_AVAILABLE = False\n",
        "        class AsyncOpenAI: pass; class OpenAIRateLimitError(Exception): pass; class OpenAIAPIError(Exception): pass\n",
        "\n",
        "    # --- Anthropic SDK ---\n",
        "    # Using REST via aiohttp for simplicity, but check if SDK is installed for completeness\n",
        "    try:\n",
        "        import anthropic\n",
        "        # Check for specific exceptions if using SDK directly\n",
        "        # from anthropic import RateLimitError as AnthropicRateLimitError, APIError as AnthropicAPIError\n",
        "        ANTHROPIC_SDK_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        logger.warning(\"Anthropic SDK not found. Using REST API via aiohttp.\")\n",
        "        ANTHROPIC_SDK_AVAILABLE = False\n",
        "        class anthropic: pass # Dummy class\n",
        "\n",
        "except NameError as e:\n",
        "    logger.warning(f\"Dependency Error in Cell 18 ({e}). Using Mocks/Placeholders.\")\n",
        "    _real_dependencies = False\n",
        "    # --- Mock Config ---\n",
        "    from dataclasses import dataclass, field\n",
        "    @dataclass class MockFmDefaults: vertex: str = \"gemini-1.5-flash-001\"; llama4_scout: str = \"mock-scout\"; llama4_maverick: str = \"mock-mav\"; llama4_embedding_model: str = \"mock-emb\"; openai: str = \"mock-gpt\"; anthropic: str = \"mock-claude\"\n",
        "    @dataclass class MockFmPricing: prompt: float = 0.5; completion: float = 1.5\n",
        "    @dataclass class MockFmConfig: keys: Dict = field(default_factory=lambda: {\"vertex\": \"auth\", \"openai\": \"sk-mock\", \"anthropic\": \"ak-mock\"}); defaults: MockFmDefaults = field(default_factory=MockFmDefaults); pricing: Dict = field(default_factory=lambda: {\"vertex\": {\"gemini-1.5-flash-001\": MockFmPricing()}, \"openai\": {\"mock-gpt\": MockFmPricing()}, \"anthropic\": {\"mock-claude\": MockFmPricing()}})\n",
        "    @dataclass class MockConfig: foundation_models: MockFmConfig = field(default_factory=MockFmConfig); miz_oki_schema_version: str = \"3.0\"; def get_model_info(self, alias): return {\"provider\": \"mock\", \"model_id\": alias, \"pricing\": {\"prompt\": 0.1, \"completion\": 0.2}}\n",
        "    _config_obj = MockConfig()\n",
        "    VERTEX_SDK_AVAILABLE = False; OPENAI_SDK_AVAILABLE = False; ANTHROPIC_SDK_AVAILABLE = False\n",
        "    # --- End Mock Setup ---\n",
        "\n",
        "logger = logging.getLogger('MIZ-OKI.FoundationModelClient')\n",
        "\n",
        "# --- Helper Classes ---\n",
        "class RateLimiter:\n",
        "    \"\"\"Simple token bucket rate limiter for async operations.\"\"\"\n",
        "    def __init__(self, rate: float, capacity: float):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rate (float): Tokens added per second.\n",
        "            capacity (float): Maximum tokens in the bucket.\n",
        "        \"\"\"\n",
        "        if rate <= 0 or capacity <= 0:\n",
        "            raise ValueError(\"Rate and capacity must be positive\")\n",
        "        self.rate = rate\n",
        "        self.capacity = capacity\n",
        "        self._tokens = capacity\n",
        "        self._last_update = time.monotonic()\n",
        "        self._lock = asyncio.Lock()\n",
        "\n",
        "    async def wait(self):\n",
        "        \"\"\"Waits if necessary to ensure rate limit is not exceeded.\"\"\"\n",
        "        async with self._lock:\n",
        "            now = time.monotonic()\n",
        "            elapsed = now - self._last_update\n",
        "            self._last_update = now\n",
        "            # Add tokens earned during elapsed time\n",
        "            self._tokens = min(self.capacity, self._tokens + elapsed * self.rate)\n",
        "\n",
        "            if self._tokens < 1:\n",
        "                # Calculate wait time needed to get 1 token\n",
        "                wait_time = (1 - self._tokens) / self.rate\n",
        "                logger.debug(f\"Rate limiter: Waiting for {wait_time:.3f} seconds...\")\n",
        "                await asyncio.sleep(wait_time)\n",
        "                # Update tokens after waiting\n",
        "                self._tokens += wait_time * self.rate\n",
        "                self._last_update = time.monotonic() # Update last_update after sleep\n",
        "\n",
        "            # Consume one token\n",
        "            self._tokens -= 1\n",
        "            # logger.debug(f\"Rate limiter: Token consumed. Remaining: {self._tokens:.2f}\")\n",
        "\n",
        "# Define common retryable exceptions for API calls\n",
        "# Add provider-specific exceptions if needed and available\n",
        "RETRYABLE_HTTP_STATUS = {429, 500, 502, 503, 504} # Too Many Requests, Server Errors\n",
        "RETRYABLE_EXCEPTIONS = (\n",
        "    aiohttp.ClientConnectionError, # Network issues\n",
        "    aiohttp.ClientPayloadError,    # Issues with response payload\n",
        "    asyncio.TimeoutError,          # Request timeout\n",
        "    # OpenAI specific (if SDK used)\n",
        "    OpenAIRateLimitError if OPENAI_SDK_AVAILABLE else ConnectionError, # Placeholder if SDK missing\n",
        "    OpenAIAPIError if OPENAI_SDK_AVAILABLE else ConnectionError, # General OpenAI API errors (some might be retryable)\n",
        "    # Anthropic specific (if SDK used) - Add specific exceptions here\n",
        "    # Vertex AI specific - SDK might raise google.api_core.exceptions.RetryError or ServiceUnavailable\n",
        "    gcp_exceptions.RetryError if GCP_SDK_AVAILABLE else ConnectionError,\n",
        "    gcp_exceptions.ServiceUnavailable if GCP_SDK_AVAILABLE else ConnectionError,\n",
        ")\n",
        "\n",
        "def is_retryable_exception(exception: BaseException) -> bool:\n",
        "    \"\"\"Checks if an exception is retryable.\"\"\"\n",
        "    if isinstance(exception, aiohttp.ClientResponseError):\n",
        "        return exception.status in RETRYABLE_HTTP_STATUS\n",
        "    return isinstance(exception, RETRYABLE_EXCEPTIONS)\n",
        "\n",
        "# --- Foundation Model Client Implementation ---\n",
        "class FoundationModelClient:\n",
        "    \"\"\"Unified client for interacting with various Foundation Models via API. Deployed as a service.\"\"\"\n",
        "    def __init__(self, config: EnhancedConfig):\n",
        "        if not config: raise InitializationError(\"Config required for FoundationModelClient.\")\n",
        "        self.config = config\n",
        "        # Cache for storing responses (key: hash of request, value: response payload)\n",
        "        self.cache = TTLCache(maxsize=500, ttl=300) # 5 min TTL cache\n",
        "        self.logger = logging.getLogger('MIZ-OKI.FoundationModelClient')\n",
        "        # Rate limiters per provider (adjust rates based on actual limits/tiers)\n",
        "        self.rate_limiters = {\n",
        "            'vertex': RateLimiter(rate=100/60, capacity=100), # Example: 100 req/min\n",
        "            'openai': RateLimiter(rate=60/60, capacity=60),   # Example: 60 req/min\n",
        "            'anthropic': RateLimiter(rate=5, capacity=10),    # Example: 5 req/sec\n",
        "            'mock': RateLimiter(rate=100, capacity=100),      # High rate for mocks\n",
        "        }\n",
        "        self.clients: Dict[str, Any] = {} # Stores initialized SDK clients or config\n",
        "        self.session: Optional[aiohttp.ClientSession] = None\n",
        "        self.initialized = False\n",
        "        self.metrics = defaultdict(lambda: defaultdict(int)) # Track calls, errors, tokens per model\n",
        "\n",
        "    async def initialize(self):\n",
        "        \"\"\"Initialize HTTP session and SDK clients asynchronously.\"\"\"\n",
        "        if self.initialized: return\n",
        "        self.logger.info(\"Initializing FoundationModelClient...\")\n",
        "        self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=120)) # 120s total timeout\n",
        "\n",
        "        # Initialize Vertex AI (if SDK available and configured)\n",
        "        if VERTEX_SDK_AVAILABLE and 'vertex' in self.config.foundation_models.keys:\n",
        "            try:\n",
        "                # SDK should be initialized in Cell 1, just store references or perform checks\n",
        "                # Store model instances directly? Or just check availability? Storing for now.\n",
        "                default_vertex_model = self.config.foundation_models.defaults.vertex\n",
        "                default_embedding_model = self.config.foundation_models.defaults.llama4_embedding_model\n",
        "                self.clients['vertex_genai'] = GenerativeModel(default_vertex_model) # Store default model instance\n",
        "                self.clients['vertex_embedding'] = TextEmbeddingModel.from_pretrained(default_embedding_model)\n",
        "                logger.info(f\"Vertex AI clients initialized (GenAI: {default_vertex_model}, Embedding: {default_embedding_model}).\")\n",
        "            except Exception as e: logger.error(f\"Vertex AI client initialization failed: {e}\", exc_info=True)\n",
        "\n",
        "        # Initialize OpenAI\n",
        "        if OPENAI_SDK_AVAILABLE and 'openai' in self.config.foundation_models.keys:\n",
        "            try:\n",
        "                self.clients['openai'] = AsyncOpenAI(api_key=self.config.foundation_models.keys['openai'], max_retries=0) # Handle retries manually with tenacity\n",
        "                logger.info(\"OpenAI client initialized.\")\n",
        "            except Exception as e: logger.error(f\"OpenAI client init failed: {e}\", exc_info=True)\n",
        "\n",
        "        # Store Anthropic config (using REST via aiohttp)\n",
        "        if 'anthropic' in self.config.foundation_models.keys:\n",
        "            self.clients['anthropic'] = {\"api_key\": self.config.foundation_models.keys['anthropic'], \"base_url\": \"https://api.anthropic.com/v1\"}\n",
        "            logger.info(\"Anthropic client config stored (using REST).\")\n",
        "\n",
        "        # Add mock client if needed for testing unresolved models\n",
        "        self.clients['mock'] = {\"status\": \"mock\"}\n",
        "\n",
        "        self.initialized = True\n",
        "        self.logger.info(\"FoundationModelClient initialized.\")\n",
        "\n",
        "    async def cleanup(self):\n",
        "        \"\"\"Close aiohttp session and potentially other client resources.\"\"\"\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "            self.session = None\n",
        "        # Close SDK clients if they have explicit close methods (OpenAI doesn't typically)\n",
        "        self.clients = {}\n",
        "        self.cache.clear()\n",
        "        self.initialized = False\n",
        "        self.logger.info(\"FoundationModelClient cleaned up.\")\n",
        "\n",
        "    def _get_client_and_model(self, model_alias_or_id: str) -> Tuple[Optional[Any], Optional[str], Optional[str]]:\n",
        "        \"\"\"Resolves alias, gets provider client/config and actual model ID.\"\"\"\n",
        "        model_info = self.config.get_model_info(model_alias_or_id)\n",
        "        if not model_info:\n",
        "            self.logger.error(f\"Could not resolve model info for alias/ID: '{model_alias_or_id}'\")\n",
        "            return None, None, None\n",
        "\n",
        "        provider = model_info[\"provider\"]\n",
        "        model_id = model_info[\"model_id\"] # The actual ID used by the provider\n",
        "\n",
        "        # Get the client or config associated with the provider\n",
        "        client_or_config = self.clients.get(provider)\n",
        "\n",
        "        # Special handling for Vertex AI which might use different client objects\n",
        "        if provider == 'vertex':\n",
        "            # Determine if it's an embedding or generative task based on model ID (simple check)\n",
        "            if 'embedding' in model_id.lower(): client_or_config = self.clients.get('vertex_embedding')\n",
        "            else: client_or_config = self.clients.get('vertex_genai')\n",
        "\n",
        "        if client_or_config is None:\n",
        "             self.logger.error(f\"Client/Config for provider '{provider}' not initialized or key missing.\")\n",
        "             return None, None, provider # Return provider even if client is missing\n",
        "\n",
        "        return client_or_config, model_id, provider\n",
        "\n",
        "    def _estimate_cost(self, provider: str, model_id: str, prompt_tokens: int, completion_tokens: int) -> Optional[float]:\n",
        "        \"\"\"Estimates cost based on token counts and config pricing.\"\"\"\n",
        "        try:\n",
        "            pricing_info = self.config.foundation_models.pricing.get(provider, {}).get(model_id)\n",
        "            if pricing_info:\n",
        "                cost = (prompt_tokens / 1_000_000 * pricing_info.prompt) + (completion_tokens / 1_000_000 * pricing_info.completion)\n",
        "                return round(cost, 6) # Return cost rounded to 6 decimal places\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Cost estimation failed for {provider}/{model_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    def _update_metrics(self, model_alias_or_id: str, status: str, duration: float, p_tokens: int = 0, c_tokens: int = 0):\n",
        "        \"\"\"Updates call metrics.\"\"\"\n",
        "        self.metrics[model_alias_or_id][status] += 1\n",
        "        self.metrics[model_alias_or_id]['total_duration'] = self.metrics[model_alias_or_id].get('total_duration', 0) + duration\n",
        "        self.metrics[model_alias_or_id]['prompt_tokens'] = self.metrics[model_alias_or_id].get('prompt_tokens', 0) + p_tokens\n",
        "        self.metrics[model_alias_or_id]['completion_tokens'] = self.metrics[model_alias_or_id].get('completion_tokens', 0) + c_tokens\n",
        "\n",
        "    def _handle_error(self, model_alias_or_id: str, error: Exception):\n",
        "        \"\"\"Handles errors during API calls.\"\"\"\n",
        "        self._update_metrics(model_alias_or_id, 'error', 0) # Record error count\n",
        "        # Log specific error types if needed\n",
        "        logger.error(f\"API call failed for {model_alias_or_id}: {type(error).__name__} - {error}\")\n",
        "\n",
        "    def _generate_cache_key(self, model_alias: str, **kwargs) -> str:\n",
        "        \"\"\"Generates a cache key based on model and relevant arguments.\"\"\"\n",
        "        key_dict = {\"model\": model_alias, **kwargs}\n",
        "        # Convert complex objects like lists/dicts to sorted tuples/items for consistent hashing\n",
        "        def make_hashable(o):\n",
        "            if isinstance(o, dict): return tuple(sorted((k, make_hashable(v)) for k, v in o.items()))\n",
        "            if isinstance(o, (list, tuple)): return tuple(make_hashable(e) for e in o)\n",
        "            return o\n",
        "        try:\n",
        "            hashable_key = make_hashable(key_dict)\n",
        "            return str(hash(hashable_key))\n",
        "        except Exception as e:\n",
        "             logger.warning(f\"Could not generate hashable cache key, using simple string: {e}\")\n",
        "             # Fallback to less reliable string representation\n",
        "             return json.dumps(key_dict, sort_keys=True, default=str)\n",
        "\n",
        "\n",
        "    # --- Provider Specific Methods ---\n",
        "\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10), retry=retry_if_exception_type(is_retryable_exception))\n",
        "    async def _vertex_generate(self, client_instance: GenerativeModel, model_id: str, prompt: str, max_tokens: int, temperature: float, **kwargs) -> Dict:\n",
        "        \"\"\"Generates text using Vertex AI GenerativeModel.\"\"\"\n",
        "        if not VERTEX_SDK_AVAILABLE: raise RuntimeError(\"Vertex AI SDK not available.\")\n",
        "        try:\n",
        "            # Ensure we're using the correct model instance if client_instance is just the default\n",
        "            if client_instance.model_name != model_id:\n",
        "                model_instance = GenerativeModel(model_id)\n",
        "                logger.debug(f\"Using specific Vertex model instance: {model_id}\")\n",
        "            else:\n",
        "                model_instance = client_instance # Use the default instance passed\n",
        "\n",
        "            generation_config = {\n",
        "                \"max_output_tokens\": max_tokens,\n",
        "                \"temperature\": temperature,\n",
        "                \"top_p\": kwargs.get(\"top_p\", 0.95)\n",
        "            }\n",
        "            # Vertex SDK generate_content is sync, run in thread\n",
        "            # Use generate_content_async if available and preferred\n",
        "            if hasattr(model_instance, 'generate_content_async'):\n",
        "                 response: GenerationResponse = await model_instance.generate_content_async(\n",
        "                     prompt, generation_config=generation_config\n",
        "                 )\n",
        "            else:\n",
        "                 response: GenerationResponse = await asyncio.to_thread(\n",
        "                     model_instance.generate_content,\n",
        "                     prompt, generation_config=generation_config\n",
        "                 )\n",
        "\n",
        "            # Extract text, handling potential lack of content or errors\n",
        "            text_response = \"\"\n",
        "            if response.candidates:\n",
        "                first_candidate = response.candidates[0]\n",
        "                if first_candidate.content and first_candidate.content.parts:\n",
        "                    text_response = \"\".join(part.text for part in first_candidate.content.parts if hasattr(part, 'text'))\n",
        "                # Check finish reason for safety issues\n",
        "                if first_candidate.finish_reason not in [FinishReason.STOP, FinishReason.MAX_TOKENS, None]: # Allow None for safety\n",
        "                     raise RuntimeError(f\"Vertex AI generation stopped due to safety or other reason: {first_candidate.finish_reason.name}\")\n",
        "            else:\n",
        "                 # Handle cases where response might be blocked or empty\n",
        "                 logger.warning(f\"Vertex AI response for {model_id} has no candidates. Response: {response}\")\n",
        "                 # Check for prompt feedback if available\n",
        "                 if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                      raise RuntimeError(f\"Vertex AI prompt blocked: {response.prompt_feedback.block_reason.name}\")\n",
        "\n",
        "\n",
        "            # Token count might be in usage_metadata\n",
        "            prompt_tokens = response.usage_metadata.prompt_token_count if hasattr(response, 'usage_metadata') else 0\n",
        "            completion_tokens = response.usage_metadata.candidates_token_count if hasattr(response, 'usage_metadata') else 0\n",
        "            return {\"text\": text_response, \"prompt_tokens\": prompt_tokens, \"completion_tokens\": completion_tokens}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Vertex AI generation error for {model_id}: {e}\", exc_info=True)\n",
        "            raise # Re-raise for tenacity retry\n",
        "\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10), retry=retry_if_exception_type(is_retryable_exception))\n",
        "    async def _vertex_embed(self, client_instance: TextEmbeddingModel, model_id: str, data: Union[str, List[str]], **kwargs) -> Dict:\n",
        "        \"\"\"Generates embeddings using Vertex AI TextEmbeddingModel.\"\"\"\n",
        "        if not VERTEX_SDK_AVAILABLE: raise RuntimeError(\"Vertex AI SDK not available.\")\n",
        "        try:\n",
        "            # Ensure we're using the correct model instance if client_instance is just the default\n",
        "            # Note: TextEmbeddingModel.from_pretrained might be sync, handle if needed\n",
        "            if client_instance._model_id != model_id: # Accessing private attr might be fragile\n",
        "                embedding_client = TextEmbeddingModel.from_pretrained(model_id)\n",
        "                logger.debug(f\"Using specific Vertex embedding instance: {model_id}\")\n",
        "            else:\n",
        "                embedding_client = client_instance # Use the default instance passed\n",
        "\n",
        "            # The SDK method handles batching automatically\n",
        "            instances = [data] if isinstance(data, str) else data\n",
        "            # Prepare input objects\n",
        "            embedding_inputs = [TextEmbeddingInput(text=text) for text in instances]\n",
        "\n",
        "            # get_embeddings is sync, run in thread\n",
        "            embeddings_response = await asyncio.to_thread(embedding_client.get_embeddings, embedding_inputs)\n",
        "\n",
        "            embeddings = [e.values for e in embeddings_response]\n",
        "            # Token count estimation for embeddings is tricky, often priced per 1k chars or per request\n",
        "            # Placeholder: estimate based on char count (crude)\n",
        "            total_chars = sum(len(text) for text in instances)\n",
        "            prompt_tokens = total_chars // 4 # Very rough estimate\n",
        "            return {\"embeddings\": embeddings, \"prompt_tokens\": prompt_tokens, \"completion_tokens\": 0}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Vertex AI embedding error for {model_id}: {e}\", exc_info=True)\n",
        "            raise # Re-raise for tenacity retry\n",
        "\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10), retry=retry_if_exception_type(is_retryable_exception))\n",
        "    async def _openai_generate(self, client: AsyncOpenAI, model_id: str, prompt: str, max_tokens: int, temperature: float, **kwargs) -> Dict:\n",
        "        \"\"\"Generates text using OpenAI API.\"\"\"\n",
        "        if not OPENAI_SDK_AVAILABLE: raise RuntimeError(\"OpenAI SDK not available.\")\n",
        "        try:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=kwargs.get(\"top_p\", 1.0)\n",
        "                # Add other OpenAI specific params if needed\n",
        "            )\n",
        "            text_response = response.choices[0].message.content if response.choices else \"\"\n",
        "            prompt_tokens = response.usage.prompt_tokens if response.usage else 0\n",
        "            completion_tokens = response.usage.completion_tokens if response.usage else 0\n",
        "            return {\"text\": text_response, \"prompt_tokens\": prompt_tokens, \"completion_tokens\": completion_tokens}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"OpenAI generation error for {model_id}: {e}\", exc_info=True)\n",
        "            raise # Re-raise for tenacity retry\n",
        "\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10), retry=retry_if_exception_type(is_retryable_exception))\n",
        "    async def _openai_embed(self, client: AsyncOpenAI, model_id: str, data: Union[str, List[str]], **kwargs) -> Dict:\n",
        "        \"\"\"Generates embeddings using OpenAI API.\"\"\"\n",
        "        if not OPENAI_SDK_AVAILABLE: raise RuntimeError(\"OpenAI SDK not available.\")\n",
        "        try:\n",
        "            # OpenAI API expects 'input' which can be str or list[str]\n",
        "            response = await client.embeddings.create(model=model_id, input=data)\n",
        "            embeddings = [item.embedding for item in response.data]\n",
        "            prompt_tokens = response.usage.prompt_tokens if response.usage else 0\n",
        "            # OpenAI embedding response format might vary, adjust parsing as needed\n",
        "            # If input was single string, return single embedding list\n",
        "            final_embeddings = embeddings[0] if isinstance(data, str) and len(embeddings) == 1 else embeddings\n",
        "            return {\"embeddings\": final_embeddings, \"prompt_tokens\": prompt_tokens, \"completion_tokens\": 0}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"OpenAI embedding error for {model_id}: {e}\", exc_info=True)\n",
        "            raise # Re-raise for tenacity retry\n",
        "\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10), retry=retry_if_exception_type(is_retryable_exception))\n",
        "    async def _anthropic_generate(self, client_config: Dict, model_id: str, prompt: str, max_tokens: int, temperature: float, **kwargs) -> Dict:\n",
        "        \"\"\"Generates text using Anthropic API (REST via aiohttp).\"\"\"\n",
        "        if not self.session: raise RuntimeError(\"Aiohttp session not initialized.\")\n",
        "        headers = {\n",
        "            \"x-api-key\": client_config[\"api_key\"],\n",
        "            \"anthropic-version\": \"2023-06-01\", # Use appropriate API version\n",
        "            \"content-type\": \"application/json\"\n",
        "        }\n",
        "        # Use messages format\n",
        "        payload = {\n",
        "            \"model\": model_id,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": kwargs.get(\"top_p\", 1.0)\n",
        "            # Add other Anthropic specific params if needed\n",
        "        }\n",
        "        url = f\"{client_config['base_url'].rstrip('/')}/messages\"\n",
        "        try:\n",
        "            async with self.session.post(url, headers=headers, json=payload) as response:\n",
        "                if response.status >= 400: # Check for HTTP errors\n",
        "                    error_text = await response.text()\n",
        "                    logger.error(f\"Anthropic API HTTP error {response.status}: {error_text}\")\n",
        "                    response.raise_for_status() # Raise ClientResponseError\n",
        "\n",
        "                result = await response.json()\n",
        "                # Extract text from content blocks\n",
        "                text_response = \"\".join([block.get(\"text\", \"\") for block in result.get(\"content\", []) if block.get(\"type\") == \"text\"])\n",
        "                # Token counts from usage field\n",
        "                prompt_tokens = result.get(\"usage\", {}).get(\"input_tokens\", 0)\n",
        "                completion_tokens = result.get(\"usage\", {}).get(\"output_tokens\", 0)\n",
        "                return {\"text\": text_response, \"prompt_tokens\": prompt_tokens, \"completion_tokens\": completion_tokens}\n",
        "        except aiohttp.ClientResponseError as http_err:\n",
        "            # Already logged above, just re-raise for tenacity\n",
        "            raise http_err\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Anthropic generation error for {model_id}: {e}\", exc_info=True)\n",
        "            raise # Re-raise for tenacity retry\n",
        "\n",
        "    # --- Unified Public Methods (Accepting/Returning MIZ OKI Payloads) ---\n",
        "\n",
        "    async def generate_text(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Generates text using the specified model. Expects/Returns MIZ OKI.\"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); model_alias = payload.get(\"model_alias\"); prompt = payload.get(\"prompt\"); max_tokens = payload.get(\"max_tokens\", 1024); temperature = payload.get(\"temperature\", 0.7); kwargs = payload.get(\"kwargs\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # --- MIZ OKI Response Setup ---\n",
        "        response = {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_id, \"trace_id\": trace_id,\n",
        "            \"workflow_execution_id\": input_data.get(\"workflow_execution_id\"), \"step_id\": input_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"FoundationModelClient\", \"target_component\": input_data.get(\"source_component\"),\n",
        "            \"status\": \"pending\", \"payload\": None, \"error_details\": None, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "        if not self.initialized: errors.append({\"code\": \"NOT_INITIALIZED\", \"message\": \"FM Client not initialized.\"})\n",
        "        if not model_alias or not prompt: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'payload.model_alias' and 'payload.prompt' required.\"})\n",
        "        if errors:\n",
        "            response[\"status\"] = \"bad_request\"; response[\"error_details\"] = errors\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        status = \"pending\"; response_payload = None; response_metadata = {}\n",
        "\n",
        "        # --- Caching ---\n",
        "        cache_key = self._generate_cache_key(model_alias=model_alias, prompt=prompt, max_tokens=max_tokens, temperature=temperature, **kwargs)\n",
        "        if cached := self.cache.get(cache_key):\n",
        "            response[\"status\"] = \"success\"; response[\"payload\"] = cached\n",
        "            response[\"metadata\"] = {\"cached\": True, \"processing_duration_ms\": (time.monotonic() - start_time) * 1000}\n",
        "            logger.debug(f\"Cache hit for text generation: {model_alias}\")\n",
        "            self._update_metrics(model_alias, 'cache_hit', 0)\n",
        "            return response\n",
        "        # --- End Caching ---\n",
        "\n",
        "        client, model_id, provider = self._get_client_and_model(model_alias)\n",
        "        if not provider or not model_id: errors.append({\"code\": \"MODEL_RESOLUTION_FAILED\", \"message\": f\"Cannot resolve model '{model_alias}'.\"})\n",
        "        # Allow provider='vertex' even if client is None initially, specific methods handle client selection\n",
        "        if not client and provider != 'vertex': errors.append({\"code\": \"CLIENT_UNAVAILABLE\", \"message\": f\"Client for provider '{provider}' unavailable.\"})\n",
        "        if errors:\n",
        "            response[\"status\"] = \"config_error\"; response[\"error_details\"] = errors\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        try:\n",
        "            await self.rate_limiters[provider].wait() # Wait for rate limit\n",
        "            provider_method = None\n",
        "            if provider == 'vertex': provider_method = self._vertex_generate\n",
        "            elif provider == 'openai': provider_method = self._openai_generate\n",
        "            elif provider == 'anthropic': provider_method = self._anthropic_generate\n",
        "            elif provider == 'mock': # Handle mock provider for testing\n",
        "                 provider_method = lambda c, m, p, mx, t, **kw: asyncio.sleep(0.01, result={\"text\": \"Mock response\", \"prompt_tokens\": 10, \"completion_tokens\": 5})\n",
        "            # Add other providers...\n",
        "\n",
        "            if not provider_method: raise NotImplementedError(f\"Generation not implemented for provider: {provider}\")\n",
        "\n",
        "            # Call the provider-specific method with retry logic handled by tenacity\n",
        "            result = await provider_method(client, model_id, prompt, max_tokens, temperature, **kwargs)\n",
        "\n",
        "            duration = time.monotonic() - start_time\n",
        "            cost = self._estimate_cost(provider, model_id, result[\"prompt_tokens\"], result[\"completion_tokens\"])\n",
        "            status = \"success\"\n",
        "            response_payload = {\"generated_text\": result[\"text\"]}\n",
        "            response_metadata = {\"provider\": provider, \"model_id\": model_id, \"duration_ms\": duration * 1000, \"prompt_tokens\": result[\"prompt_tokens\"], \"completion_tokens\": result[\"completion_tokens\"], \"estimated_cost_usd\": cost, \"cached\": False}\n",
        "            self.cache[cache_key] = response_payload # Cache successful result payload\n",
        "            self._update_metrics(model_alias, 'success', duration, result[\"prompt_tokens\"], result[\"completion_tokens\"])\n",
        "\n",
        "        except RetryError as retry_err: # Catch tenacity retry error\n",
        "             status = \"failed\"; errors.append({\"code\": \"API_RETRY_ERROR\", \"message\": f\"API call failed after multiple retries: {retry_err.last_attempt.exception()}\"}); self._handle_error(model_alias, retry_err.last_attempt.exception()); logger.error(f\"Error generating text with {model_alias} after retries: {retry_err.last_attempt.exception()}\", exc_info=True)\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"GENERATION_ERROR\", \"message\": str(e)}); self._handle_error(model_alias, e); logger.error(f\"Error generating text with {model_alias}: {e}\", exc_info=True)\n",
        "\n",
        "        response[\"status\"] = status\n",
        "        response[\"payload\"] = response_payload\n",
        "        response[\"metadata\"] = response_metadata\n",
        "        response[\"metadata\"][\"total_processing_duration_ms\"] = (time.monotonic() - start_time) * 1000 # Overwrite duration with total time\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        return response\n",
        "\n",
        "    async def generate_embedding(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Generates embeddings. Expects/Returns MIZ OKI.\"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); model_alias = payload.get(\"model_alias\"); data = payload.get(\"data\") # data can be str or List[str]\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # --- MIZ OKI Response Setup ---\n",
        "        response = {\n",
        "            \"miz_oki_version\": self.config.miz_oki_schema_version,\n",
        "            \"request_id\": request_id, \"trace_id\": trace_id,\n",
        "            \"workflow_execution_id\": input_data.get(\"workflow_execution_id\"), \"step_id\": input_data.get(\"step_id\"),\n",
        "            \"timestamp\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            \"source_component\": \"FoundationModelClient\", \"target_component\": input_data.get(\"source_component\"),\n",
        "            \"status\": \"pending\", \"payload\": None, \"error_details\": None, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "        if not self.initialized: errors.append({\"code\": \"NOT_INITIALIZED\", \"message\": \"FM Client not initialized.\"})\n",
        "        if data is None: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'payload.data' (str or List[str]) required.\"})\n",
        "        if errors:\n",
        "            response[\"status\"] = \"bad_request\"; response[\"error_details\"] = errors\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        model_alias = model_alias or self.config.foundation_models.defaults.llama4_embedding_model\n",
        "        status = \"pending\"; response_payload = None; response_metadata = {}\n",
        "\n",
        "        # --- Caching (Less effective for embeddings unless exact string match) ---\n",
        "        # cache_key = self._generate_cache_key(model_alias=model_alias, data=data)\n",
        "        # if cached := self.cache.get(cache_key): ...\n",
        "        # --- End Caching ---\n",
        "\n",
        "        client, model_id, provider = self._get_client_and_model(model_alias)\n",
        "        if not provider or not model_id: errors.append({\"code\": \"MODEL_RESOLUTION_FAILED\", \"message\": f\"Cannot resolve model '{model_alias}'.\"})\n",
        "        if not client and provider != 'vertex': errors.append({\"code\": \"CLIENT_UNAVAILABLE\", \"message\": f\"Client for provider '{provider}' unavailable.\"})\n",
        "        if errors:\n",
        "            response[\"status\"] = \"config_error\"; response[\"error_details\"] = errors\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        try:\n",
        "            await self.rate_limiters[provider].wait()\n",
        "            provider_method = None\n",
        "            if provider == 'vertex': provider_method = self._vertex_embed\n",
        "            elif provider == 'openai': provider_method = self._openai_embed\n",
        "            # Add other providers...\n",
        "            elif provider == 'mock': # Handle mock provider for testing\n",
        "                 provider_method = lambda c, m, d, **kw: asyncio.sleep(0.01, result={\"embeddings\": [[0.1]*10] if isinstance(d, list) else [0.1]*10, \"prompt_tokens\": 10, \"completion_tokens\": 0})\n",
        "\n",
        "            if not provider_method: raise NotImplementedError(f\"Embedding not implemented for provider: {provider}\")\n",
        "\n",
        "            # Call the provider-specific method with retry logic handled by tenacity\n",
        "            result = await provider_method(client, model_id, data)\n",
        "\n",
        "            duration = time.monotonic() - start_time\n",
        "            cost = self._estimate_cost(provider, model_id, result[\"prompt_tokens\"], result[\"completion_tokens\"])\n",
        "            status = \"success\"\n",
        "            # Return single embedding if input was single string, else list\n",
        "            response_payload = {\"embedding\": result[\"embeddings\"]} # Keep as list consistently\n",
        "            response_metadata = {\"provider\": provider, \"model_id\": model_id, \"duration_ms\": duration * 1000, \"prompt_tokens\": result[\"prompt_tokens\"], \"estimated_cost_usd\": cost, \"cached\": False}\n",
        "            # self.cache[cache_key] = response_payload # Cache successful result payload\n",
        "            self._update_metrics(model_alias, 'success', duration, result[\"prompt_tokens\"], result[\"completion_tokens\"])\n",
        "\n",
        "        except RetryError as retry_err: # Catch tenacity retry error\n",
        "             status = \"failed\"; errors.append({\"code\": \"API_RETRY_ERROR\", \"message\": f\"API call failed after multiple retries: {retry_err.last_attempt.exception()}\"}); self._handle_error(model_alias, retry_err.last_attempt.exception()); logger.error(f\"Error generating embedding with {model_alias} after retries: {retry_err.last_attempt.exception()}\", exc_info=True)\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"EMBEDDING_ERROR\", \"message\": str(e)}); self._handle_error(model_alias, e); logger.error(f\"Error generating embedding with {model_alias}: {e}\", exc_info=True)\n",
        "\n",
        "        response[\"status\"] = status\n",
        "        response[\"payload\"] = response_payload\n",
        "        response[\"metadata\"] = response_metadata\n",
        "        response[\"metadata\"][\"total_processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        return response\n",
        "\n",
        "    async def analyze(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Performs analysis (e.g., sentiment) using an appropriate model. Expects/Returns MIZ OKI.\"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); model_alias = payload.get(\"model_alias\"); text = payload.get(\"text\"); analysis_type = payload.get(\"analysis_type\", \"sentiment\") # Default to sentiment\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # --- MIZ OKI Response Setup ---\n",
        "        response = { # ... standard MIZ OKI response structure ...\n",
        "            \"source_component\": \"FoundationModelClient\", \"target_component\": input_data.get(\"source_component\"),\n",
        "            \"status\": \"pending\", \"payload\": None, \"error_details\": None, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "        if not self.initialized: errors.append({\"code\": \"NOT_INITIALIZED\", \"message\": \"FM Client not initialized.\"})\n",
        "        if not text: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'payload.text' required.\"})\n",
        "        # --- TODO: Add model alias resolution and client check similar to generate_text ---\n",
        "        model_alias = model_alias or self.config.foundation_models.defaults.feedback_analyzer_model # Example: use feedback model\n",
        "        client, model_id, provider = self._get_client_and_model(model_alias)\n",
        "        if not provider or not model_id or (not client and provider != 'vertex'): errors.append({\"code\": \"MODEL_UNAVAILABLE\", \"message\": f\"Model/Client for '{model_alias}' unavailable.\"})\n",
        "        # --- End TODO ---\n",
        "        if errors:\n",
        "            response[\"status\"] = \"bad_request\"; response[\"error_details\"] = errors\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        status = \"pending\"; response_payload = None; response_metadata = {}\n",
        "        try:\n",
        "            # --- TODO: Implement analysis logic using appropriate provider method ---\n",
        "            # This might involve constructing a specific prompt for generate_text\n",
        "            # or calling a dedicated analysis endpoint if available.\n",
        "            # Example using generate_text for sentiment:\n",
        "            prompt = f\"Analyze the sentiment of the following text (positive, negative, neutral):\\nText: {text}\\nSentiment:\"\n",
        "            gen_request = {\n",
        "                \"payload\": {\"prompt\": prompt, \"model_alias\": model_alias, \"max_tokens\": 10, \"temperature\": 0.1},\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"fm_analyze_{request_id}\"\n",
        "            }\n",
        "            gen_response = await self.generate_text(input_data=gen_request) # Call internal generate_text\n",
        "\n",
        "            if gen_response.get(\"status\") == \"success\":\n",
        "                analysis_result = gen_response.get(\"payload\", {}).get(\"generated_text\", \"\").strip().lower()\n",
        "                status = \"success\"\n",
        "                response_payload = {\"analysis_type\": analysis_type, \"result\": analysis_result} # Simple result\n",
        "                response_metadata = gen_response.get(\"metadata\", {}) # Inherit metadata\n",
        "            else:\n",
        "                status = \"failed\"; errors = gen_response.get(\"error_details\")\n",
        "            # --- End TODO ---\n",
        "\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"ANALYSIS_ERROR\", \"message\": str(e)}); logger.error(f\"Error during analysis with {model_alias}: {e}\", exc_info=True)\n",
        "\n",
        "        response[\"status\"] = status\n",
        "        response[\"payload\"] = response_payload\n",
        "        response[\"metadata\"] = response_metadata\n",
        "        response[\"metadata\"][\"total_processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        return response\n",
        "\n",
        "    async def extract_kg_data_from_content(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Extracts KG entities/relationships using an LLM. Expects/Returns MIZ OKI.\"\"\"\n",
        "        start_time = time.monotonic(); errors = []\n",
        "        # Parse MIZ OKI input\n",
        "        payload = input_data.get(\"payload\", {}); model_alias = payload.get(\"model_alias\"); content = payload.get(\"content\"); data_type = payload.get(\"data_type\"); context = payload.get(\"context\", {})\n",
        "        trace_id = input_data.get(\"trace_id\"); request_id = input_data.get(\"request_id\")\n",
        "\n",
        "        # --- MIZ OKI Response Setup ---\n",
        "        response = { # ... standard MIZ OKI response structure ...\n",
        "            \"source_component\": \"FoundationModelClient\", \"target_component\": input_data.get(\"source_component\"),\n",
        "            \"status\": \"pending\", \"payload\": None, \"error_details\": None, \"metadata\": {}\n",
        "        }\n",
        "\n",
        "        if not self.initialized: errors.append({\"code\": \"NOT_INITIALIZED\", \"message\": \"FM Client not initialized.\"})\n",
        "        if not content: errors.append({\"code\": \"MISSING_DATA\", \"message\": \"'payload.content' required.\"})\n",
        "        # --- TODO: Add model alias resolution and client check ---\n",
        "        model_alias = model_alias or self.config.foundation_models.defaults.llama4_maverick # Use powerful model\n",
        "        client, model_id, provider = self._get_client_and_model(model_alias)\n",
        "        if not provider or not model_id or (not client and provider != 'vertex'): errors.append({\"code\": \"MODEL_UNAVAILABLE\", \"message\": f\"Model/Client for '{model_alias}' unavailable.\"})\n",
        "        # --- End TODO ---\n",
        "        if errors:\n",
        "            response[\"status\"] = \"bad_request\"; response[\"error_details\"] = errors\n",
        "            response[\"metadata\"][\"processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "            return response\n",
        "\n",
        "        status = \"pending\"; response_payload = None; response_metadata = {}\n",
        "        try:\n",
        "            # --- TODO: Construct a robust prompt for KG extraction ---\n",
        "            # Include instructions, desired output format (JSON), examples, context.\n",
        "            prompt = f\"\"\"Extract structured Knowledge Graph data (entities and relationships) from the following content.\n",
        "Content Type: {data_type or 'Unknown'}\n",
        "Context: {json.dumps(context, default=str)}\n",
        "Content:\n",
        "---\n",
        "{content}\n",
        "---\n",
        "Output ONLY a JSON object with two keys: \"entities\" and \"relationships\".\n",
        "Entities should be a list of objects, each with \"type\", \"name\", and other relevant properties. Include \"_resolution_hints\" with original identifiers if possible.\n",
        "Relationships should be a list of objects, each with \"source_hints\", \"target_hints\", \"type\", and properties.\n",
        "Example Entity: {{\"type\": \"Person\", \"name\": \"John Doe\", \"_resolution_hints\": {{\"source_id\": \"user123\"}}}}\n",
        "Example Relationship: {{\"source_hints\": {{\"name\": \"Company A\"}}, \"target_hints\": {{\"name\": \"Product B\"}}, \"type\": \"PRODUCES\"}}\n",
        "JSON Output:\"\"\"\n",
        "            # --- End TODO ---\n",
        "\n",
        "            # Call generate_text\n",
        "            fm_request = {\n",
        "                \"payload\": {\"prompt\": prompt, \"model_alias\": model_alias, \"max_tokens\": 2048, \"temperature\": 0.1}, # Low temp for structured output\n",
        "                \"trace_id\": trace_id, \"request_id\": f\"fm_kg_extract_{request_id}\"\n",
        "            }\n",
        "            fm_response = await self.generate_text(input_data=fm_request) # Call internal generate_text\n",
        "\n",
        "            if fm_response.get(\"status\") == \"success\":\n",
        "                generated_text = fm_response.get(\"payload\", {}).get(\"generated_text\", \"\")\n",
        "                response_metadata = fm_response.get(\"metadata\", {}) # Get metadata from FM call\n",
        "                try:\n",
        "                    # Attempt to parse the JSON output from the LLM\n",
        "                    kg_data = json.loads(generated_text)\n",
        "                    if isinstance(kg_data, dict) and \"entities\" in kg_data and \"relationships\" in kg_data:\n",
        "                        status = \"success\"\n",
        "                        response_payload = {\n",
        "                            \"entities\": kg_data.get(\"entities\", []),\n",
        "                            \"relationships\": kg_data.get(\"relationships\", [])\n",
        "                        }\n",
        "                    else:\n",
        "                        status = \"failed\"; errors.append({\"code\": \"INVALID_LLM_JSON\", \"message\": \"LLM did not return valid JSON with 'entities' and 'relationships' keys.\", \"raw_output\": generated_text[:500]})\n",
        "                except json.JSONDecodeError as json_e:\n",
        "                    status = \"failed\"; errors.append({\"code\": \"LLM_JSON_PARSE_ERROR\", \"message\": f\"Failed to parse LLM JSON output: {json_e}\", \"raw_output\": generated_text[:500]})\n",
        "            else:\n",
        "                 status = \"failed\"; errors = fm_response.get(\"error_details\") # Propagate errors from generate_text\n",
        "\n",
        "        except Exception as e:\n",
        "             status = \"internal_error\"; errors.append({\"code\": \"KG_EXTRACT_ERROR\", \"message\": str(e)}); logger.error(f\"Error extracting KG data with {model_alias}: {e}\", exc_info=True)\n",
        "\n",
        "        response[\"status\"] = status\n",
        "        response[\"payload\"] = response_payload\n",
        "        response[\"metadata\"] = response_metadata\n",
        "        response[\"metadata\"][\"total_processing_duration_ms\"] = (time.monotonic() - start_time) * 1000\n",
        "        if errors: response[\"error_details\"] = errors\n",
        "        return response\n",
        "\n",
        "\n",
        "# --- Initialization (Conceptual - Service deployed separately) ---\n",
        "# fm_client: Optional[FoundationModelClient] = None\n",
        "\n",
        "# async def initialize_fm_client():\n",
        "#     global fm_client\n",
        "#     if not _config_obj:\n",
        "#         logger.critical(\"Cannot initialize FoundationModelClient: CONFIG_OBJ not loaded.\")\n",
        "#         return\n",
        "#     try:\n",
        "#         fm_client = FoundationModelClient(_config_obj)\n",
        "#         await fm_client.initialize() # Initialize async\n",
        "#         logger.info(\"FoundationModelClient initialized successfully.\")\n",
        "#     except Exception as fm_init_e:\n",
        "#         logger.critical(f\"FoundationModelClient init failed: {fm_init_e}\", exc_info=True)\n",
        "#         fm_client = None\n",
        "\n",
        "# async def cleanup_fm_client():\n",
        "#      if fm_client:\n",
        "#          await fm_client.cleanup()\n",
        "#          logger.info(\"FoundationModelClient cleaned up.\")\n",
        "\n",
        "# --- Example Usage (Conceptual - How another tool/service might call this) ---\n",
        "# async def some_other_tool_method(fm_client_proxy: FoundationModelClient):\n",
        "#      request = {\n",
        "#          \"payload\": {\"prompt\": \"Translate to French: Hello\", \"model_alias\": \"gemini-1.5-flash-001\"},\n",
        "#          \"trace_id\": \"some-trace-id\"\n",
        "#      }\n",
        "#      response = await fm_client_proxy.generate_text(input_data=request)\n",
        "#      if response[\"status\"] == \"success\":\n",
        "#          print(response[\"payload\"][\"generated_text\"])\n",
        "#      else:\n",
        "#          print(\"Error:\", response[\"error_details\"])\n",
        "\n",
        "print(\"\\n--- MIZ 3.0 Foundation Model Client (Cell 18 - New Implementation) ---\")\n",
        "print(\"Provides unified async interface for Vertex, OpenAI, Anthropic (REST).\")\n",
        "print(\"Includes rate limiting, retries, caching, cost estimation placeholders.\")\n",
        "print(\"Handles MIZ OKI I/O structure.\")\n",
        "print(\"Requires installation of provider SDKs (openai) and aiohttp, tenacity, cachetools.\")\n",
        "print(\"----------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "WVuKw3PiJiI7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "77cad789-57ef-40c8-b8d6-ebb77aa24e1b"
      },
      "id": "WVuKw3PiJiI7",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-21-e49335905976>, line 44)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-e49335905976>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    class GenerativeModel: pass; class TextEmbeddingModel: pass; class Part: pass; class FinishReason: pass; class Candidate: pass; class GenerationResponse: pass; class TextEmbeddingInput: pass\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xi8e_yO64kBR"
      },
      "id": "xi8e_yO64kBR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "ceo (Apr 10, 2025, 1:53:26 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}