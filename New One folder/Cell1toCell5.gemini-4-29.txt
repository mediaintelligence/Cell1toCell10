# ============================================
# File: pipeline_utils/__init__.py
# ============================================
"""
Shared utilities for the ETL → Customer Journey → Knowledge Graph pipeline.
Provides standardized configuration, logging, client initialization, and more.
"""
__version__ = "1.0.0"

# ============================================
# File: pipeline_utils/config.py
# ============================================
"""
Configuration management for the pipeline.
Loads config from various sources with validation and supports multiple environments.
"""
import os
import logging
import yaml
import json
from typing import Any, Dict, Optional, List, Union, Type, TypeVar
from dataclasses import dataclass, field, asdict

logger = logging.getLogger(__name__)

T = TypeVar('T', bound='PipelineConfig')

@dataclass
class PipelineConfig:
    """Configuration container with validation and defaults for the pipeline."""
    # GCP settings
    project_id: str
    dataset_id: str
    location: str = "US"

    # Storage settings
    bucket_name: str

    # PubSub topics
    parsing_topic_id: str = "etl-parsing-requests"
    cleaning_topic_id: str = "etl-cleaning-requests"
    journey_topic_id: str = "etl-journey-requests"
    catalog_topic_id: str = "etl-catalog-requests"
    kg_topic_id: str = "etl-kg-requests"

    # Neo4j settings (Secret Names)
    neo4j_uri_secret: str = "neo4j-uri"
    neo4j_user_secret: str = "neo4j-user"
    neo4j_password_secret: str = "neo4j-password"

    # Data sources (Prefixes within the bucket)
    data_sources: Dict[str, Dict[str, str]] = field(default_factory=lambda: {
        'Facebook':    {'prefix': 'mycocoons/Facebook/', 'format': 'csv'},
        'GA4Custom':   {'prefix': 'mycocoons/GA4Custom/', 'format': 'csv'},
        'GAAnalytics': {'prefix': 'mycocoons/GAAnalytics/', 'format': 'csv'},
        'Google':      {'prefix': 'mycocoons/Google/', 'format': 'csv'},
        'Shopify':     {'prefix': 'mycocoons/Shopify/', 'format': 'csv'},
    })

    # Performance settings
    discovery_batch_size: int = 2000
    trigger_max_workers: int = 20
    discovery_max_workers: int = 10

    # Table names (base names, full IDs generated by helpers)
    manifest_table_name: str = "pipeline_manifest"
    journey_table_base_name: str = "cust_journey_events"
    data_dictionary_table_name: str = "data_dictionary"

    # Retry settings
    default_retries: int = 3
    default_initial_backoff: float = 1.0
    default_max_backoff: float = 60.0
    default_backoff_factor: float = 2.0

    # Other settings
    use_pubsub: bool = True
    dry_run: bool = False
    log_level: str = "INFO"

    @classmethod
    def from_dict(cls: Type[T], config_dict: Dict[str, Any]) -> T:
        """Create config from dictionary with validation."""
        required_fields = ['project_id', 'dataset_id', 'bucket_name']
        missing = [f for f in required_fields if f not in config_dict]
        if missing:
            raise ValueError(f"Missing required configuration fields: {', '.join(missing)}")

        # Filter dict to only include fields defined in the dataclass
        known_fields = {f.name for f in cls.__dataclass_fields__.values()}
        filtered_dict = {k: v for k, v in config_dict.items() if k in known_fields}

        # Create instance with provided values and defaults
        return cls(**filtered_dict)

    @classmethod
    def from_yaml(cls: Type[T], file_path: str) -> T:
        """Load configuration from YAML file."""
        try:
            with open(file_path, 'r') as f:
                config_dict = yaml.safe_load(f)
            if not isinstance(config_dict, dict):
                 raise TypeError(f"YAML file {file_path} did not parse into a dictionary.")
            return cls.from_dict(config_dict)
        except FileNotFoundError:
            logger.error(f"Configuration file not found: {file_path}")
            raise
        except Exception as e:
            logger.error(f"Failed to load config from {file_path}: {e}", exc_info=True)
            raise

    @classmethod
    def from_env(cls: Type[T]) -> T:
        """Load configuration from environment variables with defaults."""
        # Define environment variable mapping if different from dataclass field names
        env_map = {
            'PROJECT_ID': 'project_id',
            'DATASET_ID': 'dataset_id',
            'LOCATION': 'location',
            'BUCKET_NAME': 'bucket_name',
            'PUBSUB_PARSING_TOPIC': 'parsing_topic_id',
            'PUBSUB_CLEANING_TOPIC': 'cleaning_topic_id',
            'PUBSUB_JOURNEY_TOPIC': 'journey_topic_id',
            'PUBSUB_CATALOG_TOPIC': 'catalog_topic_id',
            'PUBSUB_KG_TOPIC': 'kg_topic_id',
            'NEO4J_URI_SECRET': 'neo4j_uri_secret',
            'NEO4J_USER_SECRET': 'neo4j_user_secret',
            'NEO4J_PASSWORD_SECRET': 'neo4j_password_secret',
            'DISCOVERY_BATCH_SIZE': 'discovery_batch_size',
            'TRIGGER_MAX_WORKERS': 'trigger_max_workers',
            'DISCOVERY_MAX_WORKERS': 'discovery_max_workers',
            'MANIFEST_TABLE_NAME': 'manifest_table_name',
            'JOURNEY_TABLE_BASE_NAME': 'journey_table_base_name',
            'DATA_DICTIONARY_TABLE_NAME': 'data_dictionary_table_name',
            'USE_PUBSUB': 'use_pubsub',
            'DRY_RUN': 'dry_run',
            'LOG_LEVEL': 'log_level',
        }

        config_dict = {}
        # Populate from environment variables based on the map
        for env_var, field_name in env_map.items():
            if env_var in os.environ:
                value = os.environ[env_var]
                # Basic type conversion based on default type
                field_type = cls.__annotations__.get(field_name)
                try:
                    if field_type == int:
                        config_dict[field_name] = int(value)
                    elif field_type == float:
                        config_dict[field_name] = float(value)
                    elif field_type == bool:
                        config_dict[field_name] = value.lower() in ('true', '1', 'yes')
                    else: # Includes str, Dict, etc. (handle more complex types if needed)
                        config_dict[field_name] = value
                except ValueError:
                    logger.warning(f"Could not convert env var {env_var}='{value}' to type {field_type}. Using default if available.")

        # Create instance, defaults will be used for missing optional fields
        # Required fields are checked by from_dict
        # Add default data_sources if not overridden by ENV (complex types are harder via ENV)
        if 'data_sources' not in config_dict:
             config_dict['data_sources'] = cls().data_sources # Use default factory

        return cls.from_dict(config_dict)


    def to_dict(self) -> Dict[str, Any]:
        """Convert config to dictionary."""
        return asdict(self)

# --- Helper methods are now in naming.py using get_config() ---

# Global config instance - initialized on first access
_config_instance: Optional[PipelineConfig] = None

def get_config(config_path: Optional[str] = None) -> PipelineConfig:
    """
    Get the singleton PipelineConfig instance.
    Loads from YAML if config_path is provided and exists, otherwise from environment variables.
    """
    global _config_instance
    if _config_instance is None:
        # Prioritize YAML file if path is provided
        if config_path:
            try:
                _config_instance = PipelineConfig.from_yaml(config_path)
                logger.info(f"Loaded configuration from YAML file: {config_path}")
            except FileNotFoundError:
                logger.warning(f"Config file {config_path} not found. Falling back to environment variables.")
                _config_instance = PipelineConfig.from_env()
                logger.info("Loaded configuration from environment variables (YAML not found).")
            except Exception:
                # Error logged in from_yaml, fallback to ENV
                logger.warning(f"Error loading from YAML file {config_path}. Falling back to environment variables.")
                _config_instance = PipelineConfig.from_env()
                logger.info("Loaded configuration from environment variables (YAML load error).")
        else:
             # Default to environment variables if no path is given
            _config_instance = PipelineConfig.from_env()
            logger.info("Loaded configuration from environment variables.")
    return _config_instance


# ============================================
# File: pipeline_utils/secrets.py
# ============================================
"""
Utility functions for retrieving secrets from Google Cloud Secret Manager.
Provides caching to reduce API calls and proper error handling.
"""
import os
import logging
from typing import Optional, Dict
from google.cloud import secretmanager
from google.api_core.exceptions import NotFound, PermissionDenied, GoogleAPIError

# Re-import get_config here to avoid circular dependency issues during initialization
from pipeline_utils.config import get_config

logger = logging.getLogger(__name__)

# Cache secrets in memory for the lifetime of the process
_secret_cache: Dict[str, str] = {}
# Secret Manager client (singleton)
_secret_manager_client: Optional[secretmanager.SecretManagerServiceClient] = None

def _get_secret_manager_client() -> secretmanager.SecretManagerServiceClient:
    """Initialize and return a Secret Manager client."""
    global _secret_manager_client
    if _secret_manager_client is None:
        try:
            logger.debug("Initializing Secret Manager client...")
            _secret_manager_client = secretmanager.SecretManagerServiceClient()
            # Optional: Add a lightweight check here if needed
            logger.debug("Secret Manager client initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize Secret Manager client: {e}", exc_info=True)
            raise RuntimeError("Could not initialize Secret Manager client") from e
    return _secret_manager_client

def get_secret(
    secret_name: str,
    project_id: Optional[str] = None,
    version: str = "latest",
    use_cache: bool = True
) -> Optional[str]:
    """
    Retrieve a secret value from Google Cloud Secret Manager.

    Args:
        secret_name: The name (ID) of the secret.
        project_id: GCP project ID containing the secret (defaults to config).
        version: The secret version (default is "latest").
        use_cache: Whether to use in-memory cache.

    Returns:
        The secret value as a string, or None if retrieval fails.
    """
    if not secret_name:
        logger.error("Secret name cannot be empty.")
        return None

    # Use project_id from config if not provided
    project_id = project_id or get_config().project_id
    if not project_id:
         logger.error("Project ID is required but not provided or found in config.")
         return None

    cache_key = f"{project_id}/{secret_name}/{version}"

    if use_cache and cache_key in _secret_cache:
        logger.debug(f"Using cached secret: {secret_name}")
        return _secret_cache[cache_key]

    try:
        client = _get_secret_manager_client()
        secret_path = f"projects/{project_id}/secrets/{secret_name}/versions/{version}"
        logger.debug(f"Accessing secret: {secret_path}")

        request = {"name": secret_path}
        response = client.access_secret_version(request=request)
        secret_value = response.payload.data.decode("UTF-8")

        if use_cache:
            _secret_cache[cache_key] = secret_value

        logger.info(f"Successfully retrieved secret: {secret_name}")
        return secret_value

    except NotFound:
        logger.error(f"Secret not found: '{secret_name}' version '{version}' in project '{project_id}'.")
        return None
    except PermissionDenied:
        logger.error(f"Permission denied accessing secret: '{secret_name}' in project '{project_id}'. Check IAM roles.")
        return None
    except GoogleAPIError as e:
        logger.error(f"API error accessing secret {secret_name}: {e}", exc_info=False)
        return None
    except Exception as e: # Includes client init errors
        logger.error(f"Unexpected error retrieving secret {secret_name}: {e}", exc_info=True)
        return None

def clear_secret_cache():
    """Clear the secret cache."""
    global _secret_cache
    _secret_cache = {}
    logger.debug("Secret cache cleared")


# ============================================
# File: pipeline_utils/logging.py
# ============================================
"""
Structured logging configuration for all pipeline components.
Provides consistent formatting, log levels, and context handling.
"""
import logging
import json
import sys
import os
import uuid
import threading
from datetime import datetime
from typing import Optional, Dict, Any, Union

# Thread-local storage for trace IDs
_thread_local = threading.local()

def get_trace_id() -> str:
    """Get the current trace ID or generate a new one."""
    if not hasattr(_thread_local, 'trace_id'):
        _thread_local.trace_id = str(uuid.uuid4())
    return _thread_local.trace_id

def set_trace_id(trace_id: str):
    """Set the trace ID for the current thread."""
    _thread_local.trace_id = trace_id

class StructuredLogFormatter(logging.Formatter):
    """Formatter that outputs JSON structured logging, compatible with Cloud Logging."""

    def format(self, record):
        """Format the log record as JSON."""
        log_entry = {
            'severity': record.levelname,
            'message': record.getMessage(),
            'name': record.name, # Logger name
            'timestamp': datetime.utcfromtimestamp(record.created).isoformat() + 'Z',
            # Cloud Logging special fields for context
            'logging.googleapis.com/source_location': {
                 'file': record.pathname,
                 'line': record.lineno,
                 'function': record.funcName,
            },
             'logging.googleapis.com/trace': f"projects/{os.environ.get('PROJECT_ID', 'unknown-project')}/traces/{get_trace_id()}", # Assumes PROJECT_ID env var is set
             # Add custom context fields directly
             **(getattr(record, 'custom_extra', {}))
        }

        # Add exception info if present
        if record.exc_info:
            log_entry['exception_type'] = record.exc_info[0].__name__
            log_entry['exception_message'] = str(record.exc_info[1])
            # formatException is useful for multi-line traceback string
            log_entry['exception_traceback'] = self.formatException(record.exc_info)

        return json.dumps(log_entry, default=str) # Use default=str for non-serializable types

class ContextAddingFilter(logging.Filter):
    """Injects custom context into log records."""
    def __init__(self, context: Dict[str, Any]):
        super().__init__()
        self._context = context

    def filter(self, record):
        record.custom_extra = self._context
        return True

# Store initial setup state
_logging_initialized = False

def setup_logging(
    level: str = "INFO",
    service_name: Optional[str] = None, # For context
    component: Optional[str] = None # For context
):
    """
    Set up structured logging for the entire application.
    Uses Cloud Logging handler if available, otherwise streams JSON to stdout.
    Should be called once at application startup.
    """
    global _logging_initialized
    if _logging_initialized:
        # logger.debug("Logging already initialized.") # Avoid logging before handler setup maybe
        return

    numeric_level = getattr(logging, level.upper(), logging.INFO)
    log_format = StructuredLogFormatter()

    # Configure the root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(numeric_level)

    # Remove existing handlers to avoid duplicates
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    handler = None
    cloud_logging_available = False
    # Try setting up Google Cloud Logging handler
    try:
        import google.cloud.logging
        # Check if running in a suitable GCP environment
        # This could be more sophisticated (check metadata server)
        if os.environ.get('K_SERVICE') or os.environ.get('GOOGLE_CLOUD_PROJECT'):
            client = google.cloud.logging.Client()
            handler = google.cloud.logging.handlers.CloudLoggingHandler(client)
            handler.setFormatter(log_format) # Use our structured format
            cloud_logging_available = True
            initial_log_message = f"Initialized Google Cloud Logging handler for {service_name or 'service'}"
        else:
             initial_log_message = "Not running in a recognized GCP serverless environment. Using stdout handler."
    except ImportError:
        initial_log_message = "google-cloud-logging library not found. Using stdout handler."
    except Exception as e:
        initial_log_message = f"Failed to initialize Cloud Logging: {e}. Using stdout handler."

    # Fallback to console (stdout) handler if Cloud Logging handler not set
    if not cloud_logging_available:
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(log_format)

    root_logger.addHandler(handler)

    # Add base context
    base_context = {}
    if service_name: base_context['service'] = service_name
    if component: base_context['component'] = component # Can be overridden by get_logger context
    if base_context:
        context_filter = ContextAddingFilter(base_context)
        root_logger.addFilter(context_filter)

    # Reduce verbosity for overly noisy libraries
    noisy_loggers = ['google.api_core', 'google.auth', 'google.cloud', 'urllib3', 'oauth2client']
    for logger_name in noisy_loggers:
        logging.getLogger(logger_name).setLevel(logging.WARNING)

    # Set initialization flag and log initial message
    _logging_initialized = True
    logging.info(initial_log_message) # Log after handler is attached
    logging.info(f"Root logger level set to {logging.getLevelName(numeric_level)}")


def get_logger(name: str, **context) -> logging.Logger:
    """
    Get a logger instance with optional context added.
    Ensures setup_logging has been called.
    """
    if not _logging_initialized:
         # Basic setup if not called explicitly
         print("Warning: Logging not explicitly initialized via setup_logging. Using basic stdout config.", file=sys.stderr)
         setup_logging() # Use defaults

    logger = logging.getLogger(name)
    # If context is provided, create a logger adapter or add a filter
    # For simplicity, let's log context directly in messages for now,
    # or rely on the structured formatter picking up 'extra'
    # A more robust way is using LoggerAdapter, but requires careful use.
    # Returning the base logger is simpler for direct use with 'extra'.
    # Example usage: logger.info("Message", extra=context)
    return logger # Returning the standard logger, caller uses 'extra'


# ============================================
# File: pipeline_utils/retry.py
# ============================================
"""
Unified retry mechanisms for all pipeline components.
Provides decorators and utility functions for standardized retries.
"""
import logging
import time
import random
import functools
from typing import Callable, Tuple, Any, Type, Optional, Union

# Import common retryable exceptions
from google.api_core.exceptions import (
    GoogleAPIError, ServiceUnavailable, DeadlineExceeded, ResourceExhausted, InternalServerError, BadGateway
)
from google.cloud import exceptions as cloud_exceptions
from requests.exceptions import ConnectionError, Timeout, RequestException # If using requests

# Re-import get_config here to avoid circular dependency issues during initialization
from pipeline_utils.config import get_config

logger = logging.getLogger(__name__)

# Define exception categories for retry logic
GOOGLE_API_RETRYABLE_EXCEPTIONS = (
    # Explicitly list retryable Google API errors based on documentation/experience
    ServiceUnavailable, # 503
    DeadlineExceeded, # Typically 504 or client-side timeout
    ResourceExhausted, # 429 Quota/Rate limit
    InternalServerError, # 500
    BadGateway, # 502
    cloud_exceptions.ServerError, # Generic 5xx from google-cloud-python libs
    cloud_exceptions.TooManyRequests, # Generic 429 from google-cloud-python libs
    # GoogleAPIError is broad, use more specific types above if possible
)
NETWORK_RETRYABLE_EXCEPTIONS = (
    ConnectionError, # Includes requests.exceptions.ConnectionError
    ConnectionRefusedError,
    ConnectionResetError,
    Timeout, # Includes requests.exceptions.Timeout
    TimeoutError, # Built-in timeout
    # RequestException can be broad, consider specific sub-classes if using requests heavily
)

# Try to import Neo4j exceptions if available
try:
    from neo4j import exceptions as neo_exceptions
    NEO4J_RETRYABLE_EXCEPTIONS = (
        neo_exceptions.ServiceUnavailable, # Neo4j service unavailable
        neo_exceptions.TransientError, # Deadlocks, temporary cluster issues
        neo_exceptions.SessionExpired, # Session needs refresh
        # Avoid retrying AuthError, ClientError etc.
    )
except ImportError:
    NEO4J_RETRYABLE_EXCEPTIONS = () # Neo4j not installed

# Combine default retryable exceptions
DEFAULT_RETRYABLE_EXCEPTIONS = GOOGLE_API_RETRYABLE_EXCEPTIONS + NETWORK_RETRYABLE_EXCEPTIONS + NEO4J_RETRYABLE_EXCEPTIONS

def default_on_retry(retry_count: int, exception: Exception, delay: float, func_name: str):
    """Default callback for logging retries."""
    logger.warning(
        f"Retry {retry_count} for {func_name} in {delay:.2f}s after error: {exception.__class__.__name__}: {exception}"
    )

def retry_with_backoff(
    retries: Optional[int] = None,
    initial_backoff: Optional[float] = None,
    max_backoff: Optional[float] = None,
    backoff_factor: Optional[float] = None,
    retryable_exceptions: Tuple[Type[Exception], ...] = DEFAULT_RETRYABLE_EXCEPTIONS,
    jitter: bool = True,
    on_retry: Callable[[int, Exception, float, str], None] = default_on_retry
) -> Callable:
    """
    Decorator for retrying a function with exponential backoff.
    Uses settings from config if arguments are None.

    Args:
        retries: Max number of retries (default: config.default_retries).
        initial_backoff: Initial delay seconds (default: config.default_initial_backoff).
        max_backoff: Max delay seconds (default: config.default_max_backoff).
        backoff_factor: Multiplier for delay (default: config.default_backoff_factor).
        retryable_exceptions: Tuple of exception types that trigger a retry.
        jitter: Add random jitter to delay (default: True).
        on_retry: Callback before sleeping: `on_retry(attempt, exception, delay, func_name)`.

    Returns:
        Decorator function.
    """
    def decorator(func: Callable) -> Callable:
        # Load defaults from config if args are None
        config = get_config()
        _retries = retries if retries is not None else config.default_retries
        _initial_backoff = initial_backoff if initial_backoff is not None else config.default_initial_backoff
        _max_backoff = max_backoff if max_backoff is not None else config.default_max_backoff
        _backoff_factor = backoff_factor if backoff_factor is not None else config.default_backoff_factor
        func_name = func.__name__ # Get function name for logging

        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            current_delay = _initial_backoff
            for attempt in range(_retries + 1): # +1 to include the initial try
                try:
                    return func(*args, **kwargs)
                except retryable_exceptions as e:
                    if attempt == _retries:
                        logger.error(f"Max retries ({_retries}) exceeded for {func_name}. Last error: {e}", exc_info=True)
                        raise # Reraise the last exception

                    # Calculate delay with jitter
                    sleep_time = current_delay
                    if jitter:
                        sleep_time = random.uniform(0.5 * sleep_time, 1.5 * sleep_time)
                    sleep_time = max(0.1, sleep_time) # Ensure minimum sleep

                    # Call the callback
                    if on_retry:
                        try:
                            on_retry(attempt + 1, e, sleep_time, func_name)
                        except Exception as cb_err:
                            logger.warning(f"Error in on_retry callback for {func_name}: {cb_err}", exc_info=False)

                    # Wait
                    time.sleep(sleep_time)

                    # Increase delay for next attempt
                    current_delay = min(current_delay * _backoff_factor, _max_backoff)

                except Exception as e:
                    # Non-retryable exception
                    logger.error(f"Non-retryable error in {func_name}: {e.__class__.__name__}", exc_info=True)
                    raise # Reraise immediately
            # Should not be reachable if retries >= 0
            return None
        return wrapper
    return decorator


# ============================================
# File: pipeline_utils/clients.py
# ============================================
"""
Standardized client initialization for all services used in the pipeline.
Provides factories for BigQuery, Storage, Pub/Sub, and Neo4j clients.
Manages client instances as singletons within the process.
"""
import logging
from typing import Optional, Tuple, Dict
from google.cloud import bigquery, storage, pubsub_v1
from google.api_core.exceptions import NotFound, GoogleAPIError

# Import shared utilities AFTER basic modules to avoid circularity at import time
from pipeline_utils.config import get_config
from pipeline_utils.secrets import get_secret
from pipeline_utils.retry import retry_with_backoff, DEFAULT_RETRYABLE_EXCEPTIONS, NEO4J_RETRYABLE_EXCEPTIONS

# Try importing neo4j safely
try:
    import neo4j
    NEO4J_INSTALLED = True
except ImportError:
    NEO4J_INSTALLED = False

logger = logging.getLogger(__name__)

# Simple in-process client caching
_clients: Dict[str, object] = {}

# Use retry decorator defined in retry.py
@retry_with_backoff(retries=4, initial_backoff=0.5, max_backoff=10.0)
def get_bigquery_client(project_id: Optional[str] = None) -> bigquery.Client:
    """Get a singleton BigQuery client instance, initializing if needed."""
    global _clients
    config = get_config()
    project_id = project_id or config.project_id
    cache_key = f"bq:{project_id}"

    if cache_key not in _clients:
        logger.info(f"Initializing BigQuery client for project {project_id}...")
        try:
            client = bigquery.Client(project=project_id)
            # Perform a lightweight operation to check connectivity/auth
            client.list_datasets(max_results=1)
            _clients[cache_key] = client
            logger.info("BigQuery client initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize BigQuery client: {e}", exc_info=True)
            raise # Let retry handler manage this
    return _clients[cache_key]

@retry_with_backoff(retries=4, initial_backoff=0.5, max_backoff=10.0)
def get_storage_client(project_id: Optional[str] = None) -> storage.Client:
    """Get a singleton Storage client instance, initializing if needed."""
    global _clients
    config = get_config()
    project_id = project_id or config.project_id
    cache_key = f"gcs:{project_id}"

    if cache_key not in _clients:
        logger.info(f"Initializing Storage client for project {project_id}...")
        try:
            client = storage.Client(project=project_id)
            # Optional: Check connectivity e.g., client.get_service_account_email()
            _clients[cache_key] = client
            logger.info("Storage client initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize Storage client: {e}", exc_info=True)
            raise
    return _clients[cache_key]

@retry_with_backoff(retries=4, initial_backoff=0.5, max_backoff=10.0)
def get_pubsub_publisher(
    project_id: Optional[str] = None,
    topic_id: Optional[str] = None # If provided, verifies topic path
) -> Tuple[pubsub_v1.PublisherClient, Optional[str]]:
    """
    Get a singleton Pub/Sub publisher client instance and optionally a verified topic path.
    """
    global _clients
    config = get_config()
    project_id = project_id or config.project_id
    # Cache key for publisher client instance (shared across topics)
    publisher_cache_key = f"pubsub_publisher:{project_id}"

    # Initialize publisher client if not cached
    if publisher_cache_key not in _clients:
        logger.info(f"Initializing Pub/Sub publisher client for project {project_id}...")
        try:
            publisher = pubsub_v1.PublisherClient()
            # Optional: Add a check here if needed
            _clients[publisher_cache_key] = publisher
            logger.info("Pub/Sub publisher client initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize Pub/Sub publisher client: {e}", exc_info=True)
            raise
    publisher = _clients[publisher_cache_key]

    # Handle topic path verification and caching
    topic_path = None
    if topic_id:
        topic_cache_key = f"pubsub_topic_path:{project_id}/{topic_id}"
        if topic_cache_key in _clients:
            topic_path = _clients[topic_cache_key]
        else:
            try:
                topic_path = publisher.topic_path(project_id, topic_id)
                logger.debug(f"Verifying Pub/Sub topic path: {topic_path}")
                publisher.get_topic(request={"topic": topic_path}) # Verify existence
                _clients[topic_cache_key] = topic_path # Cache verified path
                logger.debug(f"Pub/Sub topic path verified: {topic_path}")
            except NotFound:
                logger.error(f"Pub/Sub topic '{topic_id}' not found in project '{project_id}'.")
                _clients[topic_cache_key] = None # Cache None to prevent re-checks
                topic_path = None
            except Exception as e:
                 # Catch other potential errors during verification
                logger.error(f"Error verifying Pub/Sub topic path for '{topic_id}': {e}", exc_info=False)
                topic_path = None # Do not cache on error

    return publisher, topic_path


@retry_with_backoff(retries=4, initial_backoff=1.0, max_backoff=30.0,
                    retryable_exceptions=NEO4J_RETRYABLE_EXCEPTIONS + NETWORK_RETRYABLE_EXCEPTIONS)
def get_neo4j_driver():
    """Get a singleton Neo4j driver instance, configured via secrets."""
    global _clients
    if not NEO4J_INSTALLED:
        logger.error("Neo4j driver requested but 'neo4j' package is not installed.")
        return None

    cache_key = "neo4j_driver"
    if cache_key not in _clients:
        logger.info("Initializing Neo4j driver...")
        config = get_config()
        uri = get_secret(config.neo4j_uri_secret)
        user = get_secret(config.neo4j_user_secret)
        password = get_secret(config.neo4j_password_secret)

        if not all([uri, user, password]):
            logger.error("Missing Neo4j connection details from Secret Manager. Cannot initialize driver.")
            return None # Or raise configuration error

        try:
            # Let driver infer encryption from scheme (neo4j+s / neo4j / bolt+s / bolt)
            driver = neo4j.GraphDatabase.driver(
                uri,
                auth=(user, password),
                max_connection_lifetime=3600, # Example: 1 hour
                connection_timeout=10, # Example: 10 seconds
                max_connection_pool_size=50 # Example pool size
            )
            # Verify connectivity during initialization
            driver.verify_connectivity()
            _clients[cache_key] = driver
            logger.info("Neo4j driver initialized and connection verified.")
        except Exception as e:
            logger.error(f"Failed to initialize or verify Neo4j driver: {e}", exc_info=True)
            raise # Let retry handler manage this

    return _clients[cache_key]

def close_clients():
    """Close cached clients (especially Neo4j driver)."""
    global _clients
    logger.info("Closing cached clients...")
    for key, client in list(_clients.items()):
        if key == "neo4j_driver" and client:
            try:
                logger.info("Closing Neo4j driver...")
                client.close()
            except Exception as e:
                logger.warning(f"Error closing Neo4j driver: {e}")
        # Add closing logic for other clients if necessary (e.g., PubSub publisher channel)
        # elif key.startswith("pubsub_publisher:") and client:
        #     try:
        #         # Publisher client itself doesn't have an explicit close,
        #         # but underlying gRPC channel might if needed for clean shutdown.
        #         # client.transport.channel.close() # Example, needs check
        #         pass
        #     except Exception as e:
        #         logger.warning(f"Error closing PubSub client resource: {e}")
    _clients = {} # Clear cache
    logger.info("Client cache cleared.")


# ============================================
# File: pipeline_utils/naming.py
# ============================================
"""
Standardized functions for generating BigQuery table names and other resource names
for the ETL -> Customer Journey -> Knowledge Graph pipeline.
Integrates with the config system.
"""
import re
import os
import logging
from typing import Optional

# Re-import get_config here to avoid circular dependency issues during initialization
from pipeline_utils.config import get_config

logger = logging.getLogger(__name__)

# --- Helper Functions ---
def _sanitize_bq_name(name_part: str) -> str:
    """Removes invalid chars for BQ names, converts to lowercase, handles edge cases."""
    if not name_part: return "unknown"
    sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', name_part).lower()
    sanitized = re.sub(r'_+', '_', sanitized).strip('_')
    if sanitized and sanitized[0].isdigit(): sanitized = "_" + sanitized
    return sanitized if sanitized else "sanitized_empty"

def _validate_bq_name(name: str, object_type: str = "Table name part"):
    """Validates if a name part conforms to BigQuery naming rules."""
    if not name: raise ValueError(f"{object_type} cannot be empty after sanitization.")
    if len(name) > 1024: logger.warning(f"Generated {object_type} may exceed 1024 chars: '{name[:50]}...'")
    if not re.match(r"^[a-zA-Z0-9_]+$", name): raise ValueError(f"Invalid chars in {object_type} '{name}' after sanitization.")

# --- Table ID Generation Functions ---

def get_manifest_table_id(
    project_id: Optional[str] = None,
    dataset_id: Optional[str] = None,
    table_name: Optional[str] = None
) -> str:
    """Gets the full ID for the pipeline manifest table using config defaults."""
    config = get_config()
    project_id = project_id or config.project_id
    dataset_id = dataset_id or config.dataset_id
    table_name = table_name or config.manifest_table_name
    _validate_bq_name(table_name, "Manifest table name")
    return f"{project_id}.{dataset_id}.{table_name}"


def get_staging_table_id(
    blob_name: str,
    project_id: Optional[str] = None,
    dataset_id: Optional[str] = None
) -> Optional[str]:
    """
    Generates staging table ID: stg_<source>_<identifier>_<date_suffix>
    """
    if not blob_name: logger.error("blob_name required for staging ID."); return None
    config = get_config()
    project_id = project_id or config.project_id
    dataset_id = dataset_id or config.dataset_id
    try:
        parts = blob_name.split('/')
        # Handle path structures like 'prefix/Source/file.csv' or just 'Source/file.csv'
        # Check if the first part matches any known prefix base (e.g., 'mycocoons')
        known_prefix_base = next((ds_config['prefix'].split('/')[0] for ds_config in config.data_sources.values() if '/' in ds_config.get('prefix','')), None)
        source_index = 1 if known_prefix_base and parts[0].lower() == known_prefix_base and len(parts) > 1 else 0
        source = parts[source_index] if len(parts) > source_index else "unknown_source"

        filename = os.path.splitext(os.path.basename(blob_name))[0]
        date_match = re.search(r'(_?)(\d{8})$', filename)
        date_suffix = f"_{date_match.group(2)}" if date_match else ""
        identifier = re.sub(r'(_?\d{8})$', '', filename) if date_match else filename

        sanitized_source = _sanitize_bq_name(source)
        sanitized_identifier = _sanitize_bq_name(identifier)
        if not sanitized_source or not sanitized_identifier: raise ValueError("Source/Identifier empty after sanitization.")

        table_name = f"stg_{sanitized_source}_{sanitized_identifier}{date_suffix}"
        _validate_bq_name(table_name, "Staging table name")
        full_table_id = f"{project_id}.{dataset_id}.{table_name}"
        logger.debug(f"Generated Staging ID: {full_table_id} for blob {blob_name}")
        return full_table_id
    except Exception as e:
        logger.error(f"Error generating staging table ID for blob '{blob_name}': {e}", exc_info=True)
        return None


def get_cleaned_table_id(
    stg_table_id_or_name: str,
    project_id: Optional[str] = None,
    dataset_id: Optional[str] = None
) -> Optional[str]:
    """
    Generates cleaned table ID from staging ID/name.
    Output Pattern: clean_mycocoons_{SourceCap}_{IDENTIFIER_UPPER}_{date_suffix}
    """
    if not stg_table_id_or_name: logger.error("Staging ID/name required for cleaned ID."); return None
    config = get_config()
    project_id = project_id or config.project_id
    dataset_id = dataset_id or config.dataset_id
    try:
        name_part = stg_table_id_or_name.split('.')[-1] # Get just the table name
        if not name_part.startswith('stg_'): raise ValueError(f"Input '{name_part}' doesn't start with 'stg_'.")

        core = name_part[4:]
        segs = core.split('_')
        if len(segs) < 2: raise ValueError(f"Cannot parse 'stg_' name part: '{core}'.")

        source_lower = segs[0] # Should be lowercase from initial sanitize
        date_suffix_part = f"_{segs[-1]}" if re.fullmatch(r'\d{8}', segs[-1]) else ""
        identifier_parts = segs[1:-1] if date_suffix_part else segs[1:]
        identifier_lower = '_'.join(identifier_parts)

        # Apply transformations for the 'clean_mycocoons_*' pattern
        source_cap = source_lower.capitalize() # Facebook
        identifier_upper = identifier_lower.upper() # ADSDATA

        cleaned_name = f"clean_mycocoons_{source_cap}_{identifier_upper}{date_suffix_part}"
        _validate_bq_name(cleaned_name, "Cleaned table name")
        full_table_id = f"{project_id}.{dataset_id}.{cleaned_name}"
        logger.debug(f"Generated Cleaned ID: {full_table_id} from {stg_table_id_or_name}")
        return full_table_id
    except Exception as e:
        logger.error(f"Error generating cleaned ID from '{stg_table_id_or_name}': {e}", exc_info=True)
        return None


def get_journey_table_id(
    date_suffix: str, # YYYYMMDD
    project_id: Optional[str] = None,
    dataset_id: Optional[str] = None
) -> Optional[str]:
    """
    Gets the full ID for the customer journey table using config base name.
    Pattern: {journey_table_base_name}_<date_suffix>
    """
    if not date_suffix or not re.fullmatch(r'\d{8}', date_suffix): logger.error(f"Invalid date_suffix '{date_suffix}'."); return None
    config = get_config()
    project_id = project_id or config.project_id
    dataset_id = dataset_id or config.dataset_id
    try:
        table_name = f"{config.journey_table_base_name}_{date_suffix}"
        _validate_bq_name(table_name, "Journey table name")
        full_table_id = f"{project_id}.{dataset_id}.{table_name}"
        logger.debug(f"Generated Journey ID: {full_table_id}")
        return full_table_id
    except Exception as e:
        logger.error(f"Error generating journey ID for date '{date_suffix}': {e}", exc_info=True)
        return None


def get_data_dictionary_table_id(
    project_id: Optional[str] = None,
    dataset_id: Optional[str] = None,
    table_name: Optional[str] = None
) -> str:
    """Gets the full ID for the data dictionary table using config defaults."""
    config = get_config()
    project_id = project_id or config.project_id
    dataset_id = dataset_id or config.dataset_id
    table_name = table_name or config.data_dictionary_table_name
    _validate_bq_name(table_name, "Data dictionary table name")
    return f"{project_id}.{dataset_id}.{table_name}"

# --- Extraction Functions ---

def extract_date_suffix(table_id_or_name: str) -> Optional[str]:
    """Extracts the _YYYYMMDD date suffix from a table ID or name."""
    if not table_id_or_name: return None
    name = table_id_or_name.split('.')[-1]
    date_match = re.search(r'_(\d{8})$', name)
    return date_match.group(1) if date_match else None

def extract_source_from_table_name(table_name_or_id: str) -> Optional[str]:
    """
    Extracts the logical source name (lowercase) from standardized table names.
    Handles 'stg_<source>_*' and 'clean_mycocoons_{SourceCap}_*' patterns.
    """
    if not table_name_or_id: return None
    name = table_name_or_id.split('.')[-1]
    m_stg = re.match(r'stg_([a-zA-Z0-9_]+?)_', name, re.IGNORECASE)
    if m_stg: return _sanitize_bq_name(m_stg.group(1)) # Already lowercase
    m_clean = re.match(r'clean_mycocoons_([a-zA-Z0-9_]+?)_', name, re.IGNORECASE)
    if m_clean: return _sanitize_bq_name(m_clean.group(1)) # Sanitize and lowercase
    logger.debug(f"Could not extract source from table name: '{name}'")
    return None


# ============================================
# File: pipeline_utils/monitoring.py
# ============================================
"""
Monitoring framework for collecting metrics and health information
across all pipeline components. Includes Timer context manager.
"""
import logging
import time
import threading
import json
import os
from datetime import datetime
from typing import Dict, Any, Optional, List, Union
from collections import defaultdict

logger = logging.getLogger(__name__)

# Simple thread-safe in-memory metrics store
_metrics: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
_metrics_lock = threading.Lock()

# --- Timer ---
class Timer:
    """Context manager for timing operations and recording metrics."""
    def __init__(self, metric_name: str, labels: Optional[Dict[str, str]] = None):
        self.metric_name = metric_name
        self.labels = labels
        self.start_time = None

    def __enter__(self):
        self.start_time = time.perf_counter()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.start_time is not None:
            duration_ms = (time.perf_counter() - self.start_time) * 1000
            try:
                # Use function from this module to record
                record_metric(f"{self.metric_name}_duration_ms", duration_ms, labels=self.labels)
            except Exception as e:
                logger.warning(f"Failed to record timer metric {self.metric_name}: {e}")

# --- Metric Recording Functions ---

def _get_metric_key(name: str, labels: Optional[Dict[str, str]] = None) -> str:
    """Generates a unique key for a metric name and its labels."""
    if not labels:
        return name
    # Create a sorted, consistent string representation of labels
    label_str = ",".join(f"{k}={v}" for k, v in sorted(labels.items()))
    return f"{name}{{{label_str}}}" # Use curly braces for labels like Prometheus

def record_metric(name: str, value: Any, labels: Optional[Dict[str, str]] = None):
    """
    Record a single value for a metric (e.g., gauge, summary).
    Keeps a limited history in memory.
    """
    with _metrics_lock:
        key = _get_metric_key(name, labels)
        timestamp = datetime.utcnow().isoformat() + 'Z'
        # For simplicity, store as a list of timestamped values
        _metrics[key].append({'timestamp': timestamp, 'value': value})
        # Limit history size (e.g., last 100 points)
        _metrics[key] = _metrics[key][-100:]

def increment_counter(name: str, increment: Union[int, float] = 1, labels: Optional[Dict[str, str]] = None):
    """
    Increment a counter metric. Stores the latest cumulative value.
    """
    if not isinstance(increment, (int, float)):
        logger.warning(f"Invalid increment value type for counter {name}: {type(increment)}")
        return
    with _metrics_lock:
        key = _get_metric_key(name, labels)
        timestamp = datetime.utcnow().isoformat() + 'Z'
        # Get the last value or default to 0
        last_value = 0
        if key in _metrics and _metrics[key]:
            try:
                 # Ensure the last value is numeric
                 last_recorded_value = _metrics[key][-1].get('value', 0)
                 if isinstance(last_recorded_value, (int, float)):
                      last_value = last_recorded_value
                 else:
                      logger.warning(f"Last recorded value for counter {key} is not numeric: {last_recorded_value}. Resetting counter.")
            except (IndexError, KeyError):
                 pass # No previous value

        new_value = last_value + increment
        # Store only the latest value for counters to avoid large memory usage
        _metrics[key] = [{'timestamp': timestamp, 'value': new_value}]

# --- Metric Retrieval and Export ---

def get_metrics(name_prefix: Optional[str] = None) -> Dict[str, List[Dict[str, Any]]]:
    """Get all current metric values, optionally filtered by prefix."""
    with _metrics_lock:
        # Return a deep copy to prevent modification outside the lock
        metrics_copy = {k: list(v) for k, v in _metrics.items()}
    if name_prefix:
        return {k: v for k, v in metrics_copy.items() if k.startswith(name_prefix)}
    else:
        return metrics_copy

def get_latest_metric_value(name: str, labels: Optional[Dict[str, str]] = None) -> Optional[Any]:
    """Get the latest recorded value for a specific metric."""
    with _metrics_lock:
        key = _get_metric_key(name, labels)
        if key in _metrics and _metrics[key]:
            return _metrics[key][-1].get('value')
        return None

def export_metrics_to_json(file_path: Optional[str] = None) -> str:
    """Export current metrics to JSON (file or string)."""
    metrics_data = get_metrics() # Gets a copy
    try:
        json_str = json.dumps(metrics_data, indent=2, default=str)
        if file_path:
            try:
                with open(file_path, 'w') as f:
                    f.write(json_str)
                logger.debug(f"Metrics exported to {file_path}")
            except Exception as e:
                logger.error(f"Failed to write metrics to {file_path}: {e}")
        return json_str
    except Exception as e:
        logger.error(f"Failed to serialize metrics to JSON: {e}")
        return "{}" # Return empty JSON on failure

# --- Periodic Export (Optional) ---
_export_thread: Optional[threading.Thread] = None
_export_stop_event = threading.Event()

def _metrics_export_loop(export_dir: str, interval_seconds: int):
    """Background task to periodically export metrics."""
    logger.info(f"Starting metrics export loop to {export_dir} every {interval_seconds}s.")
    while not _export_stop_event.wait(timeout=interval_seconds): # Wait for interval or stop signal
        try:
            timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
            file_path = os.path.join(export_dir, f"metrics_{timestamp}.json")
            export_metrics_to_json(file_path)
        except Exception as e:
            logger.error(f"Error during periodic metrics export: {e}", exc_info=False)
    logger.info("Metrics export loop stopped.")

def setup_periodic_metrics_export(export_dir: str, interval_seconds: int = 300):
    """Set up and start periodic export of metrics to JSON files in a background thread."""
    global _export_thread
    if _export_thread and _export_thread.is_alive():
        logger.warning("Periodic metrics export thread already running.")
        return

    if not os.path.exists(export_dir):
        try:
            os.makedirs(export_dir)
            logger.info(f"Created metrics export directory: {export_dir}")
        except Exception as e:
            logger.error(f"Failed to create metrics export directory {export_dir}: {e}. Export disabled.")
            return

    _export_stop_event.clear()
    _export_thread = threading.Thread(
        target=_metrics_export_loop,
        args=(export_dir, interval_seconds),
        daemon=True # Allows program to exit even if thread is running
    )
    _export_thread.start()

def stop_periodic_metrics_export():
    """Signal the periodic metrics export thread to stop."""
    global _export_thread
    if _export_thread and _export_thread.is_alive():
        logger.info("Stopping periodic metrics export thread...")
        _export_stop_event.set()
        _export_thread.join(timeout=5) # Wait briefly for thread to exit
        if _export_thread.is_alive():
             logger.warning("Metrics export thread did not stop gracefully.")
        _export_thread = None
    else:
         logger.info("Periodic metrics export thread not running.")


# ============================================
# File: pipeline_utils/__main__.py
# ============================================
"""Basic test entrypoint for the pipeline utilities package."""
import logging # Import logging directly here for setup

# Set up logging first using the utility function
# Must happen before other utility imports if they log at import time
from pipeline_utils.logging import setup_logging, get_logger
setup_logging(level="DEBUG", service_name="UtilsTest") # Use DEBUG for testing
logger = get_logger(__name__) # Get logger after setup

# Now import other utilities
from pipeline_utils.config import get_config
from pipeline_utils.naming import (
    get_manifest_table_id, get_staging_table_id, get_cleaned_table_id,
    get_journey_table_id, get_data_dictionary_table_id,
    extract_date_suffix, extract_source_from_table_name
)
from pipeline_utils.secrets import get_secret
from pipeline_utils.retry import retry_with_backoff
from pipeline_utils.clients import (
    get_bigquery_client, get_storage_client,
    get_pubsub_publisher, get_neo4j_driver, close_clients
)
from pipeline_utils.monitoring import Timer, record_metric, increment_counter, get_metrics, export_metrics_to_json

@retry_with_backoff(retries=1)
def _test_retry():
    print("Testing retry... this should succeed.")
    return True

@retry_with_backoff(retries=2, initial_backoff=0.1)
def _test_retry_fail():
    print("Testing retry fail...")
    raise ConnectionError("Simulated connection error for retry test")

def run_tests():
    """Runs basic tests for utility functions."""
    logger.info("--- Starting Pipeline Utilities Test ---")

    # 1. Config Test
    logger.info("Testing Config...")
    try:
        config = get_config()
        logger.info(f"Config loaded: project_id={config.project_id}, bucket={config.bucket_name}")
        assert config.project_id is not None
        assert config.dataset_id is not None
        logger.info("Config test PASSED.")
    except Exception as e:
        logger.error(f"Config test FAILED: {e}", exc_info=True)

    # 2. Naming Test
    logger.info("Testing Naming...")
    try:
        blob = "mycocoons/Shopify/orders_export_1_20230101.csv"
        stg_id = get_staging_table_id(blob)
        logger.info(f"Blob: {blob} -> Staging ID: {stg_id}")
        assert stg_id == f"{config.project_id}.{config.dataset_id}.stg_shopify_orders_export_1_20230101"

        clean_id = get_cleaned_table_id(stg_id)
        logger.info(f"Staging ID: {stg_id} -> Cleaned ID: {clean_id}")
        assert clean_id == f"{config.project_id}.{config.dataset_id}.clean_mycocoons_Shopify_ORDERS_EXPORT_1_20230101"

        date_sfx = extract_date_suffix(clean_id)
        logger.info(f"Cleaned ID: {clean_id} -> Date Suffix: {date_sfx}")
        assert date_sfx == "20230101"

        source = extract_source_from_table_name(clean_id)
        logger.info(f"Cleaned ID: {clean_id} -> Source: {source}")
        assert source == "shopify"

        source_stg = extract_source_from_table_name(stg_id)
        logger.info(f"Staging ID: {stg_id} -> Source: {source_stg}")
        assert source_stg == "shopify"

        journey_id = get_journey_table_id("20230101")
        logger.info(f"Date: 20230101 -> Journey ID: {journey_id}")
        assert journey_id == f"{config.project_id}.{config.dataset_id}.cust_journey_events_20230101"

        dict_id = get_data_dictionary_table_id()
        logger.info(f"Data Dictionary ID: {dict_id}")
        assert dict_id == f"{config.project_id}.{config.dataset_id}.data_dictionary"

        manifest_id = get_manifest_table_id()
        logger.info(f"Manifest ID: {manifest_id}")
        assert manifest_id == f"{config.project_id}.{config.dataset_id}.pipeline_manifest"

        logger.info("Naming test PASSED.")
    except Exception as e:
        logger.error(f"Naming test FAILED: {e}", exc_info=True)

    # 3. Secrets Test (Requires secrets to be set up)
    logger.info("Testing Secrets (will attempt to fetch neo4j-user)...")
    try:
        # Use a secret likely to exist from config defaults
        user_secret_name = config.neo4j_user_secret
        neo_user = get_secret(user_secret_name)
        if neo_user is None:
             # This isn't necessarily a failure of the *function*, but the secret isn't there
             logger.warning(f"Secret '{user_secret_name}' not found or inaccessible. Check Secret Manager setup and permissions.")
             # Don't assert failure here, just warn.
        else:
             logger.info(f"Successfully retrieved secret '{user_secret_name}'. Value length: {len(neo_user)}")
        logger.info("Secrets test COMPLETED (check warnings for retrieval issues).")
    except Exception as e:
        logger.error(f"Secrets test FAILED: {e}", exc_info=True)

    # 4. Retry Test
    logger.info("Testing Retry...")
    try:
        _test_retry() # Should pass
        logger.info("Retry test (success case) PASSED.")
        try:
            _test_retry_fail() # Should fail after retries
            logger.error("Retry test (fail case) FAILED: Expected exception was not raised.")
        except ConnectionError:
            logger.info("Retry test (fail case) PASSED: Correctly raised ConnectionError after retries.")
        except Exception as e:
             logger.error(f"Retry test (fail case) FAILED: Unexpected exception {type(e)} raised.", exc_info=True)

    except Exception as e:
        logger.error(f"Retry test FAILED: {e}", exc_info=True)

    # 5. Clients Test (Requires GCP/Neo4j connectivity and auth)
    logger.info("Testing Clients (will attempt initialization)...")
    try:
        bq = get_bigquery_client()
        assert bq is not None
        logger.info("BigQuery client OK.")
        gcs = get_storage_client()
        assert gcs is not None
        logger.info("Storage client OK.")
        # Test PubSub only if configured
        if config.use_pubsub:
             pub, path = get_pubsub_publisher(topic_id=config.parsing_topic_id) # Test with a topic
             assert pub is not None
             # Path might be None if topic doesn't exist, which is handled, so don't assert path
             logger.info(f"Pub/Sub client OK (Topic path for {config.parsing_topic_id}: {path}).")
        else:
             logger.info("Pub/Sub client skipped (USE_PUBSUB=False).")

        # Test Neo4j only if package installed
        if NEO4J_INSTALLED:
             neo = get_neo4j_driver()
             if neo: # Driver might be None if secrets are missing
                  assert neo is not None
                  logger.info("Neo4j driver OK.")
             else:
                  logger.warning("Neo4j driver initialization skipped/failed (check logs/secrets).")
        else:
             logger.info("Neo4j client skipped (package not installed).")

        logger.info("Clients test PASSED (check warnings for individual client issues).")
    except Exception as e:
        logger.error(f"Clients test FAILED: {e}", exc_info=True)

    # 6. Monitoring Test
    logger.info("Testing Monitoring...")
    try:
        with Timer("test_operation", labels={"type": "test"}):
            time.sleep(0.1) # Simulate work
        increment_counter("test_counter", labels={"status": "ok"})
        increment_counter("test_counter", 5, labels={"status": "ok"})
        record_metric("test_gauge", 42.5)
        metrics = get_metrics()
        assert "test_operation_duration_ms{type=test}" in metrics
        assert "test_counter{status=ok}" in metrics
        assert "test_gauge" in metrics
        assert get_latest_metric_value("test_counter", labels={"status": "ok"}) == 6
        logger.info("Current Metrics:")
        logger.info(export_metrics_to_json()) # Log metrics as JSON
        logger.info("Monitoring test PASSED.")
    except Exception as e:
        logger.error(f"Monitoring test FAILED: {e}", exc_info=True)

    # Cleanup
    logger.info("Closing clients...")
    close_clients()
    logger.info("--- Pipeline Utilities Test Finished ---")

if __name__ == "__main__":
    run_tests()


# ============================================
# File: pipeline_utils/.gitignore
# ============================================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
*.so

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
# Usually ignored by your environment's .gitignore
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
# According to pypa/pipenv#598, it is recommended to include Pipfile*
# Pipfile.lock

# PEP 582; __pypackages__
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# Environments
.env
.venv/
env/
venv/
ENV/
env.bak/
venv.bak/

# VS Code settings
.vscode/

# Config files
config.yaml
# Metrics export
metrics_*.json


# ============================================
# File: Cell 1: Discovery (P1 Final)
# ============================================
"""
Cell 1: Environment Setup & Discovery
Scans GCS for new files based on configured prefixes, checks against a manifest table,
and triggers downstream processing (Cell 2) via Pub/Sub.
Incorporates P1 fixes: Shared Naming, Updated Config/Client logic.
"""
import os
import time
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed

from google.cloud import bigquery, storage, pubsub_v1
from google.api_core.exceptions import NotFound, Conflict, GoogleAPIError

# Import shared utilities
from pipeline_utils.config import get_config
from pipeline_utils.logging import setup_logging, get_logger
from pipeline_utils.retry import retry_with_backoff
from pipeline_utils.clients import get_bigquery_client, get_storage_client, get_pubsub_publisher
from pipeline_utils.naming import get_manifest_table_id
from pipeline_utils.monitoring import Timer, increment_counter, record_metric

# Initialize logging for this component
setup_logging(level=os.environ.get("LOG_LEVEL", "INFO"), component="Discovery") # Use component context
logger = get_logger(__name__)

# Load configuration
config = get_config()

# Constants from config
PROJECT_ID = config.project_id
DATASET_ID = config.dataset_id
BUCKET_NAME = config.bucket_name
LOCATION = config.location
PARSING_TOPIC_ID = config.parsing_topic_id
USE_PUBSUB = config.use_pubsub
DRY_RUN = config.dry_run
DISCOVERY_BATCH_SIZE = config.discovery_batch_size
DISCOVERY_MAX_WORKERS = config.discovery_max_workers
TRIGGER_MAX_WORKERS = config.trigger_max_workers
MAX_RETRIES = config.default_retries # Use default retry count from config
DATA_SOURCES = config.data_sources
MANIFEST_TABLE_ID = get_manifest_table_id() # Use naming util

# Manifest schema definition (Consider moving to a shared schema definition place)
MANIFEST_SCHEMA = [
    bigquery.SchemaField("blob_name", "STRING", mode="REQUIRED"),
    bigquery.SchemaField("bucket_name", "STRING", mode="REQUIRED"),
    bigquery.SchemaField("size_bytes", "INTEGER"),
    bigquery.SchemaField("content_type", "STRING"),
    bigquery.SchemaField("md5_hash", "STRING"),
    bigquery.SchemaField("crc32c", "STRING"),
    bigquery.SchemaField("generation", "INTEGER"),
    bigquery.SchemaField("discovered_at", "TIMESTAMP", mode="REQUIRED"),
    bigquery.SchemaField("processed_at", "TIMESTAMP"),
    bigquery.SchemaField("processing_status", "STRING", mode="REQUIRED"),
    bigquery.SchemaField("target_table", "STRING"),
    bigquery.SchemaField("error_message", "STRING"),
    bigquery.SchemaField("retry_count", "INTEGER", mode="REQUIRED", default_value_expression="0"),
]

# --- Service Functions using Shared Clients and Retry ---

@retry_with_backoff()
def create_dataset_if_not_exists() -> None:
    """Creates the BigQuery dataset if it doesn't exist."""
    bq_client = get_bigquery_client()
    dataset_ref = bq_client.dataset(DATASET_ID)
    try:
        bq_client.get_dataset(dataset_ref)
        logger.info(f"Dataset {PROJECT_ID}.{DATASET_ID} already exists.")
    except NotFound:
        logger.info(f"Dataset {PROJECT_ID}.{DATASET_ID} not found. Creating...")
        dataset = bigquery.Dataset(dataset_ref); dataset.location = LOCATION
        bq_client.create_dataset(dataset, exists_ok=True)
        logger.info(f"Dataset {PROJECT_ID}.{DATASET_ID} created in {LOCATION}.")
        increment_counter("datasets_created")

@retry_with_backoff()
def create_manifest_table_if_not_exists() -> None:
    """Creates the manifest table in BigQuery if it doesn't exist."""
    bq_client = get_bigquery_client()
    try:
        bq_client.get_table(MANIFEST_TABLE_ID)
        logger.info(f"Manifest table {MANIFEST_TABLE_ID} already exists.")
    except NotFound:
        logger.info(f"Manifest table {MANIFEST_TABLE_ID} not found. Creating...")
        table_ref = bigquery.TableReference.from_string(MANIFEST_TABLE_ID)
        table = bigquery.Table(table_ref, schema=MANIFEST_SCHEMA)
        table.time_partitioning = bigquery.TimePartitioning(field="discovered_at", type_="DAY")
        table.clustering_fields = ["processing_status", "bucket_name"]
        try:
            bq_client.create_table(table); logger.info(f"Manifest table {MANIFEST_TABLE_ID} created.")
            increment_counter("tables_created", labels={"table_type": "manifest"})
        except Conflict: logger.info(f"Manifest table {MANIFEST_TABLE_ID} created concurrently.")
        except Exception as e: logger.error(f"Failed to create {MANIFEST_TABLE_ID}: {e}", exc_info=True); raise

@retry_with_backoff()
def batch_check_manifest(blob_names: List[str]) -> Dict[str, Dict[str, Any]]:
    """Checks a batch of blob names against the manifest table."""
    if not blob_names: return {}
    bq_client = get_bigquery_client()
    logger.debug(f"Batch checking manifest for {len(blob_names)} blobs.")
    query = f"SELECT blob_name, processing_status, retry_count FROM `{MANIFEST_TABLE_ID}` WHERE blob_name IN UNNEST(@blob_names) QUALIFY ROW_NUMBER() OVER (PARTITION BY blob_name ORDER BY discovered_at DESC) = 1"
    job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter("blob_names", "STRING", blob_names)])
    try:
        with Timer("batch_check_manifest"):
            results = bq_client.query(query, job_config=job_config).result(timeout=240)
        status = {row.blob_name: dict(row.items()) for row in results}
        logger.debug(f"Found {len(status)} existing manifest entries for batch.")
        record_metric("manifest_batch_check_size", len(blob_names))
        return status
    except Exception as e: logger.error(f"Error batch checking manifest: {e}", exc_info=True); increment_counter("manifest_check_errors"); raise

@retry_with_backoff()
def batch_insert_pending_manifest(rows_to_insert: List[Dict[str, Any]]) -> None:
    """Batch inserts new PENDING rows into the manifest table."""
    if not rows_to_insert: return
    bq_client = get_bigquery_client()
    logger.info(f"Batch inserting {len(rows_to_insert)} new PENDING manifest entries...")
    try:
        with Timer("batch_insert_manifest"):
            errors = bq_client.insert_rows_json(MANIFEST_TABLE_ID, rows_to_insert, skip_invalid_rows=False)
        if errors:
            error_count = sum(len(e.get('errors', [])) for e in errors)
            logger.error(f"{error_count} errors during batch manifest insert: {errors}")
            increment_counter("manifest_insert_errors", error_count)
            raise GoogleAPIError(f"{error_count} insert errors.")
        else:
            logger.debug(f"Successfully batch inserted {len(rows_to_insert)} manifest entries.")
            increment_counter("manifest_rows_inserted", len(rows_to_insert))
    except Exception as e: logger.error(f"Error batch inserting manifest: {e}", exc_info=True); increment_counter("manifest_insert_errors_total"); raise

@retry_with_backoff()
def batch_update_retries_to_pending(blob_names: List[str]) -> None:
    """Batch updates manifest rows from FAILED/Other to PENDING for retry."""
    if not blob_names: return
    bq_client = get_bigquery_client()
    logger.info(f"Batch updating {len(blob_names)} blobs to PENDING for retry...")
    update_sql = f"UPDATE `{MANIFEST_TABLE_ID}` SET processing_status = 'PENDING', error_message = 'Resetting for retry at ' || CAST(CURRENT_TIMESTAMP() AS STRING), processed_at = NULL, target_table = NULL WHERE blob_name IN UNNEST(@blob_names) AND processing_status NOT IN ('SUCCESS', 'PROCESSING', 'PENDING') AND retry_count < @max_retries"
    job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter("blob_names", "STRING", blob_names), bigquery.ScalarQueryParameter("max_retries", "INT64", MAX_RETRIES)])
    try:
        with Timer("batch_update_retries"):
            update_job = bq_client.query(update_sql, job_config=job_config); update_job.result(timeout=240)
        affected = getattr(update_job, 'num_dml_affected_rows', 'unknown')
        logger.info(f"Batch update to PENDING complete. Rows affected: {affected}")
        if isinstance(affected, int): record_metric("manifest_rows_reset_for_retry", affected)
    except Exception as e: logger.error(f"Error batch updating retries: {e}", exc_info=True); increment_counter("manifest_update_errors"); raise


# --- Core Discovery Logic ---
def discover_new_blobs(bucket_name: str, prefix: str, batch_size: int) -> List[storage.Blob]:
    """Discovers blobs, checks manifest, inserts/updates manifest."""
    storage_client = get_storage_client()
    blobs_to_process: List[storage.Blob] = []; processed_in_this_run: Set[str] = set()
    total_scanned = 0; total_new = 0; total_retry = 0; total_already_pend = 0; total_skip_proc = 0; total_skip_inv = 0
    now_ts_utc = datetime.utcnow()
    source_label = prefix.split('/')[1] if '/' in prefix else 'unknown' # Basic label from prefix

    try:
        logger.info(f"Discovering blobs in gs://{bucket_name}/{prefix}...")
        with Timer("discover_blobs", labels={"source": source_label}):
            blob_pages = storage_client.list_blobs(bucket_name, prefix=prefix, page_size=batch_size).pages
            for page_num, page in enumerate(blob_pages):
                current_batch_blobs: List[storage.Blob] = []; current_batch_blob_names: List[str] = []
                page_start = time.time()
                logger.debug(f"Processing page {page_num + 1} for '{prefix}'...")
                for blob in page:
                    total_scanned += 1
                    if blob.name.endswith('/') or blob.size == 0: total_skip_inv += 1; continue
                    if blob.name not in processed_in_this_run: current_batch_blobs.append(blob); current_batch_blob_names.append(blob.name)
                if not current_batch_blob_names: logger.debug(f"Page {page_num + 1} empty."); continue

                try: manifest_status = batch_check_manifest(current_batch_blob_names)
                except Exception as batch_check_err:
                    logger.error(f"Manifest check fail page {page_num + 1}. Skip {len(current_batch_blobs)}. Error: {batch_check_err}", exc_info=True)
                    total_skip_inv += len(current_batch_blobs); processed_in_this_run.update(current_batch_blob_names); continue

                new_rows: List[Dict[str, Any]] = []; retry_updates: List[str] = []
                for blob in current_batch_blobs:
                    status_info = manifest_status.get(blob.name); needs_processing = False
                    if status_info is None: # NEW
                        needs_processing = True; total_new += 1
                        new_rows.append({"blob_name": blob.name, "bucket_name": blob.bucket.name, "size_bytes": blob.size, "content_type": blob.content_type, "md5_hash": blob.md5_hash, "crc32c": blob.crc32c, "generation": blob.generation, "discovered_at": now_ts_utc.isoformat(), "processing_status": "PENDING", "retry_count": 0})
                    elif status_info['processing_status'] == "PENDING": needs_processing = True; total_already_pend += 1 # ALREADY PENDING
                    elif status_info['processing_status'] == "FAILED": # FAILED - RETRY?
                        retries = status_info.get('retry_count', 0) or 0
                        if retries < MAX_RETRIES: needs_processing = True; total_retry += 1; retry_updates.append(blob.name)
                        else: total_skip_proc += 1
                    elif status_info['processing_status'] in ["SUCCESS", "PROCESSING"]: total_skip_proc += 1 # DONE/IN_PROGRESS
                    else: # UNEXPECTED -> RETRY?
                        retries = status_info.get('retry_count', 0) or 0
                        if retries < MAX_RETRIES: needs_processing = True; total_retry += 1; retry_updates.append(blob.name)
                        else: total_skip_proc += 1
                    if needs_processing and blob.name not in processed_in_this_run: blobs_to_process.append(blob); processed_in_this_run.add(blob.name)
                    else: processed_in_this_run.add(blob.name) # Mark skipped/duplicate as processed this run

                if new_rows:
                    try: batch_insert_pending_manifest(new_rows)
                    except Exception as e: logger.error(f"Batch insert fail page {page_num + 1}: {e}", exc_info=True); new_names = {r['blob_name'] for r in new_rows}; blobs_to_process = [b for b in blobs_to_process if b.name not in new_names]; total_new -= len(new_rows)
                if retry_updates:
                    try: batch_update_retries_to_pending(retry_updates)
                    except Exception as e: logger.error(f"Batch update fail page {page_num + 1}: {e}", exc_info=True); blobs_to_process = [b for b in blobs_to_process if b.name not in retry_updates]; total_retry -= len(retry_updates)
                logger.debug(f"Page {page_num + 1} done in {time.time() - page_start:.2f}s.")

        logger.info(f"Discovery '{prefix}' done. Scanned={total_scanned}, New={total_new}, Retry={total_retry}, AlreadyPend={total_already_pend}, SkipProc={total_skip_proc}, SkipInv={total_skip_inv}.")
        # Record metrics
        record_metric("discovery_scanned_total", total_scanned, labels={"source": source_label})
        record_metric("discovery_pending_new", total_new, labels={"source": source_label})
        record_metric("discovery_pending_retry", total_retry, labels={"source": source_label})
        return blobs_to_process
    except NotFound as e: logger.error(f"Bucket '{bucket_name}' not found for prefix '{prefix}'. Check BUCKET_NAME config. Error: {e}", exc_info=False); return []
    except Exception as e: logger.error(f"Critical discovery error for prefix '{prefix}': {e}", exc_info=True); increment_counter("discovery_errors", labels={"source": source_label}); return []


# --- Triggering Logic ---
@retry_with_backoff(retries=2)
def trigger_parsing_job(blob: storage.Blob) -> bool:
    """Publishes trigger message via Pub/Sub."""
    if not USE_PUBSUB: logger.warning(f"Pub/Sub trigger disabled. No trigger for {getattr(blob, 'name', 'N/A')}."); return False
    if not blob: logger.error("Invalid blob object."); return False

    publisher, topic_path = get_pubsub_publisher(topic_id=PARSING_TOPIC_ID)
    if not publisher or not topic_path:
        logger.error(f"Pub/Sub topic '{PARSING_TOPIC_ID}' unavailable. Cannot trigger.")
        increment_counter("pubsub_publish_errors", labels={"topic": PARSING_TOPIC_ID or "unknown", "reason": "unavailable"})
        return False

    message_data = {"blob_name": blob.name, "bucket_name": blob.bucket.name, "trigger_timestamp": datetime.utcnow().isoformat(), "generation": blob.generation, "size_bytes": blob.size}
    message_body = json.dumps(message_data).encode("utf-8")
    try:
        with Timer("trigger_parsing_job"):
            future = publisher.publish(topic_path, message_body)
            future.result(timeout=60)
        logger.debug(f"Published parsing request for {blob.name}.")
        increment_counter("pubsub_publish_success", labels={"topic": PARSING_TOPIC_ID})
        return True
    except Exception as e:
        logger.error(f"Failed Pub/Sub publish for {blob.name}: {e}", exc_info=False)
        increment_counter("pubsub_publish_errors", labels={"topic": PARSING_TOPIC_ID or "unknown", "reason": "publish_error"})
        return False

# --- Main Workflow ---
def run_discovery_and_trigger():
    """Main function: parallel discovery, batch manifest, parallel triggering."""
    with Timer("full_discovery_trigger_process"):
        logger.info("Starting Discovery and Trigger process...")
        try:
            # Clients are retrieved within functions using get_* helpers
            if DRY_RUN: logger.info("*** DRY RUN MODE ENABLED ***")

            logger.info("Ensuring BigQuery dataset and manifest table exist...")
            create_dataset_if_not_exists(); create_manifest_table_if_not_exists()
            logger.info("BigQuery setup verified.")

            all_pending_blobs: List[storage.Blob] = []; discovery_futures = {}
            logger.info(f"Starting parallel blob discovery across {len(DATA_SOURCES)} sources (Workers: {DISCOVERY_MAX_WORKERS})...")
            # Use BUCKET_NAME from config
            with ThreadPoolExecutor(max_workers=DISCOVERY_MAX_WORKERS) as executor:
                for source, source_config in DATA_SOURCES.items():
                    prefix = source_config.get('prefix'); logger.debug(f"Submit discovery: Source={source}, Prefix=gs://{BUCKET_NAME}/{prefix}")
                    if not prefix: logger.warning(f"Source '{source}' missing 'prefix'."); continue
                    if not prefix.endswith('/'): prefix += '/'
                    future = executor.submit(discover_new_blobs, BUCKET_NAME, prefix, DISCOVERY_BATCH_SIZE)
                    discovery_futures[future] = source
                for future in as_completed(discovery_futures):
                    source = discovery_futures[future]
                    try: pending = future.result(); all_pending_blobs.extend(pending); logger.info(f"Finished '{source}', found {len(pending)} blobs.")
                    except Exception as e: logger.error(f"Discovery failed for '{source}': {e}", exc_info=True); increment_counter("discovery_source_errors", labels={"source": source})

            unique_blobs = list({blob.name: blob for blob in all_pending_blobs}.values()); total_unique = len(unique_blobs)
            logger.info(f"Total unique blobs identified for triggering: {total_unique}")
            record_metric("discovery_blobs_to_trigger", total_unique)

            if DRY_RUN: logger.info(f"*** DRY RUN: Would trigger {total_unique} blobs ***"); return

            if unique_blobs:
                if not USE_PUBSUB: logger.warning("Pub/Sub triggering disabled."); return
                # Trigger check happens inside trigger_parsing_job using get_pubsub_publisher

                logger.info(f"--- Triggering parsing for {total_unique} blobs (Workers: {TRIGGER_MAX_WORKERS}) ---")
                success_count = 0; fail_count = 0; trigger_futures = {}
                with ThreadPoolExecutor(max_workers=TRIGGER_MAX_WORKERS) as executor:
                    for blob in unique_blobs: future = executor.submit(trigger_parsing_job, blob); trigger_futures[future] = blob.name
                    processed = 0
                    for future in as_completed(trigger_futures):
                        name = trigger_futures[future]; processed += 1
                        try: ok = future.result()
                        except Exception as e: logger.error(f"Trigger future fail {name}: {e}", exc_info=True); ok = False; increment_counter("trigger_errors")
                        if ok: success_count += 1
                        else: fail_count += 1
                        if processed % 500 == 0 or processed == total_unique: logger.info(f"Trigger progress: {processed}/{total_unique}...")
                logger.info(f"Triggering complete. Success: {success_count}, Failed/Skipped: {fail_count}")
                record_metric("trigger_success_count", success_count); record_metric("trigger_failed_count", fail_count)
                if fail_count > 0: logger.warning(f"{fail_count} blobs failed trigger.")
            else: logger.info("No new/retryable blobs found requiring triggers.")

        except Exception as e: logger.critical(f"Critical error in main discovery/trigger process: {e}", exc_info=True); increment_counter("discovery_critical_errors")

# --- Execution ---
if __name__ == "__main__":
    logger.info("Starting ETL Discovery & Intake (Cell 1 - P1 Final)...")
    run_discovery_and_trigger()
    logger.info("ETL Discovery & Intake (Cell 1) finished.")


# ============================================
# File: Cell 2: Ingestion (P1 Final)
# ============================================
"""
Cell 2: Parsing & Ingestion
Listens for Pub/Sub triggers from Cell 1, loads the specified file from GCS
into a BigQuery staging table using the standardized naming convention.
Updates the central manifest table and triggers Cell 3 (Cleaning).
Incorporates P1 fixes: Shared Naming, Shared Config/Clients/Logging/Retry.
"""
import os
import time
import json
import base64
import logging
from datetime import datetime
from typing import Dict, Any, Optional, Tuple

from google.cloud import bigquery, storage, pubsub_v1
from google.api_core.exceptions import NotFound, GoogleAPIError
from google.api_core.retry import Retry, if_exception_type # Keep BQ specific retry for now

# Import shared utilities
from pipeline_utils.config import get_config
from pipeline_utils.logging import setup_logging, get_logger, set_trace_id # Add set_trace_id
from pipeline_utils.retry import retry_with_backoff
from pipeline_utils.clients import get_bigquery_client, get_storage_client, get_pubsub_publisher
from pipeline_utils.naming import get_staging_table_id, get_manifest_table_id
from pipeline_utils.monitoring import Timer, increment_counter, record_metric

# Initialize logging for this component
setup_logging(level=os.environ.get("LOG_LEVEL", "INFO"), component="Ingestion")
logger = get_logger(__name__)

# Load configuration
config = get_config()

# Constants from config
PROJECT_ID = config.project_id
DATASET_ID = config.dataset_id
LOCATION = config.location
CLEANING_TOPIC_ID = config.cleaning_topic_id
USE_PUBSUB_TRIGGER = config.use_pubsub
MANIFEST_TABLE_ID = get_manifest_table_id() # Use naming util

# Configure BQ load job specific retry policy (can differ from default)
bq_load_retry = Retry(
    initial=5.0, maximum=60.0, multiplier=2.0, deadline=900.0, # 15 min deadline
    predicate=if_exception_type(GoogleAPIError, ConnectionError) # Retry only specific errors
)

# --- Manifest Update Logic (Using shared client/retry) ---
@retry_with_backoff(retries=4) # Use shared decorator
def update_manifest_status(
    blob_name: str, status: str, bucket_name: Optional[str] = None,
    blob_info: Optional[Dict[str, Any]] = None, error_msg: Optional[str] = None,
    target_table: Optional[str] = None, discovered_at: Optional[datetime] = None
) -> None:
    """Updates a blob's status in the manifest table using MERGE."""
    bq_client = get_bigquery_client()
    if not blob_name: logger.error("blob_name required for manifest update"); return
    now_ts = datetime.utcnow(); processed_ts = now_ts if status in ["SUCCESS", "FAILED", "SKIPPED_EMPTY", "SKIPPED_ALREADY_SUCCESS", "SKIPPED_ALREADY_PROCESSING"] else None
    discovered_ts = discovered_at or now_ts; blob_info = blob_info or {}
    if target_table and '.' not in target_table: target_table = f"{PROJECT_ID}.{DATASET_ID}.{target_table.split('.')[-1]}"

    params = [ # Simplified params
        bigquery.ScalarQueryParameter("blob_name", "STRING", blob_name),
        bigquery.ScalarQueryParameter("bucket_name", "STRING", bucket_name or blob_info.get("bucket")),
        bigquery.ScalarQueryParameter("size_bytes", "INTEGER", blob_info.get("size")),
        bigquery.ScalarQueryParameter("generation", "INTEGER", blob_info.get("generation")),
        bigquery.ScalarQueryParameter("discovered_at", "TIMESTAMP", discovered_ts),
        bigquery.ScalarQueryParameter("processed_at", "TIMESTAMP", processed_ts),
        bigquery.ScalarQueryParameter("processing_status", "STRING", status),
        bigquery.ScalarQueryParameter("target_table", "STRING", target_table),
        bigquery.ScalarQueryParameter("error_message", "STRING", error_msg)
    ]
    merge_sql = f"""
        MERGE `{MANIFEST_TABLE_ID}` T USING (SELECT @blob_name AS blob_name) S ON T.blob_name = S.blob_name
        WHEN MATCHED THEN UPDATE SET
            processing_status = @processing_status, processed_at = @processed_at, error_message = @error_message,
            target_table = COALESCE(@target_table, T.target_table),
            retry_count = T.retry_count + CASE WHEN T.processing_status != 'FAILED' AND @processing_status = 'FAILED' THEN 1 ELSE 0 END,
            size_bytes = CASE WHEN T.generation IS NULL OR T.generation != @generation THEN @size_bytes ELSE T.size_bytes END,
            generation = CASE WHEN T.generation IS NULL OR T.generation != @generation THEN @generation ELSE T.generation END
        WHEN NOT MATCHED THEN INSERT (blob_name, bucket_name, size_bytes, generation, discovered_at, processed_at, processing_status, target_table, error_message, retry_count)
          VALUES (@blob_name, @bucket_name, @size_bytes, @generation, @discovered_at, @processed_at, @processing_status, @target_table, @error_message, CASE WHEN @processing_status = 'FAILED' THEN 1 ELSE 0 END)
    """
    job_config = bigquery.QueryJobConfig(query_parameters=params)
    try:
        logger.info(f"Updating manifest: {blob_name} -> {status}")
        with Timer("update_manifest_status"):
            query_job = bq_client.query(merge_sql, job_config=job_config)
            query_job.result(timeout=180)
        logger.debug(f"Manifest update complete for {blob_name}")
    except Exception as e: logger.error(f"Manifest update failed for {blob_name}: {e}", exc_info=True); raise

@retry_with_backoff()
def check_blob_in_manifest(blob_name: str) -> Optional[Dict[str, Any]]:
    """Checks manifest for the latest status of a blob."""
    bq_client = get_bigquery_client()
    # ... (Implementation remains the same) ...
    query = f"SELECT processing_status, retry_count FROM `{MANIFEST_TABLE_ID}` WHERE blob_name = @blob_name ORDER BY discovered_at DESC LIMIT 1"
    job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ScalarQueryParameter("blob_name", "STRING", blob_name)])
    try:
        results = list(bq_client.query(query, job_config=job_config).result(timeout=60))
        return dict(results[0].items()) if results else None
    except Exception as e: logger.error(f"Manifest check error for {blob_name}: {e}", exc_info=True); raise

# --- Core Ingestion Logic ---
# Use BQ specific retry for load_table_from_uri
def load_blob_to_bigquery(blob: storage.Blob, target_table_id: str) -> Tuple[bool, Optional[str]]:
    """Loads a blob to BigQuery with auto-detect (P2: use explicit schema)."""
    bq_client = get_bigquery_client()
    if not blob: logger.error("Invalid blob object"); return False, None
    if not target_table_id: logger.error("Target table ID missing"); return False, None

    uri = f"gs://{blob.bucket.name}/{blob.name}"; logger.info(f"Prepare BQ load: {uri} -> {target_table_id}")
    try: table_ref = bigquery.TableReference.from_string(target_table_id)
    except ValueError as e: logger.error(f"Invalid target table ID '{target_table_id}': {e}"); return False, None

    fmt = blob.name.lower().split('.')[-1]
    if fmt == 'csv': source_format = bigquery.SourceFormat.CSV
    elif fmt in ('json', 'jsonl'): source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON
    elif fmt == 'parquet': source_format = bigquery.SourceFormat.PARQUET
    else: logger.error(f"Unsupported format: {blob.name}"); increment_counter("ingestion_unsupported_format"); return False, target_table_id

    # P1: Still uses autodetect. P2: Replace with explicit schema.
    job_config = bigquery.LoadJobConfig(
        source_format=source_format, autodetect=True,
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        create_disposition=bigquery.CreateDisposition.CREATE_IF_NEEDED,
    )
    if source_format == bigquery.SourceFormat.CSV: job_config.skip_leading_rows = 1; job_config.allow_quoted_newlines = True; job_config.ignore_unknown_values = True

    load_job = None
    try:
        logger.info(f"Starting BQ load job {load_job.job_id if load_job else ''}...")
        with Timer("bq_load_job"):
            load_job = bq_client.load_table_from_uri(uri, table_ref, job_config=job_config, retry=bq_load_retry)
            load_job.result() # Wait for completion, uses google.api_core.retry internally

        if load_job.errors:
            error_str = "; ".join([f"{err.get('reason','?')}: {err.get('message','?')}" for err in load_job.errors[:3]])
            logger.error(f"Load job {load_job.job_id} failed: {error_str}")
            increment_counter("ingestion_load_errors", labels={"reason": load_job.error_result.get('reason', 'unknown') if load_job.error_result else 'unknown'})
            raise GoogleAPIError(f"Load job failed: {error_str}") # Raise to signal failure

        rows = load_job.output_rows if hasattr(load_job, 'output_rows') else 'unknown'
        logger.info(f"Load job {load_job.job_id} complete. Rows loaded: {rows}")
        if isinstance(rows, int): record_metric("ingestion_rows_loaded", rows)
        increment_counter("ingestion_load_success")
        return True, target_table_id
    except Exception as e:
        job_id_str = f" (Job ID: {load_job.job_id})" if load_job else ""
        logger.error(f"Load failed for {uri}{job_id_str}: {e}", exc_info=True)
        increment_counter("ingestion_load_errors", labels={"reason": "exception"})
        raise # Reraise to signal failure to main handler

# --- Triggering Next Step (Cell 3 - Cleaning) ---
@retry_with_backoff(retries=2) # Retry publish briefly
def trigger_cleaning_job(target_table_id: str, blob_name: str):
    """Publishes a message to trigger the cleaning job (Cell 3)."""
    if not USE_PUBSUB_TRIGGER: logger.warning(f"Cleaning trigger disabled."); return

    publisher, topic_path = get_pubsub_publisher(topic_id=CLEANING_TOPIC_ID)
    if not publisher or not topic_path:
        logger.error(f"Cleaning trigger topic '{CLEANING_TOPIC_ID}' unavailable. Cannot trigger.")
        increment_counter("pubsub_publish_errors", labels={"topic": CLEANING_TOPIC_ID, "reason": "unavailable"})
        raise RuntimeError("Cleaning topic unavailable") # Raise critical error if trigger fails

    # P2: Inject trace_id from context
    message_data = {"target_table_id": target_table_id, "source_blob_name": blob_name, "trigger_timestamp": datetime.utcnow().isoformat()}
    message_body = json.dumps(message_data).encode("utf-8")
    try:
        with Timer("trigger_cleaning_job"):
            future = publisher.publish(topic_path, message_body)
            future.result(timeout=30)
        logger.info(f"Published cleaning request for {target_table_id}")
        increment_counter("pubsub_publish_success", labels={"topic": CLEANING_TOPIC_ID})
    except Exception as e:
        logger.error(f"Failed cleaning publish for {target_table_id}: {e}", exc_info=True)
        increment_counter("pubsub_publish_errors", labels={"topic": CLEANING_TOPIC_ID, "reason": "publish_error"})
        raise # Reraise to allow retry

# --- Main Handler Function (Cloud Function/Run Entrypoint) ---
def handle_gcs_event(event: Dict[str, Any], context: Any):
    """Handles GCS event triggered by Pub/Sub message from Cell 1."""
    start_time = time.time()
    blob_name = None; bucket_name = None; target_table_id=None
    status = "FAILED"; error_message = None; final_manifest_blob_info = {}

    try:
        # Decode Message & Set Trace ID if available (P2)
        if 'data' not in event: raise ValueError("No 'data' field in Pub/Sub message")
        message_data = json.loads(base64.b64decode(event['data']).decode('utf-8'))
        bucket_name = message_data.get('bucket_name'); blob_name = message_data.get('blob_name')
        trace_id = message_data.get('trace_id') # P2 feature
        if trace_id: set_trace_id(trace_id) # Set for subsequent logs/calls

        if not bucket_name or not blob_name: raise ValueError("Missing bucket_name or blob_name")
        logger.info(f"Received trigger for gs://{bucket_name}/{blob_name}")
        final_manifest_blob_info = {"bucket": bucket_name, "size": message_data.get("size_bytes"), "generation": message_data.get("generation")}

        # Optional Pre-check manifest
        manifest_entry = check_blob_in_manifest(blob_name)
        if manifest_entry and manifest_entry.get('processing_status') == "SUCCESS":
            logger.info(f"Blob {blob_name} already SUCCESS. Skipping."); status = "SKIPPED_ALREADY_SUCCESS"; return

        # Get blob (uses shared client)
        storage_client = get_storage_client()
        blob = storage_client.bucket(bucket_name).get_blob(blob_name)
        if not blob: raise FileNotFoundError(f"Blob not found: gs://{bucket_name}/{blob_name}")
        final_manifest_blob_info = {"bucket": blob.bucket.name, "size": blob.size, "contentType": blob.content_type, "md5Hash": blob.md5_hash, "crc32c": blob.crc32c, "generation": blob.generation}

        if blob.size == 0:
            logger.info(f"Skipping zero-byte blob: {blob_name}"); status = "SKIPPED_EMPTY"; return

        # Update manifest to PROCESSING *before* starting load
        update_manifest_status(blob_name, "PROCESSING", bucket_name=bucket_name, blob_info=final_manifest_blob_info)

        # P1: Determine target table ID using shared utility
        target_table_id = get_staging_table_id(blob_name) # Uses config defaults
        if not target_table_id: raise ValueError(f"Cannot determine staging table ID for {blob_name}")

        # Load blob to BigQuery (raises exception on failure)
        load_success, actual_table_id = load_blob_to_bigquery(blob, target_table_id)
        target_table_id = actual_table_id or target_table_id # Use returned ID

        # If load_blob_to_bigquery succeeds, status is SUCCESS
        status = "SUCCESS"
        error_message = None # Clear any previous potential error
        logger.info(f"Successfully loaded {blob_name} to {target_table_id}")

        # Trigger cleaning job (raises exception on failure)
        trigger_cleaning_job(target_table_id, blob_name)

    except FileNotFoundError as e: status = "FAILED"; error_message = str(e); logger.error(error_message); increment_counter("ingestion_errors", labels={"reason":"not_found"})
    except ValueError as e: status = "FAILED"; error_message = f"Validation/Naming error: {e}"; logger.error(error_message, exc_info=True); increment_counter("ingestion_errors", labels={"reason":"validation"})
    except Exception as e: status = "FAILED"; error_message = f"Unexpected error: {e}"; logger.error(error_message, exc_info=True); increment_counter("ingestion_errors", labels={"reason":"unexpected"})
    finally:
        # Update manifest to final status (ensure this runs even if trigger fails)
        if status and blob_name: # Check if status was determined and blob_name exists
            try:
                update_manifest_status(
                    blob_name, status, bucket_name=bucket_name, blob_info=final_manifest_blob_info,
                    error_msg=error_message, target_table=target_table_id if status == "SUCCESS" else None
                )
            except Exception as manifest_err:
                logger.critical(f"FINAL MANIFEST UPDATE FAILED for {blob_name} -> {status}: {manifest_err}", exc_info=True)
                increment_counter("manifest_update_errors", labels={"stage":"final"})
        else:
             logger.warning("Skipping final manifest update due to missing blob_name or status.")

        duration = time.time() - start_time
        record_metric("ingestion_process_duration_seconds", duration)
        record_metric("ingestion_status", 1 if status.startswith("SUCCESS") else 0, labels={"status": status}) # 1 for success-like, 0 for fail
        logger.info(f"Ingestion finished for {blob_name or 'unknown'} in {duration:.2f}s. Final Status: {status}")


# ============================================
# File: Cell 3: Cleaning (P1 Final)
# ============================================
"""
Cell 3: Cleaning Service
Listens for Pub/Sub triggers from Cell 2, reads staging data, applies cleaning logic,
writes to a cleaned table using standardized naming, and triggers Cell 3.5 (Journey)
and Cell 4 (Catalog).
Incorporates P1 fixes: Shared Utilities, Correct Naming, Reliable Triggering.
"""
import pandas as pd
import json
import base64
import time
import re
import logging
from datetime import datetime
from typing import Dict, Any, Optional, Callable

from google.cloud import bigquery
from google.api_core.exceptions import NotFound

# Import shared utilities
from pipeline_utils.config import get_config
from pipeline_utils.logging import setup_logging, get_logger, set_trace_id
from pipeline_utils.clients import get_bigquery_client, get_pubsub_publisher
from pipeline_utils.retry import retry_with_backoff
from pipeline_utils.naming import get_cleaned_table_id, extract_date_suffix, extract_source_from_table_name
from pipeline_utils.monitoring import Timer, increment_counter, record_metric

# Initialize logging
setup_logging(level=os.environ.get("LOG_LEVEL", "INFO"), component="Cleaning")
logger = get_logger(__name__)

# Get configuration
config = get_config()
PROJECT_ID = config.project_id
DATASET_ID = config.dataset_id
LOCATION = config.location
USE_PUBSUB_TRIGGER = config.use_pubsub

# --- Source-specific Pre-cleaning Functions (Placeholders) ---
# These should contain the actual cleaning logic for each source
def pre_clean_facebook(df: pd.DataFrame, stg_table_id: str) -> pd.DataFrame:
    logger.debug(f"Running Facebook pre-cleaning for {stg_table_id}...")
    return df
def pre_clean_google_ads(df: pd.DataFrame, stg_table_id: str) -> pd.DataFrame: return df
def pre_clean_ga4(df: pd.DataFrame, stg_table_id: str) -> pd.DataFrame: return df
def pre_clean_ga3(df: pd.DataFrame, stg_table_id: str) -> pd.DataFrame: return df
def pre_clean_shopify(df: pd.DataFrame, stg_table_id: str) -> pd.DataFrame: return df
def pre_clean_generic(df: pd.DataFrame, stg_table_id: str) -> pd.DataFrame:
    logger.warning(f"Using generic pre-cleaner for {stg_table_id}")
    return df

CLEANER_MAP = {
    'facebook': pre_clean_facebook, 'google': pre_clean_google_ads,
    'ga4custom': pre_clean_ga4, 'gaanalytics': pre_clean_ga3, 'shopify': pre_clean_shopify,
}

# --- Generic Schema Standardization ---
def _standardize_schema(df: pd.DataFrame, source: str) -> pd.DataFrame:
    """Generic schema standardization. Needs careful implementation."""
    logger.debug(f"Standardizing schema for source: {source}")
    df = df.copy()
    mapping = { # Add more mappings as needed
        'user_pseudo_id': 'customer_id', 'event_name': 'event_type', 'event_timestamp_micros': 'event_timestamp',
        'revenue': 'event_value', 'cost': 'event_cost', 'date_start': 'event_timestamp'
    }
    rename_dict = {col: mapping[col.lower()] for col in df.columns if col.lower() in mapping}
    df = df.rename(columns=rename_dict)

    core_columns = ['customer_id', 'event_timestamp', 'event_type', 'source']
    for col in core_columns: # Ensure core columns exist
        if col not in df.columns: df[col] = pd.NA

    if 'event_timestamp' in df.columns: # Convert timestamp robustly
        # Try numeric conversion first (assuming microseconds or seconds)
        ts_numeric = pd.to_numeric(df['event_timestamp'], errors='coerce')
        is_micros = (ts_numeric > 1e12) & (ts_numeric < 1e17) # Heuristic for micros
        is_seconds = (ts_numeric > 1e9) & (ts_numeric < 1e11) # Heuristic for seconds
        df.loc[is_micros, 'event_timestamp'] = pd.to_datetime(ts_numeric[is_micros], unit='us', errors='coerce', utc=True)
        df.loc[is_seconds, 'event_timestamp'] = pd.to_datetime(ts_numeric[is_seconds], unit='s', errors='coerce', utc=True)
        # Try string conversion for remaining NAs
        is_na = df['event_timestamp'].isna() | pd.api.types.is_object_dtype(df['event_timestamp'])
        if is_na.any():
             df.loc[is_na, 'event_timestamp'] = pd.to_datetime(df.loc[is_na, 'event_timestamp'], errors='coerce', utc=True)
        # Ensure final type is datetime
        df['event_timestamp'] = pd.to_datetime(df['event_timestamp'], errors='coerce', utc=True)


    if 'source' not in df.columns or df['source'].isnull().all(): df['source'] = source # Add source if missing

    # Clean strings, coerce numeric etc. (Add robust cleaning)
    for col in ['customer_id', 'event_type', 'source']:
        if col in df.columns: df[col] = df[col].astype(str).str.strip().replace(['', 'nan', '(not set)'], pd.NA)
    for col in ['event_value', 'event_cost']:
        if col in df.columns: df[col] = pd.to_numeric(df[col].astype(str).str.replace(r'[$,]', '', regex=True), errors='coerce')

    return df

# --- Cleaning Logic Application ---
def apply_cleaning_logic(df: pd.DataFrame, stg_table_id: str) -> pd.DataFrame:
    """Applies source-specific pre-cleaning and general standardization."""
    source = extract_source_from_table_name(stg_table_id) or "unknown"
    cleaner = CLEANER_MAP.get(source, pre_clean_generic) # Get specific or default to generic
    logger.info(f"Using {'specific' if source in CLEANER_MAP else 'generic'} cleaner for source '{source}'")
    try:
        with Timer("pre_cleaner_execution", labels={"source": source}):
            cleaned_df = cleaner(df.copy(), stg_table_id) # Pass stg_id for context
        increment_counter("cleaning_preprocessing_success", labels={"source": source})
    except Exception as e: logger.error(f"Pre-cleaner error '{source}': {e}", exc_info=True); increment_counter("cleaning_preprocessing_errors", labels={"source": source}); cleaned_df = df.copy()
    try:
        with Timer("schema_standardization", labels={"source": source}):
            standardized_df = _standardize_schema(cleaned_df, source)
        record_metric("cleaning_rows_processed", len(standardized_df), labels={"source": source})
        return standardized_df
    except Exception as e: logger.error(f"Standardization error '{source}': {e}", exc_info=True); increment_counter("cleaning_standardization_errors", labels={"source": source}); return cleaned_df # Return pre-cleaned


# --- BigQuery Load Helpers ---
@retry_with_backoff()
def ensure_table_schema(df: pd.DataFrame, table_ref: bigquery.TableReference):
    """Ensures target table schema exists and adds new columns from DataFrame."""
    bq_client = get_bigquery_client()
    # ... (Implementation remains the same, P2: improve robustness) ...
    try:
        table = bq_client.get_table(table_ref)
        current_schema_map = {field.name.lower(): field for field in table.schema}
        new_schema = table.schema[:] ; added_cols = []
        for col_name in df.columns:
            if col_name.lower() not in current_schema_map:
                dtype = df[col_name].dtype; bq_type = "STRING" # Default
                if pd.api.types.is_integer_dtype(dtype): bq_type = "INT64"
                elif pd.api.types.is_float_dtype(dtype): bq_type = "FLOAT64"
                elif pd.api.types.is_bool_dtype(dtype): bq_type = "BOOL"
                elif pd.api.types.is_datetime64_any_dtype(dtype): bq_type = "TIMESTAMP"
                logger.info(f"Adding new col '{col_name}' ({bq_type}) to {table_ref.table_id}")
                new_field = bigquery.SchemaField(col_name, bq_type, mode="NULLABLE")
                new_schema.append(new_field); added_cols.append(col_name)
        if added_cols:
            logger.info(f"Updating schema {table_ref.table_id} with {len(added_cols)} new cols")
            table.schema = new_schema; bq_client.update_table(table, ['schema'])
            increment_counter("schema_columns_added", len(added_cols))
    except NotFound: logger.info(f"Table {table_ref.table_id} not found, will be created.")
    except Exception as e: logger.error(f"Error ensuring schema {table_ref.table_id}: {e}", exc_info=True); raise

@retry_with_backoff(retries=2)
def load_df_to_bq(df: pd.DataFrame, table_ref: bigquery.TableReference) -> int:
    """Loads a DataFrame to BigQuery, creating or overwriting."""
    bq_client = get_bigquery_client()
    if df.empty: logger.warning(f"Empty DataFrame for {table_ref.table_id}. Skip load."); return 0
    ensure_table_schema(df, table_ref)
    # P1: Still relies on inference. P2: Use explicit BQ schema.
    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_TRUNCATE", create_disposition="CREATE_IF_NEEDED",
        schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION] # Allow adding columns
    )
    logger.info(f"Loading {len(df)} rows into {table_ref.project}.{table_ref.dataset_id}.{table_ref.table_id}")
    load_job = None
    try:
        with Timer("load_dataframe_to_bq"):
            load_job = bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config)
            load_job.result(timeout=600)
        if load_job.errors: error_str = "; ".join([f"{e['reason']}:{e['message']}" for e in load_job.errors]); logger.error(f"Load job {load_job.job_id} failed: {error_str}"); increment_counter("cleaning_load_errors"); raise GoogleAPIError(f"Load job failed: {error_str}")
        rows = load_job.output_rows or len(df); logger.info(f"Load job {load_job.job_id} complete. Rows: {rows}"); record_metric("cleaning_rows_loaded", rows); return rows
    except Exception as e: logger.error(f"Error loading DF to {table_ref.table_id} (Job: {load_job.job_id if load_job else 'N/A'}): {e}", exc_info=True); increment_counter("cleaning_load_errors"); raise

# --- P1: Downstream Trigger Logic ---
@retry_with_backoff(retries=2)
def trigger_downstream_jobs(cleaned_id: str, stg_id: str):
    """Triggers downstream jobs (Catalog and Journey) via Pub/Sub."""
    if not USE_PUBSUB_TRIGGER: logger.warning("Downstream triggers disabled."); return

    date_suffix = extract_date_suffix(cleaned_id) # Get date for Journey trigger

    # Trigger Catalog (Cell 4)
    try:
        catalog_publisher, catalog_topic_path = get_pubsub_publisher(topic_id=config.catalog_topic_id)
        if catalog_publisher and catalog_topic_path:
            msg_data = {'cleaned_table_id': cleaned_id, 'source_stg_table': stg_id, 'trigger_timestamp': datetime.utcnow().isoformat()}
            future = catalog_publisher.publish(catalog_topic_path, json.dumps(msg_data).encode())
            future.result(timeout=30)
            logger.info(f"Published catalog request for {cleaned_id}")
            increment_counter("pubsub_triggers_sent", labels={"topic": config.catalog_topic_id})
        else: logger.warning(f"Catalog trigger topic '{config.catalog_topic_id}' unavailable.")
    except Exception as e: logger.error(f"Catalog trigger failed for {cleaned_id}: {e}", exc_info=True); increment_counter("pubsub_trigger_errors", labels={"topic": config.catalog_topic_id})

    # Trigger Journey Builder (Cell 3.5) - P1 Fix
    if date_suffix:
        try:
            journey_publisher, journey_topic_path = get_pubsub_publisher(topic_id=config.journey_topic_id)
            if journey_publisher and journey_topic_path:
                msg_data = {'date_suffix': date_suffix, 'source_cleaned_table_id': cleaned_id, 'trigger_timestamp': datetime.utcnow().isoformat()}
                future = journey_publisher.publish(journey_topic_path, json.dumps(msg_data).encode())
                future.result(timeout=30)
                logger.info(f"Published journey build request for date {date_suffix} (from {cleaned_id})")
                increment_counter("pubsub_triggers_sent", labels={"topic": config.journey_topic_id})
            else: logger.warning(f"Journey trigger topic '{config.journey_topic_id}' unavailable.")
        except Exception as e: logger.error(f"Journey trigger failed for {date_suffix}: {e}", exc_info=True); increment_counter("pubsub_trigger_errors", labels={"topic": config.journey_topic_id})
    else:
        logger.warning(f"No date suffix in {cleaned_id}. Skipping journey trigger.")


# --- Main Handler (Cloud Function / Run entrypoint) ---
def handle_cleaning_request(event: Dict[str, Any], context: Any):
    """Handles cleaning request triggered by Pub/Sub from Cell 2."""
    start_time = time.time(); stg_id = None; cleaned_id = None
    status = "FAILED"; error_message = None

    try:
        # Decode message & Set Trace ID (P2)
        if 'data' not in event: raise ValueError("No 'data' field")
        message_data = json.loads(base64.b64decode(event['data']).decode('utf-8'))
        stg_id = message_data.get('target_table_id'); blob_name = message_data.get('source_blob_name')
        trace_id = message_data.get('trace_id') # P2 feature
        if trace_id: set_trace_id(trace_id)
        if not stg_id: raise ValueError("Missing target_table_id")
        logger.info(f"Received cleaning request for {stg_id} (from blob: {blob_name or 'unknown'})")

        # P1: Use shared naming util for cleaned table ID
        cleaned_id = get_cleaned_table_id(stg_id)
        if not cleaned_id: raise ValueError(f"Cannot determine cleaned table ID from {stg_id}")
        target_ref = bigquery.TableReference.from_string(cleaned_id)

        # Read Staging Data (uses shared client)
        bq_client = get_bigquery_client()
        try: bq_client.get_table(stg_id) # Check existence
        except NotFound: raise FileNotFoundError(f"Source staging table {stg_id} not found")
        logger.info(f"Reading data from {stg_id}"); df = None
        with Timer("read_staging_data"): df = bq_client.query(f"SELECT * FROM `{stg_id}`").to_dataframe()
        if df is None or df.empty: logger.info(f"{stg_id} is empty. Cleaning skipped."); status = "SUCCESS_EMPTY_SOURCE"; return
        record_metric("cleaning_rows_read", len(df))

        # Apply Cleaning
        logger.info(f"Applying cleaning to {len(df)} rows from {stg_id}")
        with Timer("apply_cleaning_logic"): cleaned_df = apply_cleaning_logic(df, stg_id)
        if cleaned_df.empty: logger.warning(f"Empty DF after cleaning {stg_id}. Skip load."); status = "SUCCESS_EMPTY_POST_CLEAN"; return

        # Load Cleaned Data
        logger.info(f"Loading {len(cleaned_df)} rows to {cleaned_id}")
        rows_loaded = load_df_to_bq(cleaned_df, target_ref) # Contains retry

        status = "SUCCESS"; error_message = None
        logger.info(f"Successfully cleaned {stg_id} -> {cleaned_id} ({rows_loaded} rows)")

        # P1: Trigger downstream
        trigger_downstream_jobs(cleaned_id, stg_id) # Contains retry

    except FileNotFoundError as e: status = "FAILED_NO_SOURCE"; error_message = str(e); logger.error(error_message); increment_counter("cleaning_errors", labels={"reason":"no_source"})
    except ValueError as e: status = "FAILED_INVALID_INPUT"; error_message = str(e); logger.error(error_message, exc_info=True); increment_counter("cleaning_errors", labels={"reason":"validation"})
    except Exception as e: status = "FAILED_PROCESSING"; error_message = str(e); logger.error(f"Cleaning failed: {e}", exc_info=True); increment_counter("cleaning_errors", labels={"reason":"processing"})
    finally:
        duration = time.time() - start_time
        record_metric("cleaning_process_duration_seconds", duration)
        record_metric("cleaning_status", 1 if status.startswith("SUCCESS") else 0, labels={"status": status})
        final_log_status = status if error_message is None else f"{status} [{error_message}]"
        logger.info(f"Cleaning request for {stg_id or '?'} completed in {duration:.2f}s. Final Status: {final_log_status}")


# ============================================
# File: Cell 3.5: Journey Builder (P1 Final)
# ============================================
"""
Cell 3.5: Customer Journey Builder
Listens for Pub/Sub triggers from Cell 3, finds all 'clean_mycocoons_*' tables
for the specified date, generates and executes SQL to create a unified
'cust_journey_events_*' table, and triggers Cell 5 (KG Build).
Incorporates P1 fixes: Shared Utilities, Correct Table Finding, Reliable Triggering.
"""
import json
import base64
import time
import re
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple

from google.cloud import bigquery
from google.api_core.exceptions import NotFound, GoogleAPIError

# Import shared utilities
from pipeline_utils.config import get_config
from pipeline_utils.logging import setup_logging, get_logger, set_trace_id
from pipeline_utils.clients import get_bigquery_client, get_pubsub_publisher
from pipeline_utils.retry import retry_with_backoff
from pipeline_utils.naming import get_journey_table_id, extract_source_from_table_name
from pipeline_utils.monitoring import Timer, increment_counter, record_metric

# Initialize logging
setup_logging(level=os.environ.get("LOG_LEVEL", "INFO"), component="JourneyBuilder")
logger = get_logger(__name__)

# Get configuration
config = get_config()
PROJECT_ID = config.project_id
DATASET_ID = config.dataset_id
LOCATION = config.location
USE_PUBSUB_TRIGGER = config.use_pubsub

# --- List Clean Tables (Corrected Pattern - P1) ---
@retry_with_backoff()
def list_clean_tables_for_date(dataset: str, date_suffix: str) -> List[str]:
    """Finds clean tables matching 'clean_mycocoons_*_YYYYMMDD' pattern."""
    bq_client = get_bigquery_client()
    if not date_suffix: raise ValueError("date_suffix cannot be empty")
    full_dataset_id = f"{PROJECT_ID}.{dataset}"; logger.info(f"Listing tables in {full_dataset_id} for suffix _{date_suffix}")
    try: table_list = list(bq_client.list_tables(full_dataset_id))
    except Exception as e: logger.error(f"Error listing tables in {full_dataset_id}: {e}"); raise
    # P1: Corrected pattern based on get_cleaned_table_id output format
    pattern = re.compile(f"clean_mycocoons_[A-Z][a-zA-Z0-9]*_[A-Z0-9_]*_{date_suffix}$") # SourceCap_IDENTIFIERUPPER_date
    matching_tables = [t.table_id for t in table_list if pattern.match(t.table_id)]
    logger.info(f"Found {len(matching_tables)} clean tables for suffix _{date_suffix}.")
    record_metric("journey_source_tables_found", len(matching_tables), labels={"date_suffix": date_suffix})
    return matching_tables

# --- SQL Generation ---
def generate_journey_sql(project: str, dataset: str, date_suffix: str) -> Optional[str]:
    """Generates SQL for the unified customer journey table."""
    output_table_full_id = get_journey_table_id(date_suffix, project, dataset)
    if not output_table_full_id: return None # Error logged by util

    source_tables = list_clean_tables_for_date(dataset, date_suffix)
    if not source_tables: logger.warning(f"No clean source tables found for {date_suffix}. Cannot generate SQL."); return None

    sql_blocks = []; bq_client = get_bigquery_client()
    for tbl in source_tables:
        source_type = extract_source_from_table_name(tbl) # Uses shared util
        full_source_table_id = f"`{project}.{dataset}.{tbl}`"
        # Dynamic property selection (optional, enhance in P2)
        props_struct = "NULL" # Default
        try:
            table = bq_client.get_table(f"{project}.{dataset}.{tbl}")
            schema_cols = {f.name.lower() for f in table.schema}
            # Define common props to maybe include
            common_props = ['medium','campaign','keyword','page_path','hostname','order_id','product_id','spend','clicks','impressions']
            props_exist = [p for p in common_props if p in schema_cols]
            if props_exist: props_struct = f"TO_JSON_STRING(STRUCT({', '.join(props_exist)}))"
        except Exception as e: logger.warning(f"Schema check failed for {tbl}: {e}. Using NULL props.")

        # Assumes Cell 3 standardized: customer_id, event_timestamp, event_type, source, event_value, event_cost
        sql_blocks.append(f"""
            SELECT
              COALESCE(CAST(customer_id AS STRING), 'unknown_' || GENERATE_UUID()) AS customer_id,
              CAST(event_timestamp AS TIMESTAMP) AS event_timestamp,
              COALESCE(CAST(event_type AS STRING), 'unknown_event') AS event_name,
              COALESCE(CAST(source AS STRING), '{source_type or 'unknown'}') AS source_system,
              '{tbl}' AS source_table_suffix,
              {props_struct} AS event_properties,
              CAST(event_value AS NUMERIC) AS event_value,
              CAST(event_cost AS NUMERIC) AS event_cost
            FROM {full_source_table_id}
            WHERE event_timestamp IS NOT NULL AND customer_id IS NOT NULL -- Basic validation
        """)
    if not sql_blocks: logger.error(f"No valid SQL blocks for {date_suffix}."); return None

    full_sql = f"""
    CREATE OR REPLACE TABLE `{output_table_full_id}`
    PARTITION BY DATE(event_timestamp) CLUSTER BY customer_id, event_name
    OPTIONS(description="Unified customer journey events for {date_suffix}") AS
    WITH SourceEvents AS ({' UNION ALL '.join(sql_blocks)}),
    SequencedEvents AS (
        SELECT *,
            CAST(FARM_FINGERPRINT(TO_JSON_STRING(STRUCT(customer_id, event_timestamp, event_name, source_system, source_table_suffix))) AS STRING) AS event_id,
            ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY event_timestamp ASC, source_system ASC, source_table_suffix ASC) AS journey_step_seq
        FROM SourceEvents WHERE customer_id IS NOT NULL AND event_timestamp IS NOT NULL AND event_name IS NOT NULL
    )
    SELECT customer_id, event_timestamp, event_id, event_name, source_system, source_table_suffix, event_properties, event_value, event_cost, journey_step_seq
    FROM SequencedEvents;
    """
    logger.info(f"Generated journey SQL for {date_suffix} using {len(sql_blocks)} source tables.")
    return full_sql

# --- Run BQ Job ---
@retry_with_backoff()
def run_bq_job(sql: str, job_prefix: str = "build_journey_") -> Tuple[bool, Optional[str]]:
    """Runs BQ SQL job, returns success status and destination table ID."""
    bq_client = get_bigquery_client()
    if not sql: logger.warning("No SQL provided."); return False, None
    logger.info(f"Starting BQ job {job_prefix}..."); dest_table_id = None
    try: dest_table_match = re.search(r"CREATE OR REPLACE TABLE `([^`]+)`", sql, re.IGNORECASE); dest_table_id = dest_table_match.group(1) if dest_table_match else None
    except: pass # Ignore regex errors
    job_config = bigquery.QueryJobConfig(priority=bigquery.QueryPriority.BATCH, job_id_prefix=job_prefix, use_query_cache=False)
    try:
        with Timer("run_journey_build_query"):
            query_job = bq_client.query(sql, job_config=job_config, location=LOCATION)
            logger.info(f"BQ job started: {query_job.job_id}, Dest: {dest_table_id or '?'}")
            query_job.result(timeout=3600) # 1 hour
        if query_job.errors: logger.error(f"BQ job {query_job.job_id} failed: {query_job.errors}"); increment_counter("journey_bq_job_errors"); return False, dest_table_id
        logger.info(f"BQ job {query_job.job_id} completed."); increment_counter("journey_bq_jobs_completed")
        if dest_table_id:
             try: table = bq_client.get_table(dest_table_id); logger.info(f"Table `{dest_table_id}` Rows: {table.num_rows}"); record_metric("journey_table_rows", table.num_rows)
             except Exception as e: logger.warning(f"Cannot fetch details for {dest_table_id}: {e}")
        return True, dest_table_id
    except Exception as e: logger.error(f"BQ job execution failed: {e}", exc_info=True); increment_counter("journey_bq_job_errors"); raise

# --- P1: Trigger Next Step (KG Build - Cell 5) ---
@retry_with_backoff(retries=2)
def trigger_kg_build_job(date_suffix: str, source_journey_table_id: Optional[str]):
    """Publishes trigger message for the KG build job (Cell 5)."""
    if not USE_PUBSUB_TRIGGER: logger.warning("KG trigger disabled."); return

    publisher, topic_path = get_pubsub_publisher(topic_id=config.kg_topic_id)
    if not publisher or not topic_path: logger.error(f"KG trigger topic '{config.kg_topic_id}' unavailable."); increment_counter("pubsub_trigger_errors", labels={"topic": config.kg_topic_id}); raise RuntimeError("KG topic unavailable")

    msg_data = {"date_suffix": date_suffix, "source_journey_table_id": source_journey_table_id, "trigger_timestamp": datetime.utcnow().isoformat()}
    try:
        with Timer("trigger_kg_build_job"):
             future = publisher.publish(topic_path, json.dumps(msg_data).encode("utf-8"))
             future.result(timeout=30)
        logger.info(f"Published KG build request for {date_suffix} (Source: {source_journey_table_id or 'None'}).")
        increment_counter("pubsub_triggers_sent", labels={"topic": config.kg_topic_id})
    except Exception as e: logger.error(f"KG build publish failed for {date_suffix}: {e}", exc_info=True); increment_counter("pubsub_trigger_errors", labels={"topic": config.kg_topic_id}); raise

# --- P1: Main Handler for Pub/Sub Trigger ---
def handle_journey_request(event: Dict[str, Any], context: Any):
    """Handles journey build request triggered by Pub/Sub from Cell 3."""
    start_time = time.time(); date_suffix = None; status = "FAILED"
    try:
        if 'data' not in event: raise ValueError("No 'data' field")
        message_data = json.loads(base64.b64decode(event['data']).decode('utf-8'))
        date_suffix = message_data.get('date_suffix'); source_cleaned = message_data.get('source_cleaned_table_id')
        trace_id = message_data.get('trace_id'); # P2
        if trace_id: set_trace_id(trace_id)
        if not date_suffix or not re.fullmatch(r'\d{8}', date_suffix): raise ValueError(f"Invalid date_suffix: '{date_suffix}'")
        logger.info(f"Received journey request for date: {date_suffix} (from {source_cleaned or 'unknown'})")

        sql = generate_journey_sql(PROJECT_ID, DATASET_ID, date_suffix)
        if not sql: # No source tables found
            logger.warning(f"No SQL generated for {date_suffix}. Triggering KG build with no source table.")
            status = "SUCCESS_NO_SOURCES"
            trigger_kg_build_job(date_suffix, None)
        else:
            success, journey_table_id = run_bq_job(sql, job_prefix=f"journey_{date_suffix}_")
            if success:
                 status = "SUCCESS"; logger.info(f"Journey build success for {date_suffix}.")
                 trigger_kg_build_job(date_suffix, journey_table_id) # Trigger KG build
            else: status = "FAILED_BQ_JOB" # Error logged by run_bq_job

    except ValueError as e: status = "FAILED_INVALID_INPUT"; logger.error(f"Invalid input: {e}", exc_info=True); increment_counter("journey_errors", labels={"reason":"validation"})
    except Exception as e: status = "FAILED_UNEXPECTED"; logger.critical(f"Journey build failed: {e}", exc_info=True); increment_counter("journey_errors", labels={"reason":"unexpected"})
    finally:
        duration = time.time() - start_time
        record_metric("journey_build_duration_seconds", duration)
        record_metric("journey_build_status", 1 if status.startswith("SUCCESS") else 0, labels={"status": status})
        logger.info(f"Journey build for {date_suffix or '?'} finished in {duration:.2f}s. Status: {status}")

# ============================================
# File: Cell 4.1: Data Dictionary (P1 Final)
# ============================================
"""
Cell 4.1: Data Dictionary Population
Scans 'clean_mycocoons_*' tables in the dataset, extracts schema information,
calculates schema hashes, and updates the central data dictionary table if schemas change.
Incorporates P1 fixes: Shared Utilities, Correct Naming Pattern.
"""
import os
import time
import re
import hashlib
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from google.cloud import bigquery
from google.api_core.exceptions import NotFound

# Import shared utilities
from pipeline_utils.config import get_config
from pipeline_utils.logging import setup_logging, get_logger
from pipeline_utils.retry import retry_with_backoff
from pipeline_utils.clients import get_bigquery_client
from pipeline_utils.naming import get_data_dictionary_table_id
from pipeline_utils.monitoring import Timer, increment_counter, record_metric

# Initialize logging
setup_logging(level=os.environ.get("LOG_LEVEL", "INFO"), component="DataDictionary")
logger = get_logger(__name__)

# Get configuration
config = get_config()
PROJECT_ID = config.project_id
DATASET_ID = config.dataset_id
LOCATION = config.location
MAX_WORKERS = config.discovery_max_workers # Reuse worker setting for simplicity

# P1: Use shared utility for dictionary table ID
DATA_DICTIONARY_TABLE_ID = get_data_dictionary_table_id()

# P1: Use updated pattern matching 'clean_mycocoons_*'
TABLE_INCLUDE_PATTERN = r'^clean_mycocoons_.*'
logger.info(f"Data Dictionary processing tables matching: '{TABLE_INCLUDE_PATTERN}' in {PROJECT_ID}.{DATASET_ID}")

# --- Service Functions using Shared Clients/Retry ---
@retry_with_backoff()
def create_data_dictionary_table():
    """Creates the data dictionary table if it doesn't exist."""
    bq_client = get_bigquery_client()
    schema = [ # Consider adding table description field
        bigquery.SchemaField("table_name", "STRING", mode="REQUIRED"), bigquery.SchemaField("column_name", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("data_type", "STRING", mode="REQUIRED"), bigquery.SchemaField("mode", "STRING"),
        bigquery.SchemaField("description", "STRING"), bigquery.SchemaField("schema_hash", "STRING"),
        bigquery.SchemaField("last_captured_at", "TIMESTAMP", mode="REQUIRED"),
    ]
    try:
        table_ref = bigquery.TableReference.from_string(DATA_DICTIONARY_TABLE_ID)
        table = bigquery.Table(table_ref, schema=schema)
        table.time_partitioning = bigquery.TimePartitioning(field="last_captured_at", type_="DAY")
        table.clustering_fields = ["table_name"]
        bq_client.get_table(table_ref); logger.info(f"{DATA_DICTIONARY_TABLE_ID} exists.")
    except NotFound:
        logger.info(f"{DATA_DICTIONARY_TABLE_ID} not found. Creating...")
        try: bq_client.create_table(table); logger.info(f"Created {DATA_DICTIONARY_TABLE_ID}"); increment_counter("tables_created", labels={"table_type": "data_dictionary"})
        except Conflict: logger.info(f"{DATA_DICTIONARY_TABLE_ID} created concurrently.")
        except Exception as e: logger.error(f"Create {DATA_DICTIONARY_TABLE_ID} failed: {e}", exc_info=True); raise
    except Exception as e: logger.error(f"Check/Create {DATA_DICTIONARY_TABLE_ID} failed: {e}", exc_info=True); raise

def calculate_schema_hash(schema: List[bigquery.SchemaField]) -> str:
    """Calculates an MD5 hash of the table schema structure."""
    if not schema: return ""
    schema_repr = sorted([{"name": f.name, "type": f.field_type, "mode": f.mode or "NULLABLE"} for f in schema if hasattr(f,'name')], key=lambda x: x['name'])
    return hashlib.md5(json.dumps(schema_repr, sort_keys=True).encode()).hexdigest()

def extract_schema_info(table: bigquery.Table) -> Tuple[List[Dict[str, Any]], str]:
    """Extracts schema info and calculates hash."""
    table_name = table.table_id; schema = table.schema; ts = datetime.utcnow().isoformat() + 'Z'
    rows = []; schema_hash = ""
    if not schema: logger.warning(f"{table_name} has no schema."); return [], ""
    schema_hash = calculate_schema_hash(schema)
    if not schema_hash: logger.warning(f"Hash failed for {table_name}"); return [], ""
    for f in schema:
        if not hasattr(f, 'name') or not f.name or not hasattr(f, 'field_type') or not f.field_type: logger.warning(f"Skip invalid field {table_name}: {f}"); continue
        rows.append({"table_name": table_name, "column_name": f.name, "data_type": f.field_type, "mode": f.mode or "NULLABLE", "description": f.description or None, "schema_hash": schema_hash, "last_captured_at": ts})
    return rows, schema_hash

@retry_with_backoff(retries=5) # More retries for streaming insert
def insert_rows_to_data_dictionary(rows: List[Dict[str, Any]], table_name_processed: str) -> None:
    """Inserts schema rows into the data dictionary table."""
    bq_client = get_bigquery_client()
    if not rows: logger.debug(f"No rows to insert for {table_name_processed}."); return
    logger.debug(f"Inserting {len(rows)} rows for {table_name_processed} into {DATA_DICTIONARY_TABLE_ID}")
    try:
        with Timer("insert_data_dictionary_rows"):
            errors = bq_client.insert_rows_json(DATA_DICTIONARY_TABLE_ID, rows, skip_invalid_rows=False)
        if errors: logger.error(f"Dict insert errors for {table_name_processed}: {errors}"); increment_counter("datadict_insert_errors", len(errors)); raise GoogleAPIError(f"{len(errors)} insert errors.")
        else: logger.debug(f"Inserted {len(rows)} rows for {table_name_processed}."); increment_counter("datadict_rows_inserted", len(rows))
    except Exception as e: logger.error(f"Error inserting rows for {table_name_processed}: {e}", exc_info=True); increment_counter("datadict_insert_errors_total"); raise

@retry_with_backoff()
def get_existing_schema_hashes() -> Dict[str, str]:
    """Retrieves the latest schema hash for each table from the data dictionary."""
    bq_client = get_bigquery_client()
    hashes = {}; query = f"SELECT table_name, schema_hash FROM `{DATA_DICTIONARY_TABLE_ID}` QUALIFY ROW_NUMBER() OVER (PARTITION BY table_name ORDER BY last_captured_at DESC) = 1"
    try:
        logger.info("Fetching existing schema hashes..."); n=0
        with Timer("get_existing_schema_hashes"):
            for row in bq_client.query(query).result(timeout=120):
                if row.table_name and row.schema_hash: hashes[row.table_name] = row.schema_hash; n+=1
        logger.info(f"Retrieved {n} existing schema hashes.")
        return hashes
    except NotFound: logger.warning(f"{DATA_DICTIONARY_TABLE_ID} not found. Assuming empty."); return {}
    except Exception as e: logger.error(f"Error retrieving hashes: {e}", exc_info=True); raise

def process_table_schema(table_item: bigquery.table.TableListItem, existing_hashes: Dict[str, str]) -> Tuple[str, bool, Optional[str]]:
    """Gets schema, compares hash, inserts if new/changed."""
    bq_client = get_bigquery_client(); table_id = table_item.table_id
    logger.debug(f"Processing schema for: {table_id}")
    try:
        table = bq_client.get_table(f"{PROJECT_ID}.{DATASET_ID}.{table_id}")
        rows, current_hash = extract_schema_info(table)
        if not rows or not current_hash: return table_id, True, "Skipped (No Schema/Hash)"
        if existing_hashes.get(table_id) == current_hash: return table_id, True, "Skipped (Unchanged)"
        else:
            status = "Updated" if table_id in existing_hashes else "Inserted"
            logger.info(f"Schema change/new detected for {table_id}. {status}.")
            insert_rows_to_data_dictionary(rows, table_id)
            increment_counter(f"datadict_schema_{status.lower()}")
            return table_id, True, status
    except NotFound: logger.warning(f"{table_id} not found during schema processing."); return table_id, True, "Skipped (Not Found)"
    except Exception as e: msg = f"Error processing {table_id}: {e}"; logger.error(msg, exc_info=True); increment_counter("datadict_processing_errors"); return table_id, False, msg

def process_schemas_in_parallel(max_workers: int = MAX_WORKERS):
    """Processes schemas for relevant tables in parallel."""
    bq_client = get_bigquery_client(); proc = 0; succ = 0; skip = 0; fail = 0; start = time.time()
    try:
        logger.info(f"Listing tables in {DATASET_ID} matching '{TABLE_INCLUDE_PATTERN}'...")
        try:
            tables_iterator = bq_client.list_tables(DATASET_ID)
            pattern = re.compile(TABLE_INCLUDE_PATTERN, re.IGNORECASE)
            tables = [t for t in tables_iterator if t.table_type == 'TABLE' and pattern.match(t.table_id)]
        except Exception as list_err: logger.error(f"Error listing tables: {list_err}", exc_info=True); return
        total = len(tables)
        if not tables: logger.warning("No matching tables found."); return
        logger.info(f"Found {total} tables matching pattern.")

        existing_hashes = get_existing_schema_hashes()
        logger.info(f"Processing {total} schemas (Workers: {max_workers})...")
        with Timer("process_schemas_parallel"):
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(process_table_schema, tbl, existing_hashes): tbl.table_id for tbl in tables}
                for future in as_completed(futures):
                    name = futures[future]; proc += 1
                    try: _, ok, msg = future.result()
                    except Exception as fut_err: ok = False; logger.error(f"Future fail {name}: {fut_err}", exc_info=True)
                    if ok: ("Skipped" in msg and skip+1) or (succ+1);  # Simplified counter increment
                    else: fail += 1
                    if proc % 20 == 0 or proc == total: logger.info(f"Progress: {proc}/{total}...") # Log less frequently
    except Exception as e: logger.error(f"Schema processing setup failed: {e}", exc_info=True)
    finally:
        logger.info(f"Schema processing finished in {time.time()-start:.2f}s. Processed={proc}, Success/Update={succ}, Skipped={skip}, Failed={fail}")
        record_metric("datadict_tables_processed", proc); record_metric("datadict_tables_updated", succ)
        record_metric("datadict_tables_skipped", skip); record_metric("datadict_tables_failed", fail)

def main():
    """Main function to run data dictionary population."""
    logger.info("Starting Data Dictionary Population (Cell 4.1 - P1 Final)...")
    with Timer("data_dictionary_total_runtime"):
        try:
            create_data_dictionary_table()
            process_schemas_in_parallel(max_workers=MAX_WORKERS)
            logger.info("Data Dictionary Population finished successfully.")
        except Exception as e:
            logger.critical(f"Data dictionary process failed critically: {e}", exc_info=True)
            increment_counter("datadict_critical_errors")

# --- Execution ---
if __name__ == "__main__":
    # This cell would typically be run periodically or triggered after Cell 3 runs.
    # For simplicity, running directly here if executed as a script.
    main()


# ============================================
# File: Cell 5: KG Build (P1 Final)
# ============================================
"""
Cell 5: Knowledge Graph Creation
Listens for Pub/Sub triggers from Cell 3.5, loads data from the unified
'cust_journey_events_*' table for the specified date, calculates funnel metrics,
transforms data into nodes/relationships, and exports to Neo4j using credentials
from Secret Manager.
Incorporates P1 fixes: Shared Utilities, Read from Journey Table, Secrets for Neo4j.
"""
import json
import base64
import time
import re
import hashlib
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict
import pandas as pd
from google.cloud import bigquery
from google.api_core.exceptions import NotFound, GoogleAPIError

# Import shared utilities
from pipeline_utils.config import get_config
from pipeline_utils.logging import setup_logging, get_logger, set_trace_id
from pipeline_utils.clients import get_bigquery_client, get_neo4j_driver, close_clients
from pipeline_utils.retry import retry_with_backoff
from pipeline_utils.naming import get_journey_table_id
from pipeline_utils.monitoring import Timer, increment_counter, record_metric

# Try importing neo4j safely
try:
    import neo4j
    from neo4j.exceptions import Neo4jError, ServiceUnavailable as Neo4jServiceUnavailable
    NEO4J_INSTALLED = True
except ImportError:
    NEO4J_INSTALLED = False

# Initialize logging
setup_logging(level=os.environ.get("LOG_LEVEL", "INFO"), component="KnowledgeGraph")
logger = get_logger(__name__)

# Get configuration
config = get_config()
PROJECT_ID = config.project_id
DATASET_ID = config.dataset_id
LOCATION = config.location

# Load funnel configuration (same as before)
def load_funnel_config() -> Dict[str, List[str]]:
    default_config = { # Example
        'awareness': ['page_view', 'site_visit', 'facebook_ad_event', 'impression'],
        'interest': ['product_view', 'add_to_cart', 'search'],
        'desire': ['initiate_checkout', 'shopify_checkout_created'],
        'action': ['purchase', 'order_completed', 'shopify_order_created']
    }
    return default_config # Simplified loading
FUNNEL_STAGES_CONFIG = load_funnel_config()
EVENT_TO_STAGE_MAP = { e.lower().strip(): s for s, events in FUNNEL_STAGES_CONFIG.items() for e in events }
logger.info(f"Loaded {len(EVENT_TO_STAGE_MAP)} event->stage mappings.")

# --- P1: Load Data from Journey Table ---
@retry_with_backoff()
def load_journey_data(date_suffix: str) -> Optional[pd.DataFrame]:
    """Loads data from the unified customer journey table."""
    bq_client = get_bigquery_client()
    journey_table_id = get_journey_table_id(date_suffix) # Uses config project/dataset
    if not journey_table_id: raise ValueError(f"Cannot determine journey table ID for {date_suffix}")
    logger.info(f"Loading data from journey table: {journey_table_id}")
    query = f"SELECT * FROM `{journey_table_id}` ORDER BY customer_id, journey_step_seq"
    try:
        with Timer("load_journey_data"):
            df = bq_client.query(query, location=LOCATION).to_dataframe(timeout=1800)
        if df.empty: logger.warning(f"Journey table {journey_table_id} empty."); return None
        req_cols = ['customer_id', 'event_timestamp', 'event_id', 'event_name', 'source_system', 'journey_step_seq']
        if not all(c in df.columns for c in req_cols): raise ValueError(f"Journey table missing required columns: {req_cols}")
        df['event_timestamp'] = pd.to_datetime(df['event_timestamp'], errors='coerce', utc=True)
        for col in ['customer_id', 'event_id']: df[col] = df[col].astype(str)
        df = df.where(pd.notna(df), None) # Replace pandas NAs with Python None for Neo4j/JSON
        logger.info(f"Loaded {len(df)} events from {journey_table_id}"); record_metric("kg_journey_rows_loaded", len(df))
        return df
    except NotFound: logger.error(f"Journey table {journey_table_id} not found."); return None
    except Exception as e: logger.error(f"Error loading {journey_table_id}: {e}", exc_info=True); increment_counter("kg_journey_load_errors"); raise

# --- Neo4j Interaction (Uses shared client) ---
@retry_with_backoff(retryable_exceptions=NEO4J_RETRYABLE_EXCEPTIONS + NETWORK_RETRYABLE_EXCEPTIONS)
def run_cypher(query: str, parameters: Optional[Dict] = None, database: Optional[str] = None):
    """Runs Cypher using the shared Neo4j driver."""
    if not NEO4J_INSTALLED: raise RuntimeError("Neo4j driver not installed")
    driver = get_neo4j_driver() # Gets singleton driver (inits with secrets if needed)
    if not driver: raise ConnectionError("Neo4j driver unavailable")
    logger.debug(f"Running Cypher: {query[:150]}...")
    # Use try-with-resources for session management
    with driver.session(database=database) as session:
        is_write = any(kw in query.upper() for kw in ["CREATE", "MERGE", "SET", "DELETE", "REMOVE"])
        tx_func = session.execute_write if is_write else session.execute_read
        # Execute within a transaction
        result = tx_func(lambda tx: tx.run(query, parameters or {}).data())
        return result

def batch_write_nodes(nodes: List[Dict[str, Any]], label: str, batch_size: int = 500, merge_key: str = 'id'):
    """Writes nodes in batches using MERGE."""
    # ... (Implementation uses run_cypher, logic remains the same as before) ...
    if not nodes: return 0; total = 0
    logger.info(f"Writing {len(nodes)} '{label}' nodes...")
    query = f"UNWIND $batch AS props MERGE (n:{label} {{{merge_key}: props.{merge_key}}}) SET n += props RETURN count(n) AS count"
    for i in range(0, len(nodes), batch_size):
        batch = nodes[i:i + batch_size]; valid_batch = []
        for node in batch:
             node = {k: (v.to_pydatetime() if isinstance(v, pd.Timestamp) else v) for k, v in node.items() if v is not None}
             if merge_key not in node or node[merge_key] is None: node[merge_key] = f"{label}_{hashlib.md5(json.dumps(node, sort_keys=True, default=str).encode()).hexdigest()[:16]}"
             valid_batch.append(node)
        if not valid_batch: continue
        try:
             with Timer(f"neo4j_write_{label}_batch"): result = run_cypher(query, {"batch": valid_batch}); count = result[0]['count'] if result else 0; total += count
             record_metric(f"neo4j_{label}_nodes_batch", count)
        except Exception as e: logger.error(f"Error writing {label} batch: {e}"); increment_counter("neo4j_node_write_errors", labels={"label": label}) # Continue?
    logger.info(f"Finished writing {total} '{label}' nodes."); record_metric(f"neo4j_{label}_nodes_total", total); return total

def batch_write_relationships(relationships: List[Dict[str, Any]], batch_size: int = 500):
    """Writes relationships in batches using MERGE."""
    # ... (Implementation uses run_cypher, logic remains the same as before) ...
    if not relationships: return 0; total = 0; logger.info(f"Writing {len(relationships)} relationships...")
    rels_by_type = defaultdict(list)
    for rel in relationships: # Group by type
        rel_type_raw = rel.get('type'); start_id = rel.get('start_node_id'); end_id = rel.get('end_node_id')
        if not rel_type_raw or not start_id or not end_id: continue
        rel_type = re.sub(r'\W+', '_', str(rel_type_raw)).upper();
        if not rel_type: continue
        rels_by_type[rel_type].append(rel)
    for rel_type, rels in rels_by_type.items():
        logger.info(f"Writing {len(rels)} ':{rel_type}' relationships...")
        query = f"UNWIND $batch AS r MATCH (s {{id: r.start}}) MATCH (e {{id: r.end}}) MERGE (s)-[rel:`{rel_type}` {{rel_id: r.id}}]->(e) SET rel += r.props RETURN count(rel) AS count"
        for i in range(0, len(rels), batch_size):
            batch = rels[i:i+batch_size]; prepared = []
            for r_data in batch:
                 props = {k: (v.to_pydatetime() if isinstance(v, pd.Timestamp) else v) for k,v in r_data.get('properties', {}).items() if v is not None}
                 rel_id = r_data.get('rel_id', f"{r_data['start_node_id']}_{rel_type}_{r_data['end_node_id']}_{hashlib.md5(json.dumps(props, sort_keys=True, default=str).encode()).hexdigest()[:8]}")
                 prepared.append({'start': str(r_data['start_node_id']), 'end': str(r_data['end_node_id']), 'id': rel_id, 'props': props})
            if not prepared: continue
            try:
                 with Timer(f"neo4j_write_{rel_type}_rels_batch"): result = run_cypher(query, {"batch": prepared}); count = result[0]['count'] if result else 0; total += count
                 record_metric(f"neo4j_{rel_type}_rels_batch", count)
            except Exception as e: logger.error(f"Error writing :{rel_type} batch: {e}"); increment_counter("neo4j_rel_write_errors", labels={"type": rel_type})
    logger.info(f"Finished writing {total} relationships."); record_metric("neo4j_relationships_total", total); return total


# --- KG Construction & Funnel Logic (Adapted for Journey Table) ---
def map_event_to_funnel_stage(event_name: Optional[str]) -> Optional[str]:
    """Maps event name to funnel stage."""
    if not event_name or pd.isna(event_name): return None
    return EVENT_TO_STAGE_MAP.get(str(event_name).lower().strip())

def create_kg_nodes_rels_from_journey(journey_df: pd.DataFrame) -> Tuple[Dict[str, List[Dict]], List[Dict]]:
    """Transforms unified journey DataFrame into Neo4j nodes and relationships."""
    # ... (Implementation uses journey_df columns, logic remains similar to previous version) ...
    all_nodes = defaultdict(list); all_rels = []; logger.info("Transforming journey data into KG nodes/rels...")
    processed = {'Customer': set(), 'SourceSystem': set(), 'FunnelStage': set()}
    records = journey_df.to_dict('records')
    for rec in tqdm(records, desc="Processing Events for KG"):
        cust_id = rec.get('customer_id'); evt_id = rec.get('event_id'); ts = rec.get('event_timestamp')
        evt_name = rec.get('event_name'); src = rec.get('source_system'); step = rec.get('journey_step_seq')
        # Node Creation
        if cust_id and cust_id not in processed['Customer']: all_nodes['Customer'].append({'id': cust_id}); processed['Customer'].add(cust_id)
        if evt_id: node_props = {k:v for k,v in rec.items() if v is not None}; node_props['id'] = evt_id; node_props['name']=evt_name; all_nodes['Event'].append(node_props)
        if src and src not in processed['SourceSystem']: all_nodes['SourceSystem'].append({'id': src, 'name': src}); processed['SourceSystem'].add(src)
        stage = map_event_to_funnel_stage(evt_name)
        if stage:
             stage_id = stage.lower().replace(' ', '_')
             if stage_id not in processed['FunnelStage']: all_nodes['FunnelStage'].append({'id': stage_id, 'name': stage}); processed['FunnelStage'].add(stage_id)
             if evt_id: all_rels.append({'start_node_id': evt_id, 'end_node_id': stage_id, 'type': 'HAS_STAGE', 'properties': {'timestamp': ts}})
        # Relationships
        if cust_id and evt_id: all_rels.append({'start_node_id': cust_id, 'end_node_id': evt_id, 'type': 'PERFORMED', 'properties': {'timestamp': ts, 'step': step}})
        if evt_id and src: all_rels.append({'start_node_id': evt_id, 'end_node_id': src, 'type': 'FROM_SOURCE', 'properties': {'timestamp': ts}})
    final_nodes = {lbl: list({n['id']: n for n in nodes}.values()) for lbl, nodes in all_nodes.items()}
    logger.info(f"Prepared {sum(len(v) for v in final_nodes.values())} nodes, {len(all_rels)} rels.")
    for lbl, nodes in final_nodes.items(): record_metric(f"kg_nodes_prepared_{lbl.lower()}", len(nodes))
    record_metric("kg_relationships_prepared", len(all_rels))
    return final_nodes, all_rels

def calculate_funnel_metrics_from_journey(journey_df: pd.DataFrame) -> Dict[str, Any]:
    """Calculates funnel metrics based on the journey data."""
    # ... (Implementation remains the same logic as analysis) ...
    if journey_df is None or journey_df.empty: return {}; stage_order = list(FUNNEL_STAGES_CONFIG.keys()); users_per_stage = {s: set() for s in stage_order}; stage_counts = {s: 0 for s in stage_order}
    logger.info("Calculating funnel metrics..."); timer = Timer("calculate_funnel_metrics")
    with timer:
        if not all(c in journey_df.columns for c in ['customer_id', 'event_name']): logger.error("Missing columns for metrics"); return {}
        for _, row in journey_df.iterrows():
            cust_id = row['customer_id']; evt_name = row['event_name']
            if pd.isna(cust_id) or pd.isna(evt_name): continue
            stage = map_event_to_funnel_stage(evt_name)
            if stage: users_per_stage[stage].add(cust_id); stage_counts[stage] += 1
    metrics = {}; prev_users = 0
    for i, stage in enumerate(stage_order):
         count = len(users_per_stage[stage]); metrics[stage] = {'users': count, 'events': stage_counts[stage], 'conv_prev': 0.0}
         if i == 0: prev_users = count
         elif prev_users > 0: metrics[stage]['conv_prev'] = count / prev_users
         if count > 0: prev_users = count # Update denominator
    start_users = metrics.get(stage_order[0], {}).get('users', 0); end_users = metrics.get(stage_order[-1], {}).get('users', 0)
    overall = (end_users / start_users) if start_users > 0 else 0.0; metrics['overall'] = {'conv_rate': overall, 'start_users': start_users, 'end_users': end_users}
    logger.info(f"Funnel metrics calculated. Overall: {overall:.2%}"); record_metric("funnel_overall_conversion", overall)
    return metrics


# --- P1: Main Handler for Pub/Sub Trigger ---
def handle_kg_request(event: Dict[str, Any], context: Any):
    """Handles KG build request triggered by Pub/Sub from Cell 3.5."""
    start_time = time.time(); date_suffix = None; status = "FAILED"
    try:
        # Decode message & Set Trace ID (P2)
        if 'data' not in event: raise ValueError("No 'data' field")
        msg_data = json.loads(base64.b64decode(event['data']).decode('utf-8'))
        date_suffix = msg_data.get('date_suffix'); journey_tbl = msg_data.get('source_journey_table_id')
        trace_id = msg_data.get('trace_id'); # P2
        if trace_id: set_trace_id(trace_id)
        if not date_suffix or not re.fullmatch(r'\d{8}', date_suffix): raise ValueError(f"Invalid date_suffix: '{date_suffix}'")
        logger.info(f"Received KG request for date: {date_suffix} (Source Hint: {journey_tbl or 'None'})")

        # P1: Load data from the unified journey table
        journey_df = load_journey_data(date_suffix) # Uses shared client/retry

        if journey_df is None or journey_df.empty:
            logger.warning(f"No journey data for {date_suffix}. Skipping KG build.")
            status = "SUCCESS_NO_DATA"
        else:
            funnel_metrics = calculate_funnel_metrics_from_journey(journey_df)
            kg_nodes, kg_rels = create_kg_nodes_rels_from_journey(journey_df)

            if not kg_nodes and not kg_rels:
                logger.warning("No nodes/rels generated. Skipping Neo4j export.")
                status = "SUCCESS_NO_GRAPH_DATA"
            else:
                logger.info("Starting Neo4j export...")
                with Timer("neo4j_full_export"):
                    nodes_total = 0
                    for label, nodes in kg_nodes.items():
                        if nodes: nodes_total += batch_write_nodes(nodes, label)
                    rels_total = batch_write_relationships(kg_rels)
                logger.info(f"Neo4j export complete. Nodes={nodes_total}, Rels={rels_total}")
                status = "SUCCESS"
                # Record overall export counts
                record_metric("neo4j_export_nodes_total_overall", nodes_total)
                record_metric("neo4j_export_relationships_total_overall", rels_total)

    except ValueError as e: status = "FAILED_INVALID_INPUT"; logger.error(f"Invalid input: {e}", exc_info=True); increment_counter("kg_errors", labels={"reason":"validation"})
    except ConnectionError as e: status = "FAILED_NEO4J_CONNECTION"; logger.error(f"Neo4j connection error: {e}", exc_info=True); increment_counter("kg_errors", labels={"reason":"neo4j_connect"})
    except Exception as e: status = "FAILED_UNEXPECTED"; logger.critical(f"KG build failed unexpectedly: {e}", exc_info=True); increment_counter("kg_errors", labels={"reason":"unexpected"})
    finally:
        close_clients() # Close Neo4j driver, potentially others
        duration = time.time() - start_time
        record_metric("kg_build_duration_seconds", duration)
        record_metric("kg_build_status", 1 if status.startswith("SUCCESS") else 0, labels={"status": status})
        logger.info(f"KG build for {date_suffix or '?'} finished in {duration:.2f}s. Status: {status}")
    except JourneyTableNotFoundException as e:
         logger.error(f"Process halted: {e}") # Specific error for missing input
    except DataValidationError as e:
         logger.error(f"Process halted due to data validation error: {e}")
    except Exception as e:
        logger.critical(f"A critical error occurred in the main Cell 5 process: {e}", exc_info=True)
    finally:
        # Ensure Neo4j driver is closed if opened
        if neo4j_exporter and neo4j_exporter.driver: neo4j_exporter.close()
        end_time = time.time()
        logger.info(f"Funnel Analysis and Knowledge Graph Creation (Cell 5 - Refactored) finished in {end_time - start_time:.2f} seconds.")

if __name__ == "__main__":
    main()